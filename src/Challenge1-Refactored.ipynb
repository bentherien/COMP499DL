{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import gc\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset\n",
    "from IPython.core.display import display, HTML\n",
    "from numpy.random import RandomState\n",
    "from wide_resnet import WideResNet\n",
    "from auto_augment import AutoAugment, Cutout\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from cifar_loader import SmallSampleController\n",
    "\n",
    "\n",
    "# display(HTML(\"<style>.container { width:40% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, display=True):\n",
    "    \"\"\"\n",
    "    Summary: Implements the training procedure for a given model\n",
    "    == params ==\n",
    "    model: the model to test\n",
    "    device: cuda or cpu \n",
    "    optimizer: the optimizer for our training\n",
    "    train_loader: dataloader for our train data\n",
    "    display: output flag\n",
    "    == output ==\n",
    "    the mean train loss, the train accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    lossTracker = []\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lossTracker.append(loss.detach())\n",
    "        \n",
    "    lossTracker = [x.item()]\n",
    "    meanLoss = np.mean(lossTracker)\n",
    "    accuracy = 100. * batch_idx / len(train_loader)\n",
    "    if display:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "          accuracy, meanLoss))\n",
    "    return accuracy, meanLoss\n",
    "\n",
    "\n",
    "\n",
    "def test(model, device, test_loader,verbose=True):\n",
    "    \"\"\"\n",
    "    Summary: Implements the testing procedure for a given model\n",
    "    == params ==\n",
    "    model: the model to test\n",
    "    device: cuda or cpu \n",
    "    test_loader: dataloader for our test data\n",
    "    verbose: output flag\n",
    "    == output ==\n",
    "    the mean test loss, the test accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    lossTracker = []\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "\n",
    "    mean_test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    if verbose: print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        mean_test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "        \n",
    "    return mean_test_loss, accuracy\n",
    "\n",
    "\n",
    "def checkTest(model,device,valSets,valTracker,verbose=True):\n",
    "    \"\"\"\n",
    "    Summary: checks the test accuracy, prints, and saves statistics\n",
    "    \"\"\"\n",
    "    tempAcc = []\n",
    "    tempLoss = []\n",
    "    for val_loader in valSets:\n",
    "        acc,loss = test(model, device, val_loader,verbose = False)\n",
    "        tempAcc.append(acc)\n",
    "        tempLoss.append(loss)\n",
    "        \n",
    "    accMean = np.mean(tempAcc)\n",
    "    accStd = np.std(tempAcc)\n",
    "    \n",
    "    lossMean = np.mean(tempLoss)\n",
    "    if verbose:\n",
    "        print('[Trained for {} epochs and tested on {} sets of 2000 images]\\\n",
    "        Avg Acc: {:.2f} +- {:.2f}'.format(epochNum,VALIDATION_SET_NUM,accMean,accStd))\n",
    "        \n",
    "    valTracker[\"allLoss\"].extend(tempLoss)\n",
    "    valTracker[\"allAcc\"].extend(tempAcc)\n",
    "    valTracker[\"meanLoss\"].append(meanLoss)\n",
    "    valTracker[\"meanAcc\"].append(meanAcc)\n",
    "    valTracker[\"stdAcc\"].append(stdAcc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLatexRow(testSize,arch,acc,epoch,lr,dataAug=\"Nothing\",title=False):\n",
    "    \"\"\"\n",
    "    Summary: generates one row of latex for a results table\n",
    "    \"\"\"\n",
    "    categories = [\"Architecture\",\"Accuracy\",\"Epochs\",\"learning rate\",\"testSize\",\"data augmentation\"]\n",
    "    row = [str(arch),str(round(acc,3)),str(epoch),str(lr),str(testSize),str(dataAug)]\n",
    "    \n",
    "    if title:\n",
    "        c = \"&\".join(categories)\n",
    "        r = \"&\".join(row)\n",
    "        return \"{}\\\\\\\\\\n{}\\\\\\\\\\n\".format(c,r)\n",
    "    else:\n",
    "        r = \"&\".join(row)\n",
    "        return \"{}\\\\\\\\\\n\".format(r)\n",
    "    \n",
    "    \n",
    "def plot(list_,xlab,ylab,title,color,label,save=False):\n",
    "    \"\"\"\n",
    "    Summary: plots the given list of numbers against its idices and \n",
    "    allows for high resolution saving\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.plot(\n",
    "        list_,[(x+1) for x in range(len(list))],\n",
    "        color=color,marker=\".\",\n",
    "        label=label)\n",
    "    plt.legend()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(\"{}.pdf\".format(title))\n",
    "        os.system(\"pdftoppm -png -r 300 {}.pdf {}\".format(title))\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(model_name):\n",
    "    if \"wide\" in model_name.lower():\n",
    "        return WideResNet(28, 10, num_classes=10)\n",
    "    elif \"fix\" in model_name.lower():\n",
    "        return EfficientNet.from_pretrained(model_name) # change to not be pretrained\n",
    "    \n",
    "    \n",
    "def getOptimizer(optimizer_name,model):\n",
    "    if \"adam\" in  optimizer_name.lower():\n",
    "        return torch.optim.SGD(model.parameters(), \n",
    "                                  lr=0.09, momentum=0.9,\n",
    "                                  weight_decay=0.0005)\n",
    "    elif \"sdg\" in optimizer_name.lower():\n",
    "        return torch.optim.Adam(model.parameters(), \n",
    "                              lr=0.001, weight_decay=0)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/venv/torch11/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5b4428ca7ae9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WideResNet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/torch11/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/torch11/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/torch11/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/torch11/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/torch11/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "EPOCH_NUM = 250\n",
    "TRAIN_SAMPLE_NUM = 100\n",
    "VAL_SAMPLE_NUM = 2000\n",
    "BATCH_SIZE = 256\n",
    "VALIDATION_SET_NUM = 10\n",
    "AUGMENT = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "if AUGMENT:\n",
    "    dataAugmentation = [ \n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        AutoAugment(),\n",
    "        Cutout()\n",
    "    ]\n",
    "    augment = \"Crop,Flip,AutoAugment,Cutout\"\n",
    "else: \n",
    "    dataAugmentation = []\n",
    "    augment = \"Nothing\"\n",
    "\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose(dataAugmentation + [transforms.ToTensor(), normalize]) \n",
    "transform_val = transforms.Compose([transforms.ToTensor(), normalize]) #careful to keep this one same\n",
    "\n",
    "cifar_train = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
    "cifar_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
    "\n",
    "ss = SmallSampleController(numClasses=10,trainSampleNum=TRAIN_SAMPLE_NUM, # abstract the data-loading procedure\n",
    "                           valSampleNum=VAL_SAMPLE_NUM, batchSize=BATCH_SIZE, \n",
    "                           multiplier=VALIDATION_SET_NUM, trainDataset=cifar_train, \n",
    "                           valDataset=cifar_val)\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data, valSets = ss.generateNewSet(device,valMultiplier = VALIDATION_SET_NUM) #Sample from datasets\n",
    "\n",
    "\n",
    "model = getModel(\"WideResNet\").cuda()\n",
    "optimizer = getOptimizer(\"adam\",model)\n",
    "\n",
    "print(' => Total params: %.2fM' % (sum(p.numel() for p in model.parameters()) / 1000000.0))        \n",
    "\n",
    "trainTracker = {\"meanLoss\":[],\"accuracy\":[]}\n",
    "valTracker = {\"allLoss\":[],\"allAcc\":[],\"meanLoss\":[],\"meanAcc\":[],\"stdAcc\":[]}\n",
    "\n",
    "\n",
    "print(\"Begin Train for {} epochs\".format(EPOCH_NUM))\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    train(model, device, train_data[0], optimizer, epoch, display=True)\n",
    "    if (epoch+1) % 25 == 0:\n",
    "        checkTest(model,device,valSets,valTracker,verbose=True)\n",
    "        \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(list_=trainTracker[\"meanLoss\"],xlab=\"Mean Train Loss\",\n",
    "    ylab=\"Epochs\",title=\"Mean Train Loss over Epochs\",\n",
    "    color=\"#243A92\",label=\"mean train loss\",save=True)\n",
    "\n",
    "plot(list_=trainTracker[\"accuracy\"],xlab=\"Train Accuracy\",\n",
    "    ylab=\"Epochs\",title=\"Train Accuracy Over Epochs\",\n",
    "    color=\"#34267E\",label=\"Train Accuracy\",save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(list_=valTracker[\"meanLoss\"],xlab=\"Mean Val Loss\",\n",
    "    ylab=\"25 Epochs\",title=\"Mean Val Loss over 25 Epochs\",\n",
    "    color=\"#243A92\",label=\"mean val loss\",save=True)\n",
    "\n",
    "plot(list_=valTracker[\"meanAcc\"],xlab=\"Val Accuracy\",\n",
    "    ylab=\"25 Epochs\",title=\"Val Accuracy Over 25 Epochs\",\n",
    "    color=\"#34267E\",label=\"Val Accuracy\",save=True)\n",
    "\n",
    "plot(list_=valTracker[\"stdAcc\"],xlab=\"Val Accuracy Standard Deviation\",\n",
    "    ylab=\"25 Epochs\",title=\"Val Accuracy Standard Deviation Over 25 Epochs\",\n",
    "    color=\"#34267E\",label=\"Val Accuracy SD\",save=True)\n",
    "\n",
    "plot(list_=valTracker[\"allLoss\"],xlab=\"Val Loss\",\n",
    "    ylab=\"25 Epochs\",title=\"Val Loss Over 25 Epochs\",\n",
    "    color=\"#34267E\",label=\"Val Loss\",save=True)\n",
    "\n",
    "plot(list_=valTracker[\"allAcc\"],xlab=\"Val Accuracy\",\n",
    "    ylab=\"25 Epochs\",title=\"Val Accuracy Over 25 Epochs\",\n",
    "    color=\"#34267E\",label=\"Val Accuracy\",save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
