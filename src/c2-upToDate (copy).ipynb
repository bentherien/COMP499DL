{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import gc\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset\n",
    "from IPython.core.display import display, HTML\n",
    "from numpy.random import RandomState\n",
    "from wide_resnet import WideResNet\n",
    "from auto_augment import AutoAugment, Cutout\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from cifar_loader import SmallSampleController\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "# display(HTML(\"<style>.container { width:40% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getAcc(preds,targets):\n",
    "    return np.sum([1 if preds[i] == targets[i] else 0 for i in range(len(preds))])/len(preds)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, display=True):\n",
    "    \"\"\"\n",
    "    Summary: Implements the training procedure for a given model\n",
    "    == params ==\n",
    "    model: the model to test\n",
    "    device: cuda or cpu \n",
    "    optimizer: the optimizer for our training\n",
    "    train_loader: dataloader for our train data\n",
    "    display: output flag\n",
    "    == output ==\n",
    "    the mean train loss, the train accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    lossTracker = []\n",
    "    \n",
    "    targets=[]\n",
    "    preds=[]\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lossTracker.append(loss.detach())\n",
    "        with torch.no_grad():\n",
    "            pred = torch.argmax(output,1).cpu().numpy()\n",
    "\n",
    "            preds.extend(pred)\n",
    "            targets.extend(target.cpu().numpy())\n",
    "        \n",
    "    lossTracker = [x.item() for x in lossTracker]\n",
    "    meanLoss = np.mean(lossTracker)\n",
    "    accuracy = getAcc(preds,targets)\n",
    "    if display:\n",
    "        print('Train Epoch: {} [acc: {:.0f}%]\\tLoss: {:.6f}'.format(\n",
    "          epoch, 100. * accuracy, meanLoss))\n",
    "        \n",
    "    return accuracy, meanLoss\n",
    "\n",
    "\n",
    "\n",
    "def test(model, device, test_loader,verbose=True):\n",
    "    \"\"\"\n",
    "    Summary: Implements the testing procedure for a given model\n",
    "    == params ==\n",
    "    model: the model to test\n",
    "    device: cuda or cpu \n",
    "    test_loader: dataloader for our test data\n",
    "    verbose: output flag\n",
    "    == output ==\n",
    "    the mean test loss, the test accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "\n",
    "    meanLoss = test_loss / len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    if verbose: print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        mean_test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "        \n",
    "    return accuracy, meanLoss\n",
    "\n",
    "\n",
    "def checkTest(model,device,valSets,valTracker,latexTracker,epoch,\n",
    "              model_name,optim_name,lr,totalTestSamples,seed,verbose=True):\n",
    "    \"\"\"\n",
    "    Summary: checks the test accuracy, prints, and saves statistics\n",
    "    \"\"\"\n",
    "    tempAcc = []\n",
    "    tempLoss = []\n",
    "    for val_loader in valSets:\n",
    "        acc,loss = test(model, device, val_loader,verbose = False)\n",
    "        tempAcc.append(acc)\n",
    "        tempLoss.append(loss)\n",
    "        \n",
    "    meanAcc = np.mean(tempAcc)\n",
    "    stdAcc = np.std(tempAcc)\n",
    "    \n",
    "    meanLoss = np.mean(tempLoss)\n",
    "    if verbose:\n",
    "        print('[Trained for {} epochs and tested on {} sets of 2000 images]\\\n",
    "        Avg Acc: {:.2f} +- {:.2f} , Avg Loss: {:.2f}'.format(\n",
    "            epoch,VALIDATION_SET_NUM,meanAcc,stdAcc,meanLoss))\n",
    "        \n",
    "        \n",
    "    tableRow = getLatexRow(architecture=model_name,epoch=epoch,accuracy=meanAcc,optim=optim_name,\n",
    "                           lr=lr,totalTestSamples=totalTestSamples,dataAug=\"Nothing\",\n",
    "                           seed=seed,title=False)\n",
    "    \n",
    "    latexTracker.append(tableRow)\n",
    "        \n",
    "    valTracker[\"allLoss\"].extend(tempLoss)\n",
    "    valTracker[\"allAcc\"].extend(tempAcc)\n",
    "    valTracker[\"meanLoss\"].append(meanLoss)\n",
    "    valTracker[\"meanAcc\"].append(meanAcc)\n",
    "    valTracker[\"stdAcc\"].append(stdAcc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLatexRow(architecture,epoch,accuracy,optim,lr,\n",
    "                totalTestSamples,dataAug,seed,title=False):\n",
    "    \"\"\"\n",
    "    Summary: generates one row of latex for a results table\n",
    "    \"\"\"\n",
    "    categories = [\"Model\",\"Epoch\",\"Accuracy\",\"Optimizer\",\"lr\",\"Test Sample Num\",\n",
    "                  \"data augmentation\",\"seed\"]\n",
    "    row = [str(architecture),str(epoch),str(round(accuracy,3)),str(optim),\n",
    "           str(lr),str(totalTestSamples),str(dataAug),str(seed)]\n",
    "    \n",
    "    if title:\n",
    "        c = \"&\".join(categories)\n",
    "        r = \"&\".join(row)\n",
    "        return \"{}\\\\\\\\\\n{}\\\\\\\\\".format(c,r)\n",
    "    else:\n",
    "        r = \"&\".join(row)\n",
    "        return \"{}\\\\\\\\\".format(r)\n",
    "    \n",
    "    \n",
    "def plot(xlist,ylist,xlab,ylab,title,color,label,savedir=\".\",save=False):\n",
    "    \"\"\"\n",
    "    Summary: plots the given list of numbers against its idices and \n",
    "    allows for high resolution saving\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.plot(xlist,ylist,color=color,marker=\".\",label=label)\n",
    "    plt.legend()\n",
    "    \n",
    "    if save:\n",
    "        if not os.path.isdir(savedir):\n",
    "            os.mkdir(savedir)\n",
    "        filepath = os.path.join(savedir,\"{}\".format(title))\n",
    "        plt.savefig(filepath+\".pdf\")\n",
    "        os.system(\"pdftoppm -png -r 300 {}.pdf {}.png\".format(filepath,filepath))\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(model_name):\n",
    "    if \"wide\" in model_name.lower():\n",
    "        return WideResNet(28, 10, num_classes=10)\n",
    "    elif \"efficient\" in model_name.lower():\n",
    "        return EfficientNet.from_pretrained(model_name,num_classes = 10) # change to not be pretrained\n",
    "    elif \"vgg16\" in model_name.lower():\n",
    "        model = models.vgg16(pretrained=True)\n",
    "#         model.classifier[6] = nn.Linear(4096, 10)\n",
    "        return model\n",
    "    elif \"alexnet\" in model_name.lower():\n",
    "        model = models.alexnet(pretrained=True)\n",
    "#         model.classifier = nn.Linear(256 * 6 * 6, 10)\n",
    "        return model\n",
    "    elif \"resnet18\" in model_name.lower():\n",
    "        model = models.resnet18(pretrained=True)\n",
    "#         model.fc.out_features = 10\n",
    "        return model\n",
    "    elif \"resnet50\" in model_name.lower():\n",
    "        model = models.resnet50(pretrained=True)\n",
    "#         model.fc.out_features = 10\n",
    "        return model\n",
    "    elif \"densenet161\" in model_name.lower():\n",
    "        model = models.densenet161(pretrained=True)\n",
    "#         model.fc.out_features = 10\n",
    "        return model\n",
    "    elif \"wideresnet\" in model_name.lower():\n",
    "            model = models.wide_resnet50_2(pretrained=True)\n",
    "            return model\n",
    "    elif \"resnext101\" in model_name.lower():\n",
    "            model = models.resnext101_32x8d(pretrained=True)\n",
    "            return model\n",
    "    elif \"inception_v3\" in model_name.lower():\n",
    "            model = models.inception_v3(pretrained=True,aux_logits=False)\n",
    "            return model\n",
    "    elif \"squeezenet\" in model_name.lower():\n",
    "        model = models.squeezenet1_0(pretrained=True)\n",
    "        model.classifier[1] = nn.Conv2d(512, 10, kernel_size=(1,1), stride=(1,1))\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    \n",
    "def getOptimizer128(optimizer_name,parameters):\n",
    "    if \"sgd\" in  optimizer_name.lower():\n",
    "        LR = 0.1\n",
    "        optim = torch.optim.SGD(parameters, \n",
    "                                  lr=LR, momentum=0.9,\n",
    "                                  weight_decay=0.0005)\n",
    "        return optim, LR\n",
    "    elif \"adam\" in optimizer_name.lower():\n",
    "        LR = 0.00005\n",
    "        optim = torch.optim.Adam(parameters, \n",
    "                              lr=LR, weight_decay=0)\n",
    "        return optim, LR\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class featureExtractor(nn.Module):\n",
    "    def __init__(self,efficientNet):\n",
    "        super().__init__()\n",
    "        self.eNet = efficientNet \n",
    "        self._avg_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self._dropout = nn.Dropout(self.eNet._global_params.dropout_rate)\n",
    "        self._fc = nn.Linear(2560,10)\n",
    "        \n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x = self.eNet.extract_features(inputs)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self._dropout(x)\n",
    "        x = self._fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Generated new permutation of the CIFAR train dataset with                 seed:1619995059, train sample num: 100, test sample num: 2000\n",
      " => Total trainable parameters: 0.74M\n",
      "Begin Train for 1000 epochs\n",
      "Train Epoch: 1 [acc: 8%]\tLoss: 14.944014\n",
      "Train Epoch: 2 [acc: 11%]\tLoss: 14.469419\n",
      "Train Epoch: 3 [acc: 11%]\tLoss: 14.141998\n",
      "Train Epoch: 4 [acc: 9%]\tLoss: 12.489553\n",
      "Train Epoch: 5 [acc: 12%]\tLoss: 12.887087\n",
      "Train Epoch: 6 [acc: 11%]\tLoss: 14.108051\n",
      "Train Epoch: 7 [acc: 7%]\tLoss: 14.177855\n",
      "Train Epoch: 8 [acc: 7%]\tLoss: 13.686115\n",
      "Train Epoch: 9 [acc: 6%]\tLoss: 13.467068\n",
      "Train Epoch: 10 [acc: 9%]\tLoss: 11.572503\n",
      "Train Epoch: 11 [acc: 10%]\tLoss: 12.283890\n",
      "Train Epoch: 12 [acc: 7%]\tLoss: 13.040585\n",
      "Train Epoch: 13 [acc: 12%]\tLoss: 12.436163\n",
      "Train Epoch: 14 [acc: 13%]\tLoss: 12.887883\n",
      "Train Epoch: 15 [acc: 10%]\tLoss: 12.785753\n",
      "Train Epoch: 16 [acc: 6%]\tLoss: 13.504804\n",
      "Train Epoch: 17 [acc: 9%]\tLoss: 11.611463\n",
      "Train Epoch: 18 [acc: 6%]\tLoss: 12.801570\n",
      "Train Epoch: 19 [acc: 8%]\tLoss: 13.230545\n",
      "Train Epoch: 20 [acc: 11%]\tLoss: 10.651477\n",
      "Train Epoch: 21 [acc: 9%]\tLoss: 11.970023\n",
      "Train Epoch: 22 [acc: 9%]\tLoss: 12.679945\n",
      "Train Epoch: 23 [acc: 11%]\tLoss: 11.550677\n",
      "Train Epoch: 24 [acc: 10%]\tLoss: 11.298842\n",
      "Train Epoch: 25 [acc: 14%]\tLoss: 12.612563\n",
      "[Trained for 25 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.59 +- 0.65 , Avg Loss: 8.54\n",
      "Train Epoch: 26 [acc: 5%]\tLoss: 13.244122\n",
      "Train Epoch: 27 [acc: 5%]\tLoss: 12.995348\n",
      "Train Epoch: 28 [acc: 5%]\tLoss: 10.968582\n",
      "Train Epoch: 29 [acc: 9%]\tLoss: 12.339414\n",
      "Train Epoch: 30 [acc: 18%]\tLoss: 10.478260\n",
      "Train Epoch: 31 [acc: 11%]\tLoss: 12.211444\n",
      "Train Epoch: 32 [acc: 11%]\tLoss: 10.715727\n",
      "Train Epoch: 33 [acc: 5%]\tLoss: 13.596574\n",
      "Train Epoch: 34 [acc: 12%]\tLoss: 11.243074\n",
      "Train Epoch: 35 [acc: 10%]\tLoss: 11.112181\n",
      "Train Epoch: 36 [acc: 8%]\tLoss: 11.998434\n",
      "Train Epoch: 37 [acc: 6%]\tLoss: 11.710421\n",
      "Train Epoch: 38 [acc: 8%]\tLoss: 12.152607\n",
      "Train Epoch: 39 [acc: 7%]\tLoss: 12.426362\n",
      "Train Epoch: 40 [acc: 9%]\tLoss: 12.422640\n",
      "Train Epoch: 41 [acc: 12%]\tLoss: 11.138355\n",
      "Train Epoch: 42 [acc: 10%]\tLoss: 11.670468\n",
      "Train Epoch: 43 [acc: 8%]\tLoss: 10.870285\n",
      "Train Epoch: 44 [acc: 9%]\tLoss: 11.141709\n",
      "Train Epoch: 45 [acc: 9%]\tLoss: 10.349004\n",
      "Train Epoch: 46 [acc: 12%]\tLoss: 9.659005\n",
      "Train Epoch: 47 [acc: 7%]\tLoss: 10.413062\n",
      "Train Epoch: 48 [acc: 9%]\tLoss: 9.920093\n",
      "Train Epoch: 49 [acc: 12%]\tLoss: 10.763456\n",
      "Train Epoch: 50 [acc: 11%]\tLoss: 10.778689\n",
      "[Trained for 50 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.98 +- 0.71 , Avg Loss: 7.15\n",
      "Train Epoch: 51 [acc: 7%]\tLoss: 11.036222\n",
      "Train Epoch: 52 [acc: 10%]\tLoss: 9.866782\n",
      "Train Epoch: 53 [acc: 10%]\tLoss: 10.437495\n",
      "Train Epoch: 54 [acc: 13%]\tLoss: 10.304125\n",
      "Train Epoch: 55 [acc: 17%]\tLoss: 9.658030\n",
      "Train Epoch: 56 [acc: 10%]\tLoss: 9.562800\n",
      "Train Epoch: 57 [acc: 13%]\tLoss: 11.795044\n",
      "Train Epoch: 58 [acc: 9%]\tLoss: 9.674097\n",
      "Train Epoch: 59 [acc: 9%]\tLoss: 10.106647\n",
      "Train Epoch: 60 [acc: 7%]\tLoss: 9.175140\n",
      "Train Epoch: 61 [acc: 5%]\tLoss: 10.322984\n",
      "Train Epoch: 62 [acc: 12%]\tLoss: 8.442447\n",
      "Train Epoch: 63 [acc: 11%]\tLoss: 9.971461\n",
      "Train Epoch: 64 [acc: 8%]\tLoss: 10.583492\n",
      "Train Epoch: 65 [acc: 14%]\tLoss: 9.045457\n",
      "Train Epoch: 66 [acc: 10%]\tLoss: 8.729918\n",
      "Train Epoch: 67 [acc: 13%]\tLoss: 9.238241\n",
      "Train Epoch: 68 [acc: 13%]\tLoss: 8.744186\n",
      "Train Epoch: 69 [acc: 9%]\tLoss: 8.104785\n",
      "Train Epoch: 70 [acc: 10%]\tLoss: 9.477402\n",
      "Train Epoch: 71 [acc: 11%]\tLoss: 8.543744\n",
      "Train Epoch: 72 [acc: 11%]\tLoss: 9.072037\n",
      "Train Epoch: 73 [acc: 6%]\tLoss: 9.299921\n",
      "Train Epoch: 74 [acc: 14%]\tLoss: 8.564562\n",
      "Train Epoch: 75 [acc: 14%]\tLoss: 8.691417\n",
      "[Trained for 75 epochs and tested on 5 sets of 2000 images]        Avg Acc: 12.25 +- 0.93 , Avg Loss: 5.86\n",
      "Train Epoch: 76 [acc: 7%]\tLoss: 8.415980\n",
      "Train Epoch: 77 [acc: 7%]\tLoss: 10.491952\n",
      "Train Epoch: 78 [acc: 11%]\tLoss: 8.936954\n",
      "Train Epoch: 79 [acc: 11%]\tLoss: 8.811257\n",
      "Train Epoch: 80 [acc: 8%]\tLoss: 8.908680\n",
      "Train Epoch: 81 [acc: 17%]\tLoss: 7.300604\n",
      "Train Epoch: 82 [acc: 18%]\tLoss: 7.834800\n",
      "Train Epoch: 83 [acc: 10%]\tLoss: 8.583658\n",
      "Train Epoch: 84 [acc: 11%]\tLoss: 9.418797\n",
      "Train Epoch: 85 [acc: 14%]\tLoss: 7.827999\n",
      "Train Epoch: 86 [acc: 12%]\tLoss: 8.473660\n",
      "Train Epoch: 87 [acc: 12%]\tLoss: 8.061670\n",
      "Train Epoch: 88 [acc: 11%]\tLoss: 8.341577\n",
      "Train Epoch: 89 [acc: 8%]\tLoss: 8.108014\n",
      "Train Epoch: 90 [acc: 12%]\tLoss: 8.055243\n",
      "Train Epoch: 91 [acc: 15%]\tLoss: 7.551469\n",
      "Train Epoch: 92 [acc: 11%]\tLoss: 8.048066\n",
      "Train Epoch: 93 [acc: 13%]\tLoss: 8.958785\n",
      "Train Epoch: 94 [acc: 11%]\tLoss: 7.653662\n",
      "Train Epoch: 95 [acc: 18%]\tLoss: 6.047562\n",
      "Train Epoch: 96 [acc: 9%]\tLoss: 8.535094\n",
      "Train Epoch: 97 [acc: 12%]\tLoss: 6.315233\n",
      "Train Epoch: 98 [acc: 12%]\tLoss: 6.573236\n",
      "Train Epoch: 99 [acc: 11%]\tLoss: 8.229510\n",
      "Train Epoch: 100 [acc: 11%]\tLoss: 7.858639\n",
      "[Trained for 100 epochs and tested on 5 sets of 2000 images]        Avg Acc: 12.60 +- 0.88 , Avg Loss: 4.75\n",
      "Train Epoch: 101 [acc: 16%]\tLoss: 7.769266\n",
      "Train Epoch: 102 [acc: 14%]\tLoss: 6.330214\n",
      "Train Epoch: 103 [acc: 11%]\tLoss: 7.030639\n",
      "Train Epoch: 104 [acc: 5%]\tLoss: 7.096310\n",
      "Train Epoch: 105 [acc: 10%]\tLoss: 6.765068\n",
      "Train Epoch: 106 [acc: 15%]\tLoss: 6.779058\n",
      "Train Epoch: 107 [acc: 12%]\tLoss: 6.246009\n",
      "Train Epoch: 108 [acc: 11%]\tLoss: 7.554791\n",
      "Train Epoch: 109 [acc: 11%]\tLoss: 6.698189\n",
      "Train Epoch: 110 [acc: 6%]\tLoss: 8.246546\n",
      "Train Epoch: 111 [acc: 13%]\tLoss: 6.667802\n",
      "Train Epoch: 112 [acc: 12%]\tLoss: 5.442127\n",
      "Train Epoch: 113 [acc: 13%]\tLoss: 6.658660\n",
      "Train Epoch: 114 [acc: 11%]\tLoss: 6.863254\n",
      "Train Epoch: 115 [acc: 12%]\tLoss: 6.310802\n",
      "Train Epoch: 116 [acc: 13%]\tLoss: 6.100397\n",
      "Train Epoch: 117 [acc: 9%]\tLoss: 6.406047\n",
      "Train Epoch: 118 [acc: 9%]\tLoss: 7.344202\n",
      "Train Epoch: 119 [acc: 12%]\tLoss: 5.763378\n",
      "Train Epoch: 120 [acc: 19%]\tLoss: 6.041608\n",
      "Train Epoch: 121 [acc: 11%]\tLoss: 6.443547\n",
      "Train Epoch: 122 [acc: 14%]\tLoss: 7.043020\n",
      "Train Epoch: 123 [acc: 8%]\tLoss: 7.129352\n",
      "Train Epoch: 124 [acc: 15%]\tLoss: 5.758313\n",
      "Train Epoch: 125 [acc: 6%]\tLoss: 6.546463\n",
      "[Trained for 125 epochs and tested on 5 sets of 2000 images]        Avg Acc: 12.92 +- 0.73 , Avg Loss: 3.91\n",
      "Train Epoch: 126 [acc: 14%]\tLoss: 6.459935\n",
      "Train Epoch: 127 [acc: 7%]\tLoss: 5.251002\n",
      "Train Epoch: 128 [acc: 11%]\tLoss: 5.519302\n",
      "Train Epoch: 129 [acc: 9%]\tLoss: 6.225596\n",
      "Train Epoch: 130 [acc: 10%]\tLoss: 6.425006\n",
      "Train Epoch: 131 [acc: 14%]\tLoss: 5.402462\n",
      "Train Epoch: 132 [acc: 5%]\tLoss: 6.105267\n",
      "Train Epoch: 133 [acc: 13%]\tLoss: 6.511964\n",
      "Train Epoch: 134 [acc: 9%]\tLoss: 5.674768\n",
      "Train Epoch: 135 [acc: 6%]\tLoss: 5.163598\n",
      "Train Epoch: 136 [acc: 13%]\tLoss: 5.066605\n",
      "Train Epoch: 137 [acc: 9%]\tLoss: 5.001725\n",
      "Train Epoch: 138 [acc: 12%]\tLoss: 5.122972\n",
      "Train Epoch: 139 [acc: 11%]\tLoss: 5.781536\n",
      "Train Epoch: 140 [acc: 6%]\tLoss: 6.155682\n",
      "Train Epoch: 141 [acc: 12%]\tLoss: 5.245307\n",
      "Train Epoch: 142 [acc: 13%]\tLoss: 5.442792\n",
      "Train Epoch: 143 [acc: 15%]\tLoss: 4.993724\n",
      "Train Epoch: 144 [acc: 13%]\tLoss: 5.935438\n",
      "Train Epoch: 145 [acc: 14%]\tLoss: 4.847542\n",
      "Train Epoch: 146 [acc: 12%]\tLoss: 5.508250\n",
      "Train Epoch: 147 [acc: 9%]\tLoss: 5.183921\n",
      "Train Epoch: 148 [acc: 12%]\tLoss: 5.052660\n",
      "Train Epoch: 149 [acc: 14%]\tLoss: 5.343330\n",
      "Train Epoch: 150 [acc: 8%]\tLoss: 5.108928\n",
      "[Trained for 150 epochs and tested on 5 sets of 2000 images]        Avg Acc: 13.10 +- 0.77 , Avg Loss: 3.34\n",
      "Train Epoch: 151 [acc: 13%]\tLoss: 4.644428\n",
      "Train Epoch: 152 [acc: 12%]\tLoss: 4.844934\n",
      "Train Epoch: 153 [acc: 13%]\tLoss: 5.489962\n",
      "Train Epoch: 154 [acc: 9%]\tLoss: 5.347587\n",
      "Train Epoch: 155 [acc: 12%]\tLoss: 4.907373\n",
      "Train Epoch: 156 [acc: 13%]\tLoss: 5.971454\n",
      "Train Epoch: 157 [acc: 13%]\tLoss: 5.685354\n",
      "Train Epoch: 158 [acc: 9%]\tLoss: 4.705605\n",
      "Train Epoch: 159 [acc: 13%]\tLoss: 4.495846\n",
      "Train Epoch: 160 [acc: 8%]\tLoss: 4.932155\n",
      "Train Epoch: 161 [acc: 10%]\tLoss: 4.935142\n",
      "Train Epoch: 162 [acc: 14%]\tLoss: 3.899411\n",
      "Train Epoch: 163 [acc: 12%]\tLoss: 4.523129\n",
      "Train Epoch: 164 [acc: 9%]\tLoss: 4.861997\n",
      "Train Epoch: 165 [acc: 13%]\tLoss: 4.731317\n",
      "Train Epoch: 166 [acc: 14%]\tLoss: 4.627849\n",
      "Train Epoch: 167 [acc: 13%]\tLoss: 4.216388\n",
      "Train Epoch: 168 [acc: 11%]\tLoss: 3.969929\n",
      "Train Epoch: 169 [acc: 10%]\tLoss: 5.251029\n",
      "Train Epoch: 170 [acc: 16%]\tLoss: 4.511308\n",
      "Train Epoch: 171 [acc: 9%]\tLoss: 4.432462\n",
      "Train Epoch: 172 [acc: 11%]\tLoss: 4.557892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 173 [acc: 13%]\tLoss: 4.605859\n",
      "Train Epoch: 174 [acc: 10%]\tLoss: 4.808379\n",
      "Train Epoch: 175 [acc: 16%]\tLoss: 4.155921\n",
      "[Trained for 175 epochs and tested on 5 sets of 2000 images]        Avg Acc: 12.99 +- 0.48 , Avg Loss: 3.00\n",
      "Train Epoch: 176 [acc: 10%]\tLoss: 4.592595\n",
      "Train Epoch: 177 [acc: 11%]\tLoss: 4.650919\n",
      "Train Epoch: 178 [acc: 12%]\tLoss: 4.732216\n",
      "Train Epoch: 179 [acc: 11%]\tLoss: 4.696495\n",
      "Train Epoch: 180 [acc: 8%]\tLoss: 3.780921\n",
      "Train Epoch: 181 [acc: 18%]\tLoss: 4.378783\n",
      "Train Epoch: 182 [acc: 13%]\tLoss: 4.215981\n",
      "Train Epoch: 183 [acc: 10%]\tLoss: 4.323562\n",
      "Train Epoch: 184 [acc: 11%]\tLoss: 3.693515\n",
      "Train Epoch: 185 [acc: 12%]\tLoss: 4.305671\n",
      "Train Epoch: 186 [acc: 12%]\tLoss: 4.249485\n",
      "Train Epoch: 187 [acc: 9%]\tLoss: 3.356219\n",
      "Train Epoch: 188 [acc: 6%]\tLoss: 4.368622\n",
      "Train Epoch: 189 [acc: 16%]\tLoss: 3.732527\n",
      "Train Epoch: 190 [acc: 8%]\tLoss: 4.014707\n",
      "Train Epoch: 191 [acc: 10%]\tLoss: 3.645141\n",
      "Train Epoch: 192 [acc: 12%]\tLoss: 4.333823\n",
      "Train Epoch: 193 [acc: 12%]\tLoss: 4.038555\n",
      "Train Epoch: 194 [acc: 14%]\tLoss: 3.944203\n",
      "Train Epoch: 195 [acc: 11%]\tLoss: 3.909152\n",
      "Train Epoch: 196 [acc: 13%]\tLoss: 4.638473\n",
      "Train Epoch: 197 [acc: 9%]\tLoss: 4.193590\n",
      "Train Epoch: 198 [acc: 10%]\tLoss: 3.889916\n",
      "Train Epoch: 199 [acc: 9%]\tLoss: 4.335250\n",
      "Train Epoch: 200 [acc: 12%]\tLoss: 3.679104\n",
      "[Trained for 200 epochs and tested on 5 sets of 2000 images]        Avg Acc: 12.77 +- 0.29 , Avg Loss: 2.77\n",
      "Train Epoch: 201 [acc: 8%]\tLoss: 3.625901\n",
      "Train Epoch: 202 [acc: 12%]\tLoss: 3.403466\n",
      "Train Epoch: 203 [acc: 6%]\tLoss: 4.359385\n",
      "Train Epoch: 204 [acc: 6%]\tLoss: 4.501689\n",
      "Train Epoch: 205 [acc: 8%]\tLoss: 4.184774\n",
      "Train Epoch: 206 [acc: 13%]\tLoss: 4.115128\n",
      "Train Epoch: 207 [acc: 13%]\tLoss: 3.311629\n",
      "Train Epoch: 208 [acc: 11%]\tLoss: 3.468335\n",
      "Train Epoch: 209 [acc: 8%]\tLoss: 3.852189\n",
      "Train Epoch: 210 [acc: 14%]\tLoss: 3.581313\n",
      "Train Epoch: 211 [acc: 10%]\tLoss: 3.403903\n",
      "Train Epoch: 212 [acc: 7%]\tLoss: 4.113336\n",
      "Train Epoch: 213 [acc: 12%]\tLoss: 3.604684\n",
      "Train Epoch: 214 [acc: 11%]\tLoss: 3.706204\n",
      "Train Epoch: 215 [acc: 18%]\tLoss: 3.151128\n",
      "Train Epoch: 216 [acc: 12%]\tLoss: 3.152114\n",
      "Train Epoch: 217 [acc: 10%]\tLoss: 3.594066\n",
      "Train Epoch: 218 [acc: 12%]\tLoss: 3.758267\n",
      "Train Epoch: 219 [acc: 7%]\tLoss: 3.319255\n",
      "Train Epoch: 220 [acc: 7%]\tLoss: 3.658882\n",
      "Train Epoch: 221 [acc: 8%]\tLoss: 3.349452\n",
      "Train Epoch: 222 [acc: 11%]\tLoss: 3.582707\n",
      "Train Epoch: 223 [acc: 12%]\tLoss: 3.761394\n",
      "Train Epoch: 224 [acc: 10%]\tLoss: 3.192752\n",
      "Train Epoch: 225 [acc: 12%]\tLoss: 3.382193\n",
      "[Trained for 225 epochs and tested on 5 sets of 2000 images]        Avg Acc: 12.67 +- 0.35 , Avg Loss: 2.63\n",
      "Train Epoch: 226 [acc: 8%]\tLoss: 3.224378\n",
      "Train Epoch: 227 [acc: 9%]\tLoss: 3.586769\n",
      "Train Epoch: 228 [acc: 14%]\tLoss: 3.342383\n",
      "Train Epoch: 229 [acc: 15%]\tLoss: 3.397455\n",
      "Train Epoch: 230 [acc: 12%]\tLoss: 3.102799\n",
      "Train Epoch: 231 [acc: 11%]\tLoss: 3.691736\n",
      "Train Epoch: 232 [acc: 14%]\tLoss: 3.365860\n",
      "Train Epoch: 233 [acc: 13%]\tLoss: 3.458377\n",
      "Train Epoch: 234 [acc: 16%]\tLoss: 2.976507\n",
      "Train Epoch: 235 [acc: 11%]\tLoss: 3.191489\n",
      "Train Epoch: 236 [acc: 11%]\tLoss: 3.569333\n",
      "Train Epoch: 237 [acc: 9%]\tLoss: 3.931393\n",
      "Train Epoch: 238 [acc: 13%]\tLoss: 3.350059\n",
      "Train Epoch: 239 [acc: 6%]\tLoss: 3.095812\n",
      "Train Epoch: 240 [acc: 14%]\tLoss: 2.699281\n",
      "Train Epoch: 241 [acc: 12%]\tLoss: 3.900765\n",
      "Train Epoch: 242 [acc: 12%]\tLoss: 3.068407\n",
      "Train Epoch: 243 [acc: 10%]\tLoss: 3.104721\n",
      "Train Epoch: 244 [acc: 9%]\tLoss: 2.928612\n",
      "Train Epoch: 245 [acc: 14%]\tLoss: 3.122402\n",
      "Train Epoch: 246 [acc: 7%]\tLoss: 3.476454\n",
      "Train Epoch: 247 [acc: 12%]\tLoss: 2.837313\n",
      "Train Epoch: 248 [acc: 9%]\tLoss: 3.286373\n",
      "Train Epoch: 249 [acc: 12%]\tLoss: 3.378587\n",
      "Train Epoch: 250 [acc: 9%]\tLoss: 3.842172\n",
      "[Trained for 250 epochs and tested on 5 sets of 2000 images]        Avg Acc: 12.53 +- 0.40 , Avg Loss: 2.55\n",
      "Train Epoch: 251 [acc: 13%]\tLoss: 2.948686\n",
      "Train Epoch: 252 [acc: 16%]\tLoss: 2.878444\n",
      "Train Epoch: 253 [acc: 11%]\tLoss: 3.413543\n",
      "Train Epoch: 254 [acc: 16%]\tLoss: 2.897010\n",
      "Train Epoch: 255 [acc: 9%]\tLoss: 3.277518\n",
      "Train Epoch: 256 [acc: 12%]\tLoss: 3.254712\n",
      "Train Epoch: 257 [acc: 14%]\tLoss: 3.227621\n",
      "Train Epoch: 258 [acc: 9%]\tLoss: 3.131164\n",
      "Train Epoch: 259 [acc: 14%]\tLoss: 2.920945\n",
      "Train Epoch: 260 [acc: 17%]\tLoss: 3.631622\n",
      "Train Epoch: 261 [acc: 12%]\tLoss: 3.340198\n",
      "Train Epoch: 262 [acc: 10%]\tLoss: 2.627678\n",
      "Train Epoch: 263 [acc: 13%]\tLoss: 2.925691\n",
      "Train Epoch: 264 [acc: 11%]\tLoss: 3.288846\n",
      "Train Epoch: 265 [acc: 10%]\tLoss: 3.661471\n",
      "Train Epoch: 266 [acc: 13%]\tLoss: 3.446192\n",
      "Train Epoch: 267 [acc: 8%]\tLoss: 3.052122\n",
      "Train Epoch: 268 [acc: 16%]\tLoss: 2.969531\n",
      "Train Epoch: 269 [acc: 6%]\tLoss: 3.493488\n",
      "Train Epoch: 270 [acc: 13%]\tLoss: 4.172604\n",
      "Train Epoch: 271 [acc: 12%]\tLoss: 3.411620\n",
      "Train Epoch: 272 [acc: 13%]\tLoss: 3.437487\n",
      "Train Epoch: 273 [acc: 10%]\tLoss: 2.937047\n",
      "Train Epoch: 274 [acc: 10%]\tLoss: 3.115669\n",
      "Train Epoch: 275 [acc: 8%]\tLoss: 3.333508\n",
      "[Trained for 275 epochs and tested on 5 sets of 2000 images]        Avg Acc: 12.49 +- 0.33 , Avg Loss: 2.49\n",
      "Train Epoch: 276 [acc: 11%]\tLoss: 2.846771\n",
      "Train Epoch: 277 [acc: 11%]\tLoss: 3.549772\n",
      "Train Epoch: 278 [acc: 12%]\tLoss: 3.061007\n",
      "Train Epoch: 279 [acc: 13%]\tLoss: 2.986707\n",
      "Train Epoch: 280 [acc: 15%]\tLoss: 3.260951\n",
      "Train Epoch: 281 [acc: 13%]\tLoss: 2.663728\n",
      "Train Epoch: 282 [acc: 12%]\tLoss: 3.323705\n",
      "Train Epoch: 283 [acc: 9%]\tLoss: 3.252450\n",
      "Train Epoch: 284 [acc: 7%]\tLoss: 3.165646\n",
      "Train Epoch: 285 [acc: 15%]\tLoss: 2.752867\n",
      "Train Epoch: 286 [acc: 11%]\tLoss: 3.456072\n",
      "Train Epoch: 287 [acc: 10%]\tLoss: 3.047804\n",
      "Train Epoch: 288 [acc: 12%]\tLoss: 2.867602\n",
      "Train Epoch: 289 [acc: 8%]\tLoss: 3.214594\n",
      "Train Epoch: 290 [acc: 14%]\tLoss: 2.872820\n",
      "Train Epoch: 291 [acc: 9%]\tLoss: 2.681323\n",
      "Train Epoch: 292 [acc: 15%]\tLoss: 3.074851\n",
      "Train Epoch: 293 [acc: 9%]\tLoss: 3.016134\n",
      "Train Epoch: 294 [acc: 12%]\tLoss: 2.825527\n",
      "Train Epoch: 295 [acc: 12%]\tLoss: 2.869986\n",
      "Train Epoch: 296 [acc: 6%]\tLoss: 3.130380\n",
      "Train Epoch: 297 [acc: 10%]\tLoss: 3.240263\n",
      "Train Epoch: 298 [acc: 10%]\tLoss: 2.929585\n",
      "Train Epoch: 299 [acc: 10%]\tLoss: 3.196229\n",
      "Train Epoch: 300 [acc: 11%]\tLoss: 2.720638\n",
      "[Trained for 300 epochs and tested on 5 sets of 2000 images]        Avg Acc: 12.30 +- 0.30 , Avg Loss: 2.45\n",
      "Train Epoch: 301 [acc: 15%]\tLoss: 2.679169\n",
      "Train Epoch: 302 [acc: 12%]\tLoss: 2.621396\n",
      "Train Epoch: 303 [acc: 13%]\tLoss: 2.570026\n",
      "Train Epoch: 304 [acc: 11%]\tLoss: 3.088658\n",
      "Train Epoch: 305 [acc: 10%]\tLoss: 2.910121\n",
      "Train Epoch: 306 [acc: 11%]\tLoss: 2.855111\n",
      "Train Epoch: 307 [acc: 12%]\tLoss: 2.720904\n",
      "Train Epoch: 308 [acc: 11%]\tLoss: 2.998386\n",
      "Train Epoch: 309 [acc: 11%]\tLoss: 2.974940\n",
      "Train Epoch: 310 [acc: 9%]\tLoss: 2.534523\n",
      "Train Epoch: 311 [acc: 12%]\tLoss: 3.090204\n",
      "Train Epoch: 312 [acc: 10%]\tLoss: 3.036476\n",
      "Train Epoch: 313 [acc: 11%]\tLoss: 2.602621\n",
      "Train Epoch: 314 [acc: 10%]\tLoss: 2.702360\n",
      "Train Epoch: 315 [acc: 12%]\tLoss: 3.111757\n",
      "Train Epoch: 316 [acc: 12%]\tLoss: 2.670045\n",
      "Train Epoch: 317 [acc: 12%]\tLoss: 2.624052\n",
      "Train Epoch: 318 [acc: 5%]\tLoss: 2.907431\n",
      "Train Epoch: 319 [acc: 10%]\tLoss: 2.878222\n",
      "Train Epoch: 320 [acc: 8%]\tLoss: 2.639871\n",
      "Train Epoch: 321 [acc: 10%]\tLoss: 2.677423\n",
      "Train Epoch: 322 [acc: 7%]\tLoss: 3.012763\n",
      "Train Epoch: 323 [acc: 14%]\tLoss: 2.425542\n",
      "Train Epoch: 324 [acc: 8%]\tLoss: 2.689836\n",
      "Train Epoch: 325 [acc: 9%]\tLoss: 2.665433\n",
      "[Trained for 325 epochs and tested on 5 sets of 2000 images]        Avg Acc: 12.14 +- 0.24 , Avg Loss: 2.42\n",
      "Train Epoch: 326 [acc: 14%]\tLoss: 2.697907\n",
      "Train Epoch: 327 [acc: 12%]\tLoss: 2.747673\n",
      "Train Epoch: 328 [acc: 10%]\tLoss: 2.759158\n",
      "Train Epoch: 329 [acc: 5%]\tLoss: 3.444299\n",
      "Train Epoch: 330 [acc: 12%]\tLoss: 2.659256\n",
      "Train Epoch: 331 [acc: 9%]\tLoss: 2.605391\n",
      "Train Epoch: 332 [acc: 15%]\tLoss: 2.681283\n",
      "Train Epoch: 333 [acc: 11%]\tLoss: 3.028300\n",
      "Train Epoch: 334 [acc: 9%]\tLoss: 3.018827\n",
      "Train Epoch: 335 [acc: 10%]\tLoss: 2.953986\n",
      "Train Epoch: 336 [acc: 9%]\tLoss: 2.624409\n",
      "Train Epoch: 337 [acc: 12%]\tLoss: 2.810558\n",
      "Train Epoch: 338 [acc: 8%]\tLoss: 3.001813\n",
      "Train Epoch: 339 [acc: 14%]\tLoss: 2.569235\n",
      "Train Epoch: 340 [acc: 14%]\tLoss: 3.006211\n",
      "Train Epoch: 341 [acc: 8%]\tLoss: 3.194261\n",
      "Train Epoch: 342 [acc: 12%]\tLoss: 2.813073\n",
      "Train Epoch: 343 [acc: 10%]\tLoss: 2.969184\n",
      "Train Epoch: 344 [acc: 11%]\tLoss: 2.830093\n",
      "Train Epoch: 345 [acc: 8%]\tLoss: 2.803642\n",
      "Train Epoch: 346 [acc: 9%]\tLoss: 2.903782\n",
      "Train Epoch: 347 [acc: 9%]\tLoss: 2.727782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 348 [acc: 11%]\tLoss: 2.688836\n",
      "Train Epoch: 349 [acc: 6%]\tLoss: 2.898606\n",
      "Train Epoch: 350 [acc: 13%]\tLoss: 2.654753\n",
      "[Trained for 350 epochs and tested on 5 sets of 2000 images]        Avg Acc: 12.05 +- 0.19 , Avg Loss: 2.39\n",
      "Train Epoch: 351 [acc: 10%]\tLoss: 2.504740\n",
      "Train Epoch: 352 [acc: 13%]\tLoss: 2.385286\n",
      "Train Epoch: 353 [acc: 12%]\tLoss: 2.601981\n",
      "Train Epoch: 354 [acc: 12%]\tLoss: 2.491493\n",
      "Train Epoch: 355 [acc: 12%]\tLoss: 2.883533\n",
      "Train Epoch: 356 [acc: 13%]\tLoss: 2.949444\n",
      "Train Epoch: 357 [acc: 9%]\tLoss: 2.529346\n",
      "Train Epoch: 358 [acc: 9%]\tLoss: 2.713572\n",
      "Train Epoch: 359 [acc: 13%]\tLoss: 2.455952\n",
      "Train Epoch: 360 [acc: 13%]\tLoss: 2.818832\n",
      "Train Epoch: 361 [acc: 9%]\tLoss: 2.628139\n",
      "Train Epoch: 362 [acc: 11%]\tLoss: 3.033123\n",
      "Train Epoch: 363 [acc: 12%]\tLoss: 2.836163\n",
      "Train Epoch: 364 [acc: 10%]\tLoss: 2.581667\n",
      "Train Epoch: 365 [acc: 11%]\tLoss: 2.580299\n",
      "Train Epoch: 366 [acc: 10%]\tLoss: 2.679166\n",
      "Train Epoch: 367 [acc: 10%]\tLoss: 2.948472\n",
      "Train Epoch: 368 [acc: 9%]\tLoss: 2.752243\n",
      "Train Epoch: 369 [acc: 11%]\tLoss: 2.656811\n",
      "Train Epoch: 370 [acc: 10%]\tLoss: 2.985666\n",
      "Train Epoch: 371 [acc: 11%]\tLoss: 2.981843\n",
      "Train Epoch: 372 [acc: 10%]\tLoss: 2.843325\n",
      "Train Epoch: 373 [acc: 13%]\tLoss: 2.691112\n",
      "Train Epoch: 374 [acc: 11%]\tLoss: 2.553073\n",
      "Train Epoch: 375 [acc: 10%]\tLoss: 2.569880\n",
      "[Trained for 375 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.85 +- 0.15 , Avg Loss: 2.37\n",
      "Train Epoch: 376 [acc: 10%]\tLoss: 2.876431\n",
      "Train Epoch: 377 [acc: 11%]\tLoss: 2.819800\n",
      "Train Epoch: 378 [acc: 15%]\tLoss: 2.620563\n",
      "Train Epoch: 379 [acc: 10%]\tLoss: 2.826262\n",
      "Train Epoch: 380 [acc: 11%]\tLoss: 2.850797\n",
      "Train Epoch: 381 [acc: 8%]\tLoss: 2.771507\n",
      "Train Epoch: 382 [acc: 11%]\tLoss: 2.517005\n",
      "Train Epoch: 383 [acc: 7%]\tLoss: 2.807322\n",
      "Train Epoch: 384 [acc: 10%]\tLoss: 2.602134\n",
      "Train Epoch: 385 [acc: 8%]\tLoss: 2.469221\n",
      "Train Epoch: 386 [acc: 11%]\tLoss: 2.931152\n",
      "Train Epoch: 387 [acc: 9%]\tLoss: 2.811392\n",
      "Train Epoch: 388 [acc: 14%]\tLoss: 2.870207\n",
      "Train Epoch: 389 [acc: 12%]\tLoss: 2.497414\n",
      "Train Epoch: 390 [acc: 6%]\tLoss: 2.956388\n",
      "Train Epoch: 391 [acc: 12%]\tLoss: 2.706749\n",
      "Train Epoch: 392 [acc: 10%]\tLoss: 2.669844\n",
      "Train Epoch: 393 [acc: 11%]\tLoss: 2.750645\n",
      "Train Epoch: 394 [acc: 10%]\tLoss: 2.783165\n",
      "Train Epoch: 395 [acc: 11%]\tLoss: 2.705818\n",
      "Train Epoch: 396 [acc: 8%]\tLoss: 2.744318\n",
      "Train Epoch: 397 [acc: 11%]\tLoss: 2.605773\n",
      "Train Epoch: 398 [acc: 12%]\tLoss: 2.605169\n",
      "Train Epoch: 399 [acc: 9%]\tLoss: 2.497822\n",
      "Train Epoch: 400 [acc: 10%]\tLoss: 2.724364\n",
      "[Trained for 400 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.74 +- 0.09 , Avg Loss: 2.36\n",
      "Train Epoch: 401 [acc: 8%]\tLoss: 2.614417\n",
      "Train Epoch: 402 [acc: 13%]\tLoss: 2.474429\n",
      "Train Epoch: 403 [acc: 13%]\tLoss: 2.669463\n",
      "Train Epoch: 404 [acc: 12%]\tLoss: 2.333098\n",
      "Train Epoch: 405 [acc: 13%]\tLoss: 2.368389\n",
      "Train Epoch: 406 [acc: 10%]\tLoss: 2.590423\n",
      "Train Epoch: 407 [acc: 11%]\tLoss: 2.538998\n",
      "Train Epoch: 408 [acc: 10%]\tLoss: 2.536110\n",
      "Train Epoch: 409 [acc: 13%]\tLoss: 2.718892\n",
      "Train Epoch: 410 [acc: 11%]\tLoss: 2.455311\n",
      "Train Epoch: 411 [acc: 11%]\tLoss: 2.686392\n",
      "Train Epoch: 412 [acc: 10%]\tLoss: 2.527541\n",
      "Train Epoch: 413 [acc: 10%]\tLoss: 2.538636\n",
      "Train Epoch: 414 [acc: 9%]\tLoss: 2.653684\n",
      "Train Epoch: 415 [acc: 12%]\tLoss: 2.669744\n",
      "Train Epoch: 416 [acc: 13%]\tLoss: 2.757123\n",
      "Train Epoch: 417 [acc: 13%]\tLoss: 2.439397\n",
      "Train Epoch: 418 [acc: 11%]\tLoss: 2.982949\n",
      "Train Epoch: 419 [acc: 9%]\tLoss: 2.593896\n",
      "Train Epoch: 420 [acc: 12%]\tLoss: 2.679291\n",
      "Train Epoch: 421 [acc: 12%]\tLoss: 2.446886\n",
      "Train Epoch: 422 [acc: 8%]\tLoss: 2.538716\n",
      "Train Epoch: 423 [acc: 14%]\tLoss: 2.552259\n",
      "Train Epoch: 424 [acc: 8%]\tLoss: 2.557589\n",
      "Train Epoch: 425 [acc: 12%]\tLoss: 3.034159\n",
      "[Trained for 425 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.60 +- 0.13 , Avg Loss: 2.35\n",
      "Train Epoch: 426 [acc: 11%]\tLoss: 2.587362\n",
      "Train Epoch: 427 [acc: 11%]\tLoss: 2.677565\n",
      "Train Epoch: 428 [acc: 12%]\tLoss: 2.552594\n",
      "Train Epoch: 429 [acc: 10%]\tLoss: 2.508487\n",
      "Train Epoch: 430 [acc: 11%]\tLoss: 2.425272\n",
      "Train Epoch: 431 [acc: 14%]\tLoss: 2.546365\n",
      "Train Epoch: 432 [acc: 13%]\tLoss: 2.302956\n",
      "Train Epoch: 433 [acc: 11%]\tLoss: 2.587787\n",
      "Train Epoch: 434 [acc: 9%]\tLoss: 2.954267\n",
      "Train Epoch: 435 [acc: 10%]\tLoss: 2.561595\n",
      "Train Epoch: 436 [acc: 12%]\tLoss: 2.960680\n",
      "Train Epoch: 437 [acc: 10%]\tLoss: 2.494704\n",
      "Train Epoch: 438 [acc: 11%]\tLoss: 2.698019\n",
      "Train Epoch: 439 [acc: 12%]\tLoss: 2.702722\n",
      "Train Epoch: 440 [acc: 14%]\tLoss: 2.733882\n",
      "Train Epoch: 441 [acc: 7%]\tLoss: 2.665137\n",
      "Train Epoch: 442 [acc: 10%]\tLoss: 2.664823\n",
      "Train Epoch: 443 [acc: 13%]\tLoss: 2.493045\n",
      "Train Epoch: 444 [acc: 10%]\tLoss: 2.486832\n",
      "Train Epoch: 445 [acc: 10%]\tLoss: 2.415993\n",
      "Train Epoch: 446 [acc: 12%]\tLoss: 2.570654\n",
      "Train Epoch: 447 [acc: 12%]\tLoss: 2.465370\n",
      "Train Epoch: 448 [acc: 9%]\tLoss: 2.726322\n",
      "Train Epoch: 449 [acc: 10%]\tLoss: 2.519790\n",
      "Train Epoch: 450 [acc: 11%]\tLoss: 2.498568\n",
      "[Trained for 450 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.53 +- 0.02 , Avg Loss: 2.34\n",
      "Train Epoch: 451 [acc: 11%]\tLoss: 2.574635\n",
      "Train Epoch: 452 [acc: 15%]\tLoss: 2.513135\n",
      "Train Epoch: 453 [acc: 13%]\tLoss: 2.262491\n",
      "Train Epoch: 454 [acc: 9%]\tLoss: 2.556183\n",
      "Train Epoch: 455 [acc: 10%]\tLoss: 2.552030\n",
      "Train Epoch: 456 [acc: 9%]\tLoss: 2.370528\n",
      "Train Epoch: 457 [acc: 10%]\tLoss: 2.566777\n",
      "Train Epoch: 458 [acc: 13%]\tLoss: 2.477095\n",
      "Train Epoch: 459 [acc: 10%]\tLoss: 2.569588\n",
      "Train Epoch: 460 [acc: 11%]\tLoss: 2.338695\n",
      "Train Epoch: 461 [acc: 9%]\tLoss: 2.815918\n",
      "Train Epoch: 462 [acc: 10%]\tLoss: 2.844695\n",
      "Train Epoch: 463 [acc: 11%]\tLoss: 2.472818\n",
      "Train Epoch: 464 [acc: 15%]\tLoss: 2.311074\n",
      "Train Epoch: 465 [acc: 14%]\tLoss: 2.640282\n",
      "Train Epoch: 466 [acc: 11%]\tLoss: 2.323275\n",
      "Train Epoch: 467 [acc: 11%]\tLoss: 2.523912\n",
      "Train Epoch: 468 [acc: 11%]\tLoss: 2.398187\n",
      "Train Epoch: 469 [acc: 10%]\tLoss: 2.577477\n",
      "Train Epoch: 470 [acc: 10%]\tLoss: 2.666335\n",
      "Train Epoch: 471 [acc: 8%]\tLoss: 2.411904\n",
      "Train Epoch: 472 [acc: 10%]\tLoss: 2.592883\n",
      "Train Epoch: 473 [acc: 13%]\tLoss: 2.437840\n",
      "Train Epoch: 474 [acc: 9%]\tLoss: 3.048355\n",
      "Train Epoch: 475 [acc: 11%]\tLoss: 2.486295\n",
      "[Trained for 475 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.41 +- 0.11 , Avg Loss: 2.34\n",
      "Train Epoch: 476 [acc: 12%]\tLoss: 2.629366\n",
      "Train Epoch: 477 [acc: 12%]\tLoss: 2.710123\n",
      "Train Epoch: 478 [acc: 12%]\tLoss: 2.478328\n",
      "Train Epoch: 479 [acc: 10%]\tLoss: 2.511002\n",
      "Train Epoch: 480 [acc: 10%]\tLoss: 2.361300\n",
      "Train Epoch: 481 [acc: 10%]\tLoss: 2.469370\n",
      "Train Epoch: 482 [acc: 12%]\tLoss: 2.367422\n",
      "Train Epoch: 483 [acc: 11%]\tLoss: 2.480994\n",
      "Train Epoch: 484 [acc: 11%]\tLoss: 2.389759\n",
      "Train Epoch: 485 [acc: 10%]\tLoss: 2.736988\n",
      "Train Epoch: 486 [acc: 11%]\tLoss: 2.428080\n",
      "Train Epoch: 487 [acc: 11%]\tLoss: 2.616847\n",
      "Train Epoch: 488 [acc: 11%]\tLoss: 2.660803\n",
      "Train Epoch: 489 [acc: 12%]\tLoss: 2.317854\n",
      "Train Epoch: 490 [acc: 11%]\tLoss: 2.402927\n",
      "Train Epoch: 491 [acc: 11%]\tLoss: 2.473582\n",
      "Train Epoch: 492 [acc: 12%]\tLoss: 2.460686\n",
      "Train Epoch: 493 [acc: 8%]\tLoss: 2.445005\n",
      "Train Epoch: 494 [acc: 11%]\tLoss: 2.453230\n",
      "Train Epoch: 495 [acc: 9%]\tLoss: 2.386410\n",
      "Train Epoch: 496 [acc: 10%]\tLoss: 2.765480\n",
      "Train Epoch: 497 [acc: 9%]\tLoss: 2.623645\n",
      "Train Epoch: 498 [acc: 12%]\tLoss: 2.552446\n",
      "Train Epoch: 499 [acc: 12%]\tLoss: 2.431615\n",
      "Train Epoch: 500 [acc: 10%]\tLoss: 2.381400\n",
      "[Trained for 500 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.33 +- 0.14 , Avg Loss: 2.33\n",
      "Train Epoch: 501 [acc: 10%]\tLoss: 2.571364\n",
      "Train Epoch: 502 [acc: 11%]\tLoss: 2.769697\n",
      "Train Epoch: 503 [acc: 11%]\tLoss: 2.697223\n",
      "Train Epoch: 504 [acc: 12%]\tLoss: 2.612240\n",
      "Train Epoch: 505 [acc: 12%]\tLoss: 2.335452\n",
      "Train Epoch: 506 [acc: 9%]\tLoss: 2.713546\n",
      "Train Epoch: 507 [acc: 13%]\tLoss: 2.461296\n",
      "Train Epoch: 508 [acc: 11%]\tLoss: 2.524679\n",
      "Train Epoch: 509 [acc: 11%]\tLoss: 2.554888\n",
      "Train Epoch: 510 [acc: 11%]\tLoss: 2.620383\n",
      "Train Epoch: 511 [acc: 12%]\tLoss: 2.631353\n",
      "Train Epoch: 512 [acc: 9%]\tLoss: 2.468726\n",
      "Train Epoch: 513 [acc: 9%]\tLoss: 2.539511\n",
      "Train Epoch: 514 [acc: 11%]\tLoss: 2.439458\n",
      "Train Epoch: 515 [acc: 10%]\tLoss: 2.523019\n",
      "Train Epoch: 516 [acc: 12%]\tLoss: 2.495772\n",
      "Train Epoch: 517 [acc: 13%]\tLoss: 2.510759\n",
      "Train Epoch: 518 [acc: 15%]\tLoss: 2.337760\n",
      "Train Epoch: 519 [acc: 11%]\tLoss: 2.360476\n",
      "Train Epoch: 520 [acc: 13%]\tLoss: 2.434438\n",
      "Train Epoch: 521 [acc: 9%]\tLoss: 2.412332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 522 [acc: 9%]\tLoss: 2.563887\n",
      "Train Epoch: 523 [acc: 10%]\tLoss: 2.700627\n",
      "Train Epoch: 524 [acc: 8%]\tLoss: 2.425941\n",
      "Train Epoch: 525 [acc: 9%]\tLoss: 2.327449\n",
      "[Trained for 525 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.25 +- 0.17 , Avg Loss: 2.32\n",
      "Train Epoch: 526 [acc: 11%]\tLoss: 2.382989\n",
      "Train Epoch: 527 [acc: 13%]\tLoss: 2.509565\n",
      "Train Epoch: 528 [acc: 10%]\tLoss: 2.482944\n",
      "Train Epoch: 529 [acc: 9%]\tLoss: 2.498296\n",
      "Train Epoch: 530 [acc: 11%]\tLoss: 2.461454\n",
      "Train Epoch: 531 [acc: 11%]\tLoss: 2.418592\n",
      "Train Epoch: 532 [acc: 9%]\tLoss: 2.353960\n",
      "Train Epoch: 533 [acc: 11%]\tLoss: 2.347179\n",
      "Train Epoch: 534 [acc: 10%]\tLoss: 2.334309\n",
      "Train Epoch: 535 [acc: 10%]\tLoss: 2.438868\n",
      "Train Epoch: 536 [acc: 14%]\tLoss: 2.535753\n",
      "Train Epoch: 537 [acc: 11%]\tLoss: 2.383984\n",
      "Train Epoch: 538 [acc: 10%]\tLoss: 2.545972\n",
      "Train Epoch: 539 [acc: 10%]\tLoss: 2.661451\n",
      "Train Epoch: 540 [acc: 9%]\tLoss: 2.586443\n",
      "Train Epoch: 541 [acc: 13%]\tLoss: 2.419617\n",
      "Train Epoch: 542 [acc: 13%]\tLoss: 2.285273\n",
      "Train Epoch: 543 [acc: 11%]\tLoss: 2.747884\n",
      "Train Epoch: 544 [acc: 11%]\tLoss: 2.388026\n",
      "Train Epoch: 545 [acc: 12%]\tLoss: 2.490253\n",
      "Train Epoch: 546 [acc: 12%]\tLoss: 2.412807\n",
      "Train Epoch: 547 [acc: 10%]\tLoss: 2.493366\n",
      "Train Epoch: 548 [acc: 11%]\tLoss: 2.431723\n",
      "Train Epoch: 549 [acc: 13%]\tLoss: 2.428394\n",
      "Train Epoch: 550 [acc: 12%]\tLoss: 2.467156\n",
      "[Trained for 550 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.20 +- 0.13 , Avg Loss: 2.32\n",
      "Train Epoch: 551 [acc: 11%]\tLoss: 2.399282\n",
      "Train Epoch: 552 [acc: 12%]\tLoss: 2.414061\n",
      "Train Epoch: 553 [acc: 8%]\tLoss: 2.478359\n",
      "Train Epoch: 554 [acc: 10%]\tLoss: 2.368834\n",
      "Train Epoch: 555 [acc: 9%]\tLoss: 2.494446\n",
      "Train Epoch: 556 [acc: 10%]\tLoss: 2.445504\n",
      "Train Epoch: 557 [acc: 11%]\tLoss: 2.346609\n",
      "Train Epoch: 558 [acc: 12%]\tLoss: 2.475170\n",
      "Train Epoch: 559 [acc: 9%]\tLoss: 2.358603\n",
      "Train Epoch: 560 [acc: 11%]\tLoss: 2.535104\n",
      "Train Epoch: 561 [acc: 16%]\tLoss: 2.268945\n",
      "Train Epoch: 562 [acc: 12%]\tLoss: 2.430196\n",
      "Train Epoch: 563 [acc: 14%]\tLoss: 2.373129\n",
      "Train Epoch: 564 [acc: 11%]\tLoss: 2.390835\n",
      "Train Epoch: 565 [acc: 12%]\tLoss: 2.509887\n",
      "Train Epoch: 566 [acc: 11%]\tLoss: 2.330068\n",
      "Train Epoch: 567 [acc: 12%]\tLoss: 2.868810\n",
      "Train Epoch: 568 [acc: 11%]\tLoss: 2.322437\n",
      "Train Epoch: 569 [acc: 10%]\tLoss: 2.447580\n",
      "Train Epoch: 570 [acc: 13%]\tLoss: 2.341369\n",
      "Train Epoch: 571 [acc: 8%]\tLoss: 2.421857\n",
      "Train Epoch: 572 [acc: 9%]\tLoss: 2.442434\n",
      "Train Epoch: 573 [acc: 9%]\tLoss: 2.524454\n",
      "Train Epoch: 574 [acc: 12%]\tLoss: 2.389594\n",
      "Train Epoch: 575 [acc: 11%]\tLoss: 2.490613\n",
      "[Trained for 575 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.22 +- 0.12 , Avg Loss: 2.32\n",
      "Train Epoch: 576 [acc: 11%]\tLoss: 2.685441\n",
      "Train Epoch: 577 [acc: 10%]\tLoss: 2.377496\n",
      "Train Epoch: 578 [acc: 13%]\tLoss: 2.378100\n",
      "Train Epoch: 579 [acc: 11%]\tLoss: 2.388864\n",
      "Train Epoch: 580 [acc: 11%]\tLoss: 2.435073\n",
      "Train Epoch: 581 [acc: 12%]\tLoss: 2.321092\n",
      "Train Epoch: 582 [acc: 12%]\tLoss: 2.358243\n",
      "Train Epoch: 583 [acc: 10%]\tLoss: 2.465377\n",
      "Train Epoch: 584 [acc: 12%]\tLoss: 2.426738\n",
      "Train Epoch: 585 [acc: 11%]\tLoss: 2.523246\n",
      "Train Epoch: 586 [acc: 10%]\tLoss: 2.463641\n",
      "Train Epoch: 587 [acc: 10%]\tLoss: 2.432747\n",
      "Train Epoch: 588 [acc: 12%]\tLoss: 2.467128\n",
      "Train Epoch: 589 [acc: 12%]\tLoss: 2.567250\n",
      "Train Epoch: 590 [acc: 11%]\tLoss: 2.438308\n",
      "Train Epoch: 591 [acc: 12%]\tLoss: 2.333644\n",
      "Train Epoch: 592 [acc: 11%]\tLoss: 2.456195\n",
      "Train Epoch: 593 [acc: 11%]\tLoss: 2.358136\n",
      "Train Epoch: 594 [acc: 11%]\tLoss: 2.354548\n",
      "Train Epoch: 595 [acc: 11%]\tLoss: 2.526783\n",
      "Train Epoch: 596 [acc: 12%]\tLoss: 2.383414\n",
      "Train Epoch: 597 [acc: 9%]\tLoss: 2.418911\n",
      "Train Epoch: 598 [acc: 9%]\tLoss: 2.323230\n",
      "Train Epoch: 599 [acc: 10%]\tLoss: 2.487889\n",
      "Train Epoch: 600 [acc: 9%]\tLoss: 2.426426\n",
      "[Trained for 600 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.21 +- 0.14 , Avg Loss: 2.31\n",
      "Train Epoch: 601 [acc: 12%]\tLoss: 2.345622\n",
      "Train Epoch: 602 [acc: 12%]\tLoss: 2.604287\n",
      "Train Epoch: 603 [acc: 9%]\tLoss: 2.596234\n",
      "Train Epoch: 604 [acc: 10%]\tLoss: 2.402710\n",
      "Train Epoch: 605 [acc: 10%]\tLoss: 2.352404\n",
      "Train Epoch: 606 [acc: 10%]\tLoss: 2.352202\n",
      "Train Epoch: 607 [acc: 11%]\tLoss: 2.255163\n",
      "Train Epoch: 608 [acc: 9%]\tLoss: 2.489695\n",
      "Train Epoch: 609 [acc: 10%]\tLoss: 2.361504\n",
      "Train Epoch: 610 [acc: 11%]\tLoss: 2.626294\n",
      "Train Epoch: 611 [acc: 10%]\tLoss: 2.415314\n",
      "Train Epoch: 612 [acc: 11%]\tLoss: 2.453013\n",
      "Train Epoch: 613 [acc: 11%]\tLoss: 2.416441\n",
      "Train Epoch: 614 [acc: 10%]\tLoss: 2.485351\n",
      "Train Epoch: 615 [acc: 9%]\tLoss: 2.318372\n",
      "Train Epoch: 616 [acc: 11%]\tLoss: 2.284096\n",
      "Train Epoch: 617 [acc: 11%]\tLoss: 2.410658\n",
      "Train Epoch: 618 [acc: 9%]\tLoss: 2.516494\n",
      "Train Epoch: 619 [acc: 13%]\tLoss: 2.293447\n",
      "Train Epoch: 620 [acc: 12%]\tLoss: 2.333777\n",
      "Train Epoch: 621 [acc: 14%]\tLoss: 2.370793\n",
      "Train Epoch: 622 [acc: 9%]\tLoss: 2.353015\n",
      "Train Epoch: 623 [acc: 9%]\tLoss: 2.440051\n",
      "Train Epoch: 624 [acc: 11%]\tLoss: 2.344421\n",
      "Train Epoch: 625 [acc: 11%]\tLoss: 2.327878\n",
      "[Trained for 625 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.20 +- 0.12 , Avg Loss: 2.31\n",
      "Train Epoch: 626 [acc: 9%]\tLoss: 2.337900\n",
      "Train Epoch: 627 [acc: 11%]\tLoss: 2.415976\n",
      "Train Epoch: 628 [acc: 10%]\tLoss: 2.505779\n",
      "Train Epoch: 629 [acc: 10%]\tLoss: 2.446026\n",
      "Train Epoch: 630 [acc: 12%]\tLoss: 2.269686\n",
      "Train Epoch: 631 [acc: 8%]\tLoss: 2.436994\n",
      "Train Epoch: 632 [acc: 13%]\tLoss: 2.349352\n",
      "Train Epoch: 633 [acc: 10%]\tLoss: 2.476550\n",
      "Train Epoch: 634 [acc: 11%]\tLoss: 2.301867\n",
      "Train Epoch: 635 [acc: 10%]\tLoss: 2.407603\n",
      "Train Epoch: 636 [acc: 11%]\tLoss: 2.437328\n",
      "Train Epoch: 637 [acc: 12%]\tLoss: 2.316821\n",
      "Train Epoch: 638 [acc: 10%]\tLoss: 2.355452\n",
      "Train Epoch: 639 [acc: 14%]\tLoss: 2.275676\n",
      "Train Epoch: 640 [acc: 10%]\tLoss: 2.348849\n",
      "Train Epoch: 641 [acc: 8%]\tLoss: 2.441957\n",
      "Train Epoch: 642 [acc: 11%]\tLoss: 2.323997\n",
      "Train Epoch: 643 [acc: 11%]\tLoss: 2.377478\n",
      "Train Epoch: 644 [acc: 10%]\tLoss: 2.374907\n",
      "Train Epoch: 645 [acc: 10%]\tLoss: 2.528331\n",
      "Train Epoch: 646 [acc: 11%]\tLoss: 2.481616\n",
      "Train Epoch: 647 [acc: 9%]\tLoss: 2.464931\n",
      "Train Epoch: 648 [acc: 12%]\tLoss: 2.446644\n",
      "Train Epoch: 649 [acc: 9%]\tLoss: 2.338668\n",
      "Train Epoch: 650 [acc: 10%]\tLoss: 2.358475\n",
      "[Trained for 650 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.16 +- 0.13 , Avg Loss: 2.31\n",
      "Train Epoch: 651 [acc: 10%]\tLoss: 2.316450\n",
      "Train Epoch: 652 [acc: 11%]\tLoss: 2.307071\n",
      "Train Epoch: 653 [acc: 11%]\tLoss: 2.445463\n",
      "Train Epoch: 654 [acc: 11%]\tLoss: 2.311059\n",
      "Train Epoch: 655 [acc: 11%]\tLoss: 2.358847\n",
      "Train Epoch: 656 [acc: 9%]\tLoss: 2.386977\n",
      "Train Epoch: 657 [acc: 11%]\tLoss: 2.318860\n",
      "Train Epoch: 658 [acc: 11%]\tLoss: 2.329862\n",
      "Train Epoch: 659 [acc: 14%]\tLoss: 2.370387\n",
      "Train Epoch: 660 [acc: 9%]\tLoss: 2.347512\n",
      "Train Epoch: 661 [acc: 10%]\tLoss: 2.318247\n",
      "Train Epoch: 662 [acc: 7%]\tLoss: 2.557177\n",
      "Train Epoch: 663 [acc: 11%]\tLoss: 2.301771\n",
      "Train Epoch: 664 [acc: 12%]\tLoss: 2.272898\n",
      "Train Epoch: 665 [acc: 11%]\tLoss: 2.369019\n",
      "Train Epoch: 666 [acc: 10%]\tLoss: 2.393282\n",
      "Train Epoch: 667 [acc: 10%]\tLoss: 2.482852\n",
      "Train Epoch: 668 [acc: 10%]\tLoss: 2.442437\n",
      "Train Epoch: 669 [acc: 9%]\tLoss: 2.389942\n",
      "Train Epoch: 670 [acc: 11%]\tLoss: 2.397559\n",
      "Train Epoch: 671 [acc: 12%]\tLoss: 2.384933\n",
      "Train Epoch: 672 [acc: 9%]\tLoss: 2.420550\n",
      "Train Epoch: 673 [acc: 9%]\tLoss: 2.354238\n",
      "Train Epoch: 674 [acc: 10%]\tLoss: 2.401930\n",
      "Train Epoch: 675 [acc: 11%]\tLoss: 2.520864\n",
      "[Trained for 675 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.10 +- 0.13 , Avg Loss: 2.31\n",
      "Train Epoch: 676 [acc: 10%]\tLoss: 2.617326\n",
      "Train Epoch: 677 [acc: 11%]\tLoss: 2.514987\n",
      "Train Epoch: 678 [acc: 9%]\tLoss: 2.346237\n",
      "Train Epoch: 679 [acc: 12%]\tLoss: 2.342900\n",
      "Train Epoch: 680 [acc: 10%]\tLoss: 2.524184\n",
      "Train Epoch: 681 [acc: 9%]\tLoss: 2.394274\n",
      "Train Epoch: 682 [acc: 11%]\tLoss: 2.367976\n",
      "Train Epoch: 683 [acc: 11%]\tLoss: 2.465418\n",
      "Train Epoch: 684 [acc: 10%]\tLoss: 2.368728\n",
      "Train Epoch: 685 [acc: 10%]\tLoss: 2.348407\n",
      "Train Epoch: 686 [acc: 11%]\tLoss: 2.318917\n",
      "Train Epoch: 687 [acc: 10%]\tLoss: 2.419858\n",
      "Train Epoch: 688 [acc: 12%]\tLoss: 2.294370\n",
      "Train Epoch: 689 [acc: 12%]\tLoss: 2.286222\n",
      "Train Epoch: 690 [acc: 11%]\tLoss: 2.307898\n",
      "Train Epoch: 691 [acc: 10%]\tLoss: 2.377158\n",
      "Train Epoch: 692 [acc: 11%]\tLoss: 2.310729\n",
      "Train Epoch: 693 [acc: 12%]\tLoss: 2.298388\n",
      "Train Epoch: 694 [acc: 9%]\tLoss: 2.369213\n",
      "Train Epoch: 695 [acc: 10%]\tLoss: 2.484590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 696 [acc: 8%]\tLoss: 2.487889\n",
      "Train Epoch: 697 [acc: 12%]\tLoss: 2.397910\n",
      "Train Epoch: 698 [acc: 9%]\tLoss: 2.425637\n",
      "Train Epoch: 699 [acc: 12%]\tLoss: 2.390606\n",
      "Train Epoch: 700 [acc: 10%]\tLoss: 2.529527\n",
      "[Trained for 700 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.11 +- 0.15 , Avg Loss: 2.31\n",
      "Train Epoch: 701 [acc: 11%]\tLoss: 2.386973\n",
      "Train Epoch: 702 [acc: 10%]\tLoss: 2.490532\n",
      "Train Epoch: 703 [acc: 10%]\tLoss: 2.366021\n",
      "Train Epoch: 704 [acc: 11%]\tLoss: 2.404841\n",
      "Train Epoch: 705 [acc: 11%]\tLoss: 2.407950\n",
      "Train Epoch: 706 [acc: 13%]\tLoss: 2.388613\n",
      "Train Epoch: 707 [acc: 10%]\tLoss: 2.435801\n",
      "Train Epoch: 708 [acc: 11%]\tLoss: 2.256011\n",
      "Train Epoch: 709 [acc: 10%]\tLoss: 2.370331\n",
      "Train Epoch: 710 [acc: 11%]\tLoss: 2.380036\n",
      "Train Epoch: 711 [acc: 11%]\tLoss: 2.340738\n",
      "Train Epoch: 712 [acc: 11%]\tLoss: 2.331635\n",
      "Train Epoch: 713 [acc: 10%]\tLoss: 2.548195\n",
      "Train Epoch: 714 [acc: 10%]\tLoss: 2.476269\n",
      "Train Epoch: 715 [acc: 9%]\tLoss: 2.400183\n",
      "Train Epoch: 716 [acc: 12%]\tLoss: 2.350704\n",
      "Train Epoch: 717 [acc: 10%]\tLoss: 2.433204\n",
      "Train Epoch: 718 [acc: 12%]\tLoss: 2.302725\n",
      "Train Epoch: 719 [acc: 13%]\tLoss: 2.435119\n",
      "Train Epoch: 720 [acc: 11%]\tLoss: 2.326244\n",
      "Train Epoch: 721 [acc: 10%]\tLoss: 2.405745\n",
      "Train Epoch: 722 [acc: 11%]\tLoss: 2.372332\n",
      "Train Epoch: 723 [acc: 11%]\tLoss: 2.357974\n",
      "Train Epoch: 724 [acc: 12%]\tLoss: 2.365144\n",
      "Train Epoch: 725 [acc: 12%]\tLoss: 2.269048\n",
      "[Trained for 725 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.11 +- 0.17 , Avg Loss: 2.30\n",
      "Train Epoch: 726 [acc: 10%]\tLoss: 2.345632\n",
      "Train Epoch: 727 [acc: 9%]\tLoss: 2.497575\n",
      "Train Epoch: 728 [acc: 9%]\tLoss: 2.475779\n",
      "Train Epoch: 729 [acc: 13%]\tLoss: 2.343593\n",
      "Train Epoch: 730 [acc: 9%]\tLoss: 2.359678\n",
      "Train Epoch: 731 [acc: 9%]\tLoss: 2.405177\n",
      "Train Epoch: 732 [acc: 8%]\tLoss: 2.372303\n",
      "Train Epoch: 733 [acc: 12%]\tLoss: 2.263782\n",
      "Train Epoch: 734 [acc: 11%]\tLoss: 2.428289\n",
      "Train Epoch: 735 [acc: 12%]\tLoss: 2.346534\n",
      "Train Epoch: 736 [acc: 11%]\tLoss: 2.342978\n",
      "Train Epoch: 737 [acc: 11%]\tLoss: 2.318597\n",
      "Train Epoch: 738 [acc: 13%]\tLoss: 2.300530\n",
      "Train Epoch: 739 [acc: 11%]\tLoss: 2.319668\n",
      "Train Epoch: 740 [acc: 11%]\tLoss: 2.443960\n",
      "Train Epoch: 741 [acc: 10%]\tLoss: 2.370414\n",
      "Train Epoch: 742 [acc: 9%]\tLoss: 2.456101\n",
      "Train Epoch: 743 [acc: 10%]\tLoss: 2.349723\n",
      "Train Epoch: 744 [acc: 9%]\tLoss: 2.501680\n",
      "Train Epoch: 745 [acc: 12%]\tLoss: 2.414064\n",
      "Train Epoch: 746 [acc: 11%]\tLoss: 2.283870\n",
      "Train Epoch: 747 [acc: 12%]\tLoss: 2.335787\n",
      "Train Epoch: 748 [acc: 10%]\tLoss: 2.327818\n",
      "Train Epoch: 749 [acc: 10%]\tLoss: 2.354911\n",
      "Train Epoch: 750 [acc: 11%]\tLoss: 2.376978\n",
      "[Trained for 750 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.11 +- 0.17 , Avg Loss: 2.30\n",
      "Train Epoch: 751 [acc: 11%]\tLoss: 2.479434\n",
      "Train Epoch: 752 [acc: 10%]\tLoss: 2.343381\n",
      "Train Epoch: 753 [acc: 10%]\tLoss: 2.360635\n",
      "Train Epoch: 754 [acc: 13%]\tLoss: 2.304599\n",
      "Train Epoch: 755 [acc: 12%]\tLoss: 2.331077\n",
      "Train Epoch: 756 [acc: 10%]\tLoss: 2.466691\n",
      "Train Epoch: 757 [acc: 11%]\tLoss: 2.363900\n",
      "Train Epoch: 758 [acc: 11%]\tLoss: 2.416157\n",
      "Train Epoch: 759 [acc: 11%]\tLoss: 2.357511\n",
      "Train Epoch: 760 [acc: 12%]\tLoss: 2.382927\n",
      "Train Epoch: 761 [acc: 11%]\tLoss: 2.360140\n",
      "Train Epoch: 762 [acc: 10%]\tLoss: 2.465137\n",
      "Train Epoch: 763 [acc: 10%]\tLoss: 2.383590\n",
      "Train Epoch: 764 [acc: 11%]\tLoss: 2.327575\n",
      "Train Epoch: 765 [acc: 8%]\tLoss: 2.370847\n",
      "Train Epoch: 766 [acc: 10%]\tLoss: 2.329354\n",
      "Train Epoch: 767 [acc: 11%]\tLoss: 2.320085\n",
      "Train Epoch: 768 [acc: 10%]\tLoss: 2.378052\n",
      "Train Epoch: 769 [acc: 10%]\tLoss: 2.387752\n",
      "Train Epoch: 770 [acc: 11%]\tLoss: 2.318141\n",
      "Train Epoch: 771 [acc: 13%]\tLoss: 2.340767\n",
      "Train Epoch: 772 [acc: 12%]\tLoss: 2.372337\n",
      "Train Epoch: 773 [acc: 12%]\tLoss: 2.489641\n",
      "Train Epoch: 774 [acc: 10%]\tLoss: 2.437658\n",
      "Train Epoch: 775 [acc: 9%]\tLoss: 2.424591\n",
      "[Trained for 775 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.13 +- 0.17 , Avg Loss: 2.30\n",
      "Train Epoch: 776 [acc: 10%]\tLoss: 2.328443\n",
      "Train Epoch: 777 [acc: 11%]\tLoss: 2.459160\n",
      "Train Epoch: 778 [acc: 10%]\tLoss: 2.296256\n",
      "Train Epoch: 779 [acc: 10%]\tLoss: 2.352985\n",
      "Train Epoch: 780 [acc: 9%]\tLoss: 2.375549\n",
      "Train Epoch: 781 [acc: 9%]\tLoss: 2.345837\n",
      "Train Epoch: 782 [acc: 11%]\tLoss: 2.294236\n",
      "Train Epoch: 783 [acc: 11%]\tLoss: 2.488346\n",
      "Train Epoch: 784 [acc: 10%]\tLoss: 2.309056\n",
      "Train Epoch: 785 [acc: 11%]\tLoss: 2.370321\n",
      "Train Epoch: 786 [acc: 10%]\tLoss: 2.308901\n",
      "Train Epoch: 787 [acc: 10%]\tLoss: 2.467834\n",
      "Train Epoch: 788 [acc: 11%]\tLoss: 2.288050\n",
      "Train Epoch: 789 [acc: 12%]\tLoss: 2.346574\n",
      "Train Epoch: 790 [acc: 11%]\tLoss: 2.444338\n",
      "Train Epoch: 791 [acc: 9%]\tLoss: 2.306562\n",
      "Train Epoch: 792 [acc: 11%]\tLoss: 2.320982\n",
      "Train Epoch: 793 [acc: 9%]\tLoss: 2.390460\n",
      "Train Epoch: 794 [acc: 10%]\tLoss: 2.372635\n",
      "Train Epoch: 795 [acc: 12%]\tLoss: 2.480246\n",
      "Train Epoch: 796 [acc: 10%]\tLoss: 2.377455\n",
      "Train Epoch: 797 [acc: 10%]\tLoss: 2.384388\n",
      "Train Epoch: 798 [acc: 8%]\tLoss: 2.403055\n",
      "Train Epoch: 799 [acc: 12%]\tLoss: 2.342538\n",
      "Train Epoch: 800 [acc: 9%]\tLoss: 2.393803\n",
      "[Trained for 800 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.12 +- 0.17 , Avg Loss: 2.30\n",
      "Train Epoch: 801 [acc: 10%]\tLoss: 2.471699\n",
      "Train Epoch: 802 [acc: 10%]\tLoss: 2.406668\n",
      "Train Epoch: 803 [acc: 10%]\tLoss: 2.342706\n",
      "Train Epoch: 804 [acc: 11%]\tLoss: 2.548092\n",
      "Train Epoch: 805 [acc: 11%]\tLoss: 2.301618\n",
      "Train Epoch: 806 [acc: 11%]\tLoss: 2.379205\n",
      "Train Epoch: 807 [acc: 12%]\tLoss: 2.351447\n",
      "Train Epoch: 808 [acc: 12%]\tLoss: 2.318608\n",
      "Train Epoch: 809 [acc: 10%]\tLoss: 2.336229\n",
      "Train Epoch: 810 [acc: 9%]\tLoss: 2.326068\n",
      "Train Epoch: 811 [acc: 12%]\tLoss: 2.417911\n",
      "Train Epoch: 812 [acc: 10%]\tLoss: 2.359789\n",
      "Train Epoch: 813 [acc: 12%]\tLoss: 2.396017\n",
      "Train Epoch: 814 [acc: 11%]\tLoss: 2.382432\n",
      "Train Epoch: 815 [acc: 11%]\tLoss: 2.337870\n",
      "Train Epoch: 816 [acc: 11%]\tLoss: 2.316716\n",
      "Train Epoch: 817 [acc: 10%]\tLoss: 2.418745\n",
      "Train Epoch: 818 [acc: 10%]\tLoss: 2.321099\n",
      "Train Epoch: 819 [acc: 9%]\tLoss: 2.377249\n",
      "Train Epoch: 820 [acc: 14%]\tLoss: 2.296629\n",
      "Train Epoch: 821 [acc: 11%]\tLoss: 2.423863\n",
      "Train Epoch: 822 [acc: 9%]\tLoss: 2.424769\n",
      "Train Epoch: 823 [acc: 11%]\tLoss: 2.544171\n",
      "Train Epoch: 824 [acc: 11%]\tLoss: 2.316775\n",
      "Train Epoch: 825 [acc: 10%]\tLoss: 2.331278\n",
      "[Trained for 825 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.12 +- 0.15 , Avg Loss: 2.30\n",
      "Train Epoch: 826 [acc: 10%]\tLoss: 2.338792\n",
      "Train Epoch: 827 [acc: 12%]\tLoss: 2.378706\n",
      "Train Epoch: 828 [acc: 12%]\tLoss: 2.311926\n",
      "Train Epoch: 829 [acc: 10%]\tLoss: 2.347244\n",
      "Train Epoch: 830 [acc: 12%]\tLoss: 2.395278\n",
      "Train Epoch: 831 [acc: 10%]\tLoss: 2.304302\n",
      "Train Epoch: 832 [acc: 11%]\tLoss: 2.309218\n",
      "Train Epoch: 833 [acc: 10%]\tLoss: 2.315206\n",
      "Train Epoch: 834 [acc: 11%]\tLoss: 2.381722\n",
      "Train Epoch: 835 [acc: 11%]\tLoss: 2.339127\n",
      "Train Epoch: 836 [acc: 11%]\tLoss: 2.450609\n",
      "Train Epoch: 837 [acc: 10%]\tLoss: 2.314220\n",
      "Train Epoch: 838 [acc: 11%]\tLoss: 2.389959\n",
      "Train Epoch: 839 [acc: 12%]\tLoss: 2.287397\n",
      "Train Epoch: 840 [acc: 11%]\tLoss: 2.431196\n",
      "Train Epoch: 841 [acc: 11%]\tLoss: 2.456173\n",
      "Train Epoch: 842 [acc: 11%]\tLoss: 2.310418\n",
      "Train Epoch: 843 [acc: 10%]\tLoss: 2.344209\n",
      "Train Epoch: 844 [acc: 12%]\tLoss: 2.285349\n",
      "Train Epoch: 845 [acc: 11%]\tLoss: 2.351746\n",
      "Train Epoch: 846 [acc: 11%]\tLoss: 2.350244\n",
      "Train Epoch: 847 [acc: 9%]\tLoss: 2.402786\n",
      "Train Epoch: 848 [acc: 13%]\tLoss: 2.375848\n",
      "Train Epoch: 849 [acc: 9%]\tLoss: 2.303747\n",
      "Train Epoch: 850 [acc: 11%]\tLoss: 2.286182\n",
      "[Trained for 850 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.08 +- 0.19 , Avg Loss: 2.30\n",
      "Train Epoch: 851 [acc: 9%]\tLoss: 2.363884\n",
      "Train Epoch: 852 [acc: 13%]\tLoss: 2.253669\n",
      "Train Epoch: 853 [acc: 10%]\tLoss: 2.306130\n",
      "Train Epoch: 854 [acc: 10%]\tLoss: 2.345301\n",
      "Train Epoch: 855 [acc: 12%]\tLoss: 2.286703\n",
      "Train Epoch: 856 [acc: 12%]\tLoss: 2.306907\n",
      "Train Epoch: 857 [acc: 11%]\tLoss: 2.335023\n",
      "Train Epoch: 858 [acc: 10%]\tLoss: 2.296643\n",
      "Train Epoch: 859 [acc: 10%]\tLoss: 2.391059\n",
      "Train Epoch: 860 [acc: 11%]\tLoss: 2.301639\n",
      "Train Epoch: 861 [acc: 10%]\tLoss: 2.308510\n",
      "Train Epoch: 862 [acc: 13%]\tLoss: 2.267760\n",
      "Train Epoch: 863 [acc: 11%]\tLoss: 2.320375\n",
      "Train Epoch: 864 [acc: 11%]\tLoss: 2.353744\n",
      "Train Epoch: 865 [acc: 9%]\tLoss: 2.319557\n",
      "Train Epoch: 866 [acc: 11%]\tLoss: 2.290461\n",
      "Train Epoch: 867 [acc: 10%]\tLoss: 2.420795\n",
      "Train Epoch: 868 [acc: 11%]\tLoss: 2.398275\n",
      "Train Epoch: 869 [acc: 11%]\tLoss: 2.383067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 870 [acc: 11%]\tLoss: 2.348017\n",
      "Train Epoch: 871 [acc: 12%]\tLoss: 2.286249\n",
      "Train Epoch: 872 [acc: 9%]\tLoss: 2.504167\n",
      "Train Epoch: 873 [acc: 13%]\tLoss: 2.307657\n",
      "Train Epoch: 874 [acc: 10%]\tLoss: 2.431086\n",
      "Train Epoch: 875 [acc: 11%]\tLoss: 2.345846\n",
      "[Trained for 875 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.07 +- 0.19 , Avg Loss: 2.30\n",
      "Train Epoch: 876 [acc: 13%]\tLoss: 2.400087\n",
      "Train Epoch: 877 [acc: 10%]\tLoss: 2.344926\n",
      "Train Epoch: 878 [acc: 10%]\tLoss: 2.386945\n",
      "Train Epoch: 879 [acc: 11%]\tLoss: 2.381140\n",
      "Train Epoch: 880 [acc: 10%]\tLoss: 2.311771\n",
      "Train Epoch: 881 [acc: 10%]\tLoss: 2.385614\n",
      "Train Epoch: 882 [acc: 9%]\tLoss: 2.325305\n",
      "Train Epoch: 883 [acc: 10%]\tLoss: 2.304736\n",
      "Train Epoch: 884 [acc: 10%]\tLoss: 2.373249\n",
      "Train Epoch: 885 [acc: 10%]\tLoss: 2.310162\n",
      "Train Epoch: 886 [acc: 10%]\tLoss: 2.393570\n",
      "Train Epoch: 887 [acc: 11%]\tLoss: 2.345614\n",
      "Train Epoch: 888 [acc: 9%]\tLoss: 2.392345\n",
      "Train Epoch: 889 [acc: 10%]\tLoss: 2.334359\n",
      "Train Epoch: 890 [acc: 13%]\tLoss: 2.294270\n",
      "Train Epoch: 891 [acc: 10%]\tLoss: 2.324240\n",
      "Train Epoch: 892 [acc: 12%]\tLoss: 2.280545\n",
      "Train Epoch: 893 [acc: 10%]\tLoss: 2.559525\n",
      "Train Epoch: 894 [acc: 10%]\tLoss: 2.434497\n",
      "Train Epoch: 895 [acc: 11%]\tLoss: 2.291015\n",
      "Train Epoch: 896 [acc: 10%]\tLoss: 2.310290\n",
      "Train Epoch: 897 [acc: 11%]\tLoss: 2.369569\n",
      "Train Epoch: 898 [acc: 10%]\tLoss: 2.339275\n",
      "Train Epoch: 899 [acc: 12%]\tLoss: 2.325374\n",
      "Train Epoch: 900 [acc: 10%]\tLoss: 2.413240\n",
      "[Trained for 900 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.06 +- 0.16 , Avg Loss: 2.30\n",
      "Train Epoch: 901 [acc: 11%]\tLoss: 2.380348\n",
      "Train Epoch: 902 [acc: 10%]\tLoss: 2.347996\n",
      "Train Epoch: 903 [acc: 11%]\tLoss: 2.322110\n",
      "Train Epoch: 904 [acc: 10%]\tLoss: 2.322940\n",
      "Train Epoch: 905 [acc: 11%]\tLoss: 2.405414\n",
      "Train Epoch: 906 [acc: 11%]\tLoss: 2.293330\n",
      "Train Epoch: 907 [acc: 8%]\tLoss: 2.395847\n",
      "Train Epoch: 908 [acc: 9%]\tLoss: 2.306443\n",
      "Train Epoch: 909 [acc: 12%]\tLoss: 2.310519\n",
      "Train Epoch: 910 [acc: 11%]\tLoss: 2.395782\n",
      "Train Epoch: 911 [acc: 10%]\tLoss: 2.380617\n",
      "Train Epoch: 912 [acc: 11%]\tLoss: 2.388743\n",
      "Train Epoch: 913 [acc: 12%]\tLoss: 2.292197\n",
      "Train Epoch: 914 [acc: 11%]\tLoss: 2.328644\n",
      "Train Epoch: 915 [acc: 11%]\tLoss: 2.280744\n",
      "Train Epoch: 916 [acc: 11%]\tLoss: 2.300533\n",
      "Train Epoch: 917 [acc: 8%]\tLoss: 2.503223\n",
      "Train Epoch: 918 [acc: 10%]\tLoss: 2.367103\n",
      "Train Epoch: 919 [acc: 10%]\tLoss: 2.329913\n",
      "Train Epoch: 920 [acc: 11%]\tLoss: 2.339308\n",
      "Train Epoch: 921 [acc: 9%]\tLoss: 2.422770\n",
      "Train Epoch: 922 [acc: 10%]\tLoss: 2.338527\n",
      "Train Epoch: 923 [acc: 10%]\tLoss: 2.334853\n",
      "Train Epoch: 924 [acc: 10%]\tLoss: 2.382092\n",
      "Train Epoch: 925 [acc: 10%]\tLoss: 2.311648\n",
      "[Trained for 925 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.05 +- 0.16 , Avg Loss: 2.30\n",
      "Train Epoch: 926 [acc: 10%]\tLoss: 2.451417\n",
      "Train Epoch: 927 [acc: 11%]\tLoss: 2.361800\n",
      "Train Epoch: 928 [acc: 10%]\tLoss: 2.345762\n",
      "Train Epoch: 929 [acc: 9%]\tLoss: 2.382727\n",
      "Train Epoch: 930 [acc: 9%]\tLoss: 2.419143\n",
      "Train Epoch: 931 [acc: 10%]\tLoss: 2.321907\n",
      "Train Epoch: 932 [acc: 11%]\tLoss: 2.286981\n",
      "Train Epoch: 933 [acc: 11%]\tLoss: 2.421408\n",
      "Train Epoch: 934 [acc: 9%]\tLoss: 2.343268\n",
      "Train Epoch: 935 [acc: 11%]\tLoss: 2.386514\n",
      "Train Epoch: 936 [acc: 10%]\tLoss: 2.307075\n",
      "Train Epoch: 937 [acc: 10%]\tLoss: 2.299050\n",
      "Train Epoch: 938 [acc: 10%]\tLoss: 2.310888\n",
      "Train Epoch: 939 [acc: 10%]\tLoss: 2.445640\n",
      "Train Epoch: 940 [acc: 11%]\tLoss: 2.407382\n",
      "Train Epoch: 941 [acc: 9%]\tLoss: 2.424056\n",
      "Train Epoch: 942 [acc: 10%]\tLoss: 2.320871\n",
      "Train Epoch: 943 [acc: 9%]\tLoss: 2.370387\n",
      "Train Epoch: 944 [acc: 11%]\tLoss: 2.355048\n",
      "Train Epoch: 945 [acc: 10%]\tLoss: 2.322390\n",
      "Train Epoch: 946 [acc: 10%]\tLoss: 2.390531\n",
      "Train Epoch: 947 [acc: 10%]\tLoss: 2.324827\n",
      "Train Epoch: 948 [acc: 10%]\tLoss: 2.297085\n",
      "Train Epoch: 949 [acc: 10%]\tLoss: 2.432262\n",
      "Train Epoch: 950 [acc: 11%]\tLoss: 2.423956\n",
      "[Trained for 950 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.12 +- 0.14 , Avg Loss: 2.30\n",
      "Train Epoch: 951 [acc: 10%]\tLoss: 2.371967\n",
      "Train Epoch: 952 [acc: 11%]\tLoss: 2.319557\n",
      "Train Epoch: 953 [acc: 12%]\tLoss: 2.302406\n",
      "Train Epoch: 954 [acc: 11%]\tLoss: 2.295833\n",
      "Train Epoch: 955 [acc: 10%]\tLoss: 2.348629\n",
      "Train Epoch: 956 [acc: 10%]\tLoss: 2.317133\n",
      "Train Epoch: 957 [acc: 11%]\tLoss: 2.301884\n",
      "Train Epoch: 958 [acc: 9%]\tLoss: 2.312616\n",
      "Train Epoch: 959 [acc: 12%]\tLoss: 2.322730\n",
      "Train Epoch: 960 [acc: 10%]\tLoss: 2.353533\n",
      "Train Epoch: 961 [acc: 10%]\tLoss: 2.325182\n",
      "Train Epoch: 962 [acc: 12%]\tLoss: 2.281789\n",
      "Train Epoch: 963 [acc: 10%]\tLoss: 2.308774\n",
      "Train Epoch: 964 [acc: 9%]\tLoss: 2.357788\n",
      "Train Epoch: 965 [acc: 10%]\tLoss: 2.312670\n",
      "Train Epoch: 966 [acc: 11%]\tLoss: 2.309422\n",
      "Train Epoch: 967 [acc: 10%]\tLoss: 2.285822\n",
      "Train Epoch: 968 [acc: 13%]\tLoss: 2.354769\n",
      "Train Epoch: 969 [acc: 11%]\tLoss: 2.341715\n",
      "Train Epoch: 970 [acc: 13%]\tLoss: 2.314453\n",
      "Train Epoch: 971 [acc: 10%]\tLoss: 2.302585\n",
      "Train Epoch: 972 [acc: 9%]\tLoss: 2.349497\n",
      "Train Epoch: 973 [acc: 12%]\tLoss: 2.255640\n",
      "Train Epoch: 974 [acc: 10%]\tLoss: 2.353806\n",
      "Train Epoch: 975 [acc: 11%]\tLoss: 2.364361\n",
      "[Trained for 975 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.12 +- 0.13 , Avg Loss: 2.30\n",
      "Train Epoch: 976 [acc: 9%]\tLoss: 2.392629\n",
      "Train Epoch: 977 [acc: 10%]\tLoss: 2.387463\n",
      "Train Epoch: 978 [acc: 9%]\tLoss: 2.308203\n",
      "Train Epoch: 979 [acc: 10%]\tLoss: 2.323002\n",
      "Train Epoch: 980 [acc: 10%]\tLoss: 2.309091\n",
      "Train Epoch: 981 [acc: 11%]\tLoss: 2.359244\n",
      "Train Epoch: 982 [acc: 11%]\tLoss: 2.326353\n",
      "Train Epoch: 983 [acc: 12%]\tLoss: 2.276453\n",
      "Train Epoch: 984 [acc: 11%]\tLoss: 2.352913\n",
      "Train Epoch: 985 [acc: 10%]\tLoss: 2.405666\n",
      "Train Epoch: 986 [acc: 10%]\tLoss: 2.327869\n",
      "Train Epoch: 987 [acc: 10%]\tLoss: 2.403815\n",
      "Train Epoch: 988 [acc: 9%]\tLoss: 2.352491\n",
      "Train Epoch: 989 [acc: 10%]\tLoss: 2.302871\n",
      "Train Epoch: 990 [acc: 8%]\tLoss: 2.393147\n",
      "Train Epoch: 991 [acc: 10%]\tLoss: 2.395665\n",
      "Train Epoch: 992 [acc: 13%]\tLoss: 2.312788\n",
      "Train Epoch: 993 [acc: 11%]\tLoss: 2.313125\n",
      "Train Epoch: 994 [acc: 11%]\tLoss: 2.300082\n",
      "Train Epoch: 995 [acc: 13%]\tLoss: 2.354319\n",
      "Train Epoch: 996 [acc: 9%]\tLoss: 2.294086\n",
      "Train Epoch: 997 [acc: 11%]\tLoss: 2.309199\n",
      "Train Epoch: 998 [acc: 9%]\tLoss: 2.425352\n",
      "Train Epoch: 999 [acc: 14%]\tLoss: 2.349376\n",
      "Train Epoch: 1000 [acc: 11%]\tLoss: 2.307515\n",
      "[Trained for 1000 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.10 +- 0.16 , Avg Loss: 2.30\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "#     resnet18 = models.resnet18()\n",
    "#     alexnet = models.alexnet()\n",
    "#     vgg16 = models.vgg16()\n",
    "#     squeezenet = models.squeezenet1_0()\n",
    "#     densenet = models.densenet161()\n",
    "#     inception = models.inception_v3()\n",
    "#     googlenet = models.googlenet()\n",
    "#     shufflenet = models.shufflenet_v2_x1_0()\n",
    "#     mobilenet = models.mobilenet_v2()\n",
    "#     resnext50_32x4d = models.resnext50_32x4d()\n",
    "#     wide_resnet50_2 = models.wide_resnet50_2()\n",
    "#     mnasnet = models.mnasnet1_0()\n",
    "\n",
    "OPTIM = \"Adam\"\n",
    "MODEL = \"squeezenet\"\n",
    "EPOCH_NUM = 1000\n",
    "TRAIN_SAMPLE_NUM = 100\n",
    "VAL_SAMPLE_NUM = 2000\n",
    "BATCH_SIZE = 64\n",
    "VALIDATION_SET_NUM = 5\n",
    "AUGMENT = True\n",
    "VAL_DISPLAY_DIVISOR =25\n",
    "CIFAR_TRAIN = True\n",
    "\n",
    "#cifar-10:\n",
    "#mean = (0.4914, 0.4822, 0.4465)\n",
    "#std = (0.247, 0.243, 0.261)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "if AUGMENT:\n",
    "    dataAugmentation = [ \n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        AutoAugment(),\n",
    "        Cutout()\n",
    "    ]\n",
    "    augment = \"Crop,Flip,AutoAugment,Cutout\"\n",
    "else: \n",
    "    dataAugmentation = []\n",
    "    augment = \"Nothing\"\n",
    "\n",
    "\n",
    "# We resize images to allow using imagenet pre-trained models, is there a better way?\n",
    "# resize = transforms.Resize(299) \n",
    "\n",
    "transform_train = transforms.Compose(dataAugmentation + [transforms.ToTensor(), normalize]) \n",
    "transform_val = transforms.Compose([transforms.ToTensor(), normalize]) #careful to keep this one same\n",
    "\n",
    "cifar_train = datasets.CIFAR10(root='.',train=CIFAR_TRAIN, transform=transform_train, download=True)\n",
    "cifar_val = datasets.CIFAR10(root='.',train=CIFAR_TRAIN, transform=transform_val, download=True)\n",
    "\n",
    "ss = SmallSampleController(numClasses=10,trainSampleNum=TRAIN_SAMPLE_NUM, # abstract the data-loading procedure\n",
    "                           valSampleNum=VAL_SAMPLE_NUM, batchSize=BATCH_SIZE, \n",
    "                           multiplier=VALIDATION_SET_NUM, trainDataset=cifar_train, \n",
    "                           valDataset=cifar_val)\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data, valSets, seed = ss.generateNewSet(device,valMultiplier = VALIDATION_SET_NUM) #Sample from datasets\n",
    "\n",
    "\n",
    "\n",
    "eNet = getModel(MODEL).cuda()\n",
    "\n",
    "# for param in eNet.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "# model = featureExtractor(eNet).cuda()\n",
    "model = eNet\n",
    "\n",
    "# model.fc.out_features = 10\n",
    "    \n",
    "optimizer,LR = getOptimizer128(OPTIM,model.classifier.parameters())\n",
    "\n",
    "# print(model.parameters)\n",
    "\n",
    "print(' => Total trainable parameters: %.2fM' % (sum(p.numel() for p in model.parameters()) / 1000000.0))        \n",
    "\n",
    "trainTracker = {\"meanLoss\":[],\"accuracy\":[]}\n",
    "valTracker = {\"allLoss\":[],\"allAcc\":[],\"meanLoss\":[],\"meanAcc\":[],\"stdAcc\":[]}\n",
    "latexTracker = []\n",
    "\n",
    "print(\"Begin Train for {} epochs\".format(EPOCH_NUM))\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    acc, loss = train(model, device, train_data[0], optimizer, epoch+1, display=True)\n",
    "    trainTracker[\"meanLoss\"].append(loss)\n",
    "    trainTracker[\"accuracy\"].append(acc)\n",
    "    \n",
    "    if (epoch+1) % VAL_DISPLAY_DIVISOR == 0:\n",
    "        checkTest(model,device,valSets,valTracker,latexTracker,epoch+1,\n",
    "              model_name=MODEL,optim_name=OPTIM,lr=LR,totalTestSamples=VAL_SAMPLE_NUM*VALIDATION_SET_NUM,\n",
    "                  seed=seed,verbose=True)\n",
    "        \n",
    "          \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = latexTracker[-1][:-2] \n",
    "\n",
    "def writeTex(latexTracker,dirname):\n",
    "    if not os.path.isdir(dirname):\n",
    "        os.mkdir(dirname)\n",
    "        \n",
    "    f= open(os.path.join(dirname,\"latexTable.txt\"),\"w\")\n",
    "    for x in latexTracker:\n",
    "        f.write(x)\n",
    "    f.close()\n",
    "\n",
    "writeTex(latexTracker,dirname)\n",
    "\n",
    "for x in latexTracker:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochList = [x+1 for x in range(len(trainTracker[\"meanLoss\"]))]\n",
    "\n",
    "plot(xlist=epochList,ylist=trainTracker[\"meanLoss\"],xlab=\"Mean Train Loss\",\n",
    "    ylab=\"Epochs\",title=\"Mean Train Loss over Epochs\",\n",
    "    color=\"#243A92\",label=\"mean train loss\",savedir=dirname,save=True)\n",
    "\n",
    "plot(xlist=epochList,ylist=trainTracker[\"accuracy\"],xlab=\"Train Accuracy\",\n",
    "    ylab=\"Epochs\",title=\"Train Accuracy Over Epochs\",\n",
    "    color=\"#34267E\",label=\"Train Accuracy\",savedir=dirname,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochList = [VAL_DISPLAY_DIVISOR*(x+1) for x in range(len(valTracker[\"meanLoss\"]))]\n",
    "\n",
    "plot(xlist=epochList,ylist=valTracker[\"meanLoss\"],xlab=\"Epochs\",\n",
    "    ylab=\"Mean Val Loss\",title=\"Mean Val Loss over Epochs\",\n",
    "    color=\"#243A92\",label=\"mean val loss\",savedir=dirname,save=True)\n",
    "\n",
    "plot(xlist=epochList,ylist=valTracker[\"meanAcc\"],xlab=\"Epochs\",\n",
    "    ylab=\"Val Accuracy\",title=\"Val Accuracy Over Epochs\",\n",
    "    color=\"#34267E\",label=\"Val Accuracy\",savedir=dirname,save=True)\n",
    "\n",
    "plot(xlist=epochList,ylist=valTracker[\"stdAcc\"],xlab=\"Epochs\",\n",
    "    ylab=\"Val Accuracy Standard Deviation\",title=\"Val Accuracy Standard Deviation Over Epochs\",\n",
    "    color=\"#34267E\",label=\"Val Accuracy SD\",savedir=dirname,save=True)\n",
    "\n",
    "\n",
    "valSetEvalCount = VAL_DISPLAY_DIVISOR * EPOCH_NUM * VALIDATION_SET_NUM\n",
    "epochList = [VAL_DISPLAY_DIVISOR*(x+1) for x in range(len(valTracker[\"meanLoss\"]))\\\n",
    "             for y in range(VALIDATION_SET_NUM)]\n",
    "\n",
    "\n",
    "plot(xlist=epochList,ylist=valTracker[\"allLoss\"],xlab=\"Val Set Evaluations\",\n",
    "    ylab=\"Val Loss\",title=\"Val loss over val set evaluations ({} \\\n",
    "every {} epochs)\".format(VALIDATION_SET_NUM,VAL_DISPLAY_DIVISOR),\n",
    "    color=\"#34267E\",label=\"Val Loss\",savedir=dirname,save=True)\n",
    "\n",
    "plot(xlist=epochList,ylist=valTracker[\"allAcc\"],xlab=\"Val Set Evaluations\",\n",
    "    ylab=\"Val Accuracy\",title=\"Val loss over val set evaluations ({} \\\n",
    "every {} epochs) \".format(VALIDATION_SET_NUM,VAL_DISPLAY_DIVISOR),\n",
    "    color=\"#34267E\",label=\"Val Accuracy\",savedir=dirname,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
