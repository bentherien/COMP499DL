{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import gc\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset\n",
    "from IPython.core.display import display, HTML\n",
    "from numpy.random import RandomState\n",
    "from wide_resnet import WideResNet\n",
    "from auto_augment import AutoAugment, Cutout\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from cifar_loader import SmallSampleController\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "# display(HTML(\"<style>.container { width:40% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getAcc(preds,targets):\n",
    "    return np.sum([1 if preds[i] == targets[i] else 0 for i in range(len(preds))])/len(preds)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, display=True):\n",
    "    \"\"\"\n",
    "    Summary: Implements the training procedure for a given model\n",
    "    == params ==\n",
    "    model: the model to test\n",
    "    device: cuda or cpu \n",
    "    optimizer: the optimizer for our training\n",
    "    train_loader: dataloader for our train data\n",
    "    display: output flag\n",
    "    == output ==\n",
    "    the mean train loss, the train accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    lossTracker = []\n",
    "    \n",
    "    targets=[]\n",
    "    preds=[]\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lossTracker.append(loss.detach())\n",
    "        with torch.no_grad():\n",
    "            pred = torch.argmax(output,1).cpu().numpy()\n",
    "\n",
    "            preds.extend(pred)\n",
    "            targets.extend(target.cpu().numpy())\n",
    "        \n",
    "    lossTracker = [x.item() for x in lossTracker]\n",
    "    meanLoss = np.mean(lossTracker)\n",
    "    accuracy = getAcc(preds,targets)\n",
    "    if display:\n",
    "        print('Train Epoch: {} [acc: {:.0f}%]\\tLoss: {:.6f}'.format(\n",
    "          epoch, 100. * accuracy, meanLoss))\n",
    "        \n",
    "    return accuracy, meanLoss\n",
    "\n",
    "\n",
    "\n",
    "def test(model, device, test_loader,verbose=True):\n",
    "    \"\"\"\n",
    "    Summary: Implements the testing procedure for a given model\n",
    "    == params ==\n",
    "    model: the model to test\n",
    "    device: cuda or cpu \n",
    "    test_loader: dataloader for our test data\n",
    "    verbose: output flag\n",
    "    == output ==\n",
    "    the mean test loss, the test accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "\n",
    "    meanLoss = test_loss / len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    if verbose: print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        mean_test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "        \n",
    "    return accuracy, meanLoss\n",
    "\n",
    "\n",
    "def checkTest(model,device,valSets,valTracker,latexTracker,epoch,\n",
    "              model_name,optim_name,lr,totalTestSamples,seed,verbose=True):\n",
    "    \"\"\"\n",
    "    Summary: checks the test accuracy, prints, and saves statistics\n",
    "    \"\"\"\n",
    "    tempAcc = []\n",
    "    tempLoss = []\n",
    "    for val_loader in valSets:\n",
    "        acc,loss = test(model, device, val_loader,verbose = False)\n",
    "        tempAcc.append(acc)\n",
    "        tempLoss.append(loss)\n",
    "        \n",
    "    meanAcc = np.mean(tempAcc)\n",
    "    stdAcc = np.std(tempAcc)\n",
    "    \n",
    "    meanLoss = np.mean(tempLoss)\n",
    "    if verbose:\n",
    "        print('[Trained for {} epochs and tested on {} sets of 2000 images]\\\n",
    "        Avg Acc: {:.2f} +- {:.2f} , Avg Loss: {:.2f}'.format(\n",
    "            epoch,VALIDATION_SET_NUM,meanAcc,stdAcc,meanLoss))\n",
    "        \n",
    "        \n",
    "    tableRow = getLatexRow(architecture=model_name,epoch=epoch,accuracy=meanAcc,optim=optim_name,\n",
    "                           lr=lr,totalTestSamples=totalTestSamples,dataAug=\"Nothing\",\n",
    "                           seed=seed,title=False)\n",
    "    \n",
    "    latexTracker.append(tableRow)\n",
    "        \n",
    "    valTracker[\"allLoss\"].extend(tempLoss)\n",
    "    valTracker[\"allAcc\"].extend(tempAcc)\n",
    "    valTracker[\"meanLoss\"].append(meanLoss)\n",
    "    valTracker[\"meanAcc\"].append(meanAcc)\n",
    "    valTracker[\"stdAcc\"].append(stdAcc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLatexRow(architecture,epoch,accuracy,optim,lr,\n",
    "                totalTestSamples,dataAug,seed,title=False):\n",
    "    \"\"\"\n",
    "    Summary: generates one row of latex for a results table\n",
    "    \"\"\"\n",
    "    categories = [\"Model\",\"Epoch\",\"Accuracy\",\"Optimizer\",\"lr\",\"Test Sample Num\",\n",
    "                  \"data augmentation\",\"seed\"]\n",
    "    row = [str(architecture),str(epoch),str(round(accuracy,3)),str(optim),\n",
    "           str(lr),str(totalTestSamples),str(dataAug),str(seed)]\n",
    "    \n",
    "    if title:\n",
    "        c = \"&\".join(categories)\n",
    "        r = \"&\".join(row)\n",
    "        return \"{}\\\\\\\\\\n{}\\\\\\\\\".format(c,r)\n",
    "    else:\n",
    "        r = \"&\".join(row)\n",
    "        return \"{}\\\\\\\\\".format(r)\n",
    "    \n",
    "    \n",
    "def plot(xlist,ylist,xlab,ylab,title,color,label,savedir=\".\",save=False):\n",
    "    \"\"\"\n",
    "    Summary: plots the given list of numbers against its idices and \n",
    "    allows for high resolution saving\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.plot(xlist,ylist,color=color,marker=\".\",label=label)\n",
    "    plt.legend()\n",
    "    \n",
    "    if save:\n",
    "        if not os.path.isdir(savedir):\n",
    "            os.mkdir(savedir)\n",
    "        filepath = os.path.join(savedir,\"{}\".format(title))\n",
    "        plt.savefig(filepath+\".pdf\")\n",
    "        os.system(\"pdftoppm -png -r 300 {}.pdf {}.png\".format(filepath,filepath))\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(model_name):\n",
    "    if \"wide\" in model_name.lower():\n",
    "        return WideResNet(28, 10, num_classes=10)\n",
    "    elif \"efficient\" in model_name.lower():\n",
    "        return EfficientNet.from_pretrained(model_name,num_classes = 10) # change to not be pretrained\n",
    "    elif \"vgg16\" in model_name.lower():\n",
    "        model = models.vgg16(pretrained=True)\n",
    "#         model.classifier[6] = nn.Linear(4096, 10)\n",
    "        return model\n",
    "    elif \"alexnet\" in model_name.lower():\n",
    "        model = models.alexnet(pretrained=True)\n",
    "#         model.classifier = nn.Linear(256 * 6 * 6, 10)\n",
    "        return model\n",
    "    elif \"resnet18\" in model_name.lower():\n",
    "        model = models.resnet18(pretrained=True)\n",
    "#         model.fc.out_features = 10\n",
    "        return model\n",
    "    \n",
    "    \n",
    "def getOptimizer128(optimizer_name,parameters):\n",
    "    if \"sgd\" in  optimizer_name.lower():\n",
    "        LR = 0.1\n",
    "        optim = torch.optim.SGD(parameters, \n",
    "                                  lr=LR, momentum=0.9,\n",
    "                                  weight_decay=0.0005)\n",
    "        return optim, LR\n",
    "    elif \"adam\" in optimizer_name.lower():\n",
    "        LR = 0.00005\n",
    "        optim = torch.optim.Adam(parameters, \n",
    "                              lr=LR, weight_decay=0)\n",
    "        return optim, LR\n",
    "    elif \"rmsprop\" in optimizer_name.lower():\n",
    "        LR = 0.01\n",
    "        optim = torch.optim.RMSprop(params, lr=0.256, alpha=0.99, eps=1e-08, \n",
    "                            weight_decay=0.00005, momentum=0.00, centered=False)\n",
    "        return optim, LR\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class featureExtractor(nn.Module):\n",
    "    def __init__(self,efficientNet):\n",
    "        super().__init__()\n",
    "        self.eNet = efficientNet \n",
    "        self._avg_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self._dropout = nn.Dropout(self.eNet._global_params.dropout_rate)\n",
    "        self._fc = nn.Linear(2560,10)\n",
    "        \n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x = self.eNet.extract_features(inputs)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self._dropout(x)\n",
    "        x = self._fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Generated new permutation of the CIFAR train dataset with                 seed:1619993594, train sample num: 100, test sample num: 2000\n",
      "Loaded pretrained weights for efficientnet-b2\n",
      "<bound method Module.parameters of EfficientNet(\n",
      "  (_conv_stem): Conv2dStaticSamePadding(\n",
      "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
      "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
      "  )\n",
      "  (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "  (_blocks): ModuleList(\n",
      "    (0): MBConvBlock(\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (1): MBConvBlock(\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), groups=16, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        16, 4, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        4, 16, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (2): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (3): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (4): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (5): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (6): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        288, 288, kernel_size=(5, 5), stride=(1, 1), groups=288, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        288, 12, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        12, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (7): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        288, 288, kernel_size=(5, 5), stride=(1, 1), groups=288, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        288, 12, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        12, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (8): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        288, 288, kernel_size=(3, 3), stride=[2, 2], groups=288, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        288, 12, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        12, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(88, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (9): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(528, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        528, 528, kernel_size=(3, 3), stride=(1, 1), groups=528, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(528, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        528, 22, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        22, 528, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(88, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (10): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(528, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        528, 528, kernel_size=(3, 3), stride=(1, 1), groups=528, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(528, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        528, 22, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        22, 528, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(88, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (11): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(528, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        528, 528, kernel_size=(3, 3), stride=(1, 1), groups=528, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(528, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        528, 22, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        22, 528, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(88, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (12): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(528, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        528, 528, kernel_size=(5, 5), stride=[1, 1], groups=528, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(528, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        528, 22, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        22, 528, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        528, 120, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(120, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (13): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(720, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        720, 720, kernel_size=(5, 5), stride=(1, 1), groups=720, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(720, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        720, 30, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        30, 720, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(120, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (14): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(720, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        720, 720, kernel_size=(5, 5), stride=(1, 1), groups=720, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(720, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        720, 30, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        30, 720, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(120, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (15): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(720, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        720, 720, kernel_size=(5, 5), stride=(1, 1), groups=720, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(720, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        720, 30, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        30, 720, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(120, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (16): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(720, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        720, 720, kernel_size=(5, 5), stride=[2, 2], groups=720, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(720, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        720, 30, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        30, 720, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        720, 208, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(208, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (17): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1248, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1248, 1248, kernel_size=(5, 5), stride=(1, 1), groups=1248, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1248, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1248, 52, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        52, 1248, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(208, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (18): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1248, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1248, 1248, kernel_size=(5, 5), stride=(1, 1), groups=1248, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1248, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1248, 52, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        52, 1248, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(208, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (19): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1248, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1248, 1248, kernel_size=(5, 5), stride=(1, 1), groups=1248, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1248, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1248, 52, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        52, 1248, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(208, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (20): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1248, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1248, 1248, kernel_size=(5, 5), stride=(1, 1), groups=1248, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1248, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1248, 52, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        52, 1248, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(208, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (21): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1248, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1248, 1248, kernel_size=(3, 3), stride=[1, 1], groups=1248, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1248, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1248, 52, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        52, 1248, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1248, 352, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(352, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (22): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        352, 2112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(2112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        2112, 2112, kernel_size=(3, 3), stride=(1, 1), groups=2112, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(2112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        2112, 88, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        88, 2112, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        2112, 352, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(352, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "  )\n",
      "  (_conv_head): Conv2dStaticSamePadding(\n",
      "    352, 1408, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "    (static_padding): Identity()\n",
      "  )\n",
      "  (_bn1): BatchNorm2d(1408, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
      "  (_dropout): Dropout(p=0.3, inplace=False)\n",
      "  (_fc): Linear(in_features=1408, out_features=10, bias=True)\n",
      "  (_swish): MemoryEfficientSwish()\n",
      ")>\n",
      " => Total trainable parameters: 7.72M\n",
      "Begin Train for 100000 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [acc: 12%]\tLoss: 2.394218\n",
      "Train Epoch: 2 [acc: 12%]\tLoss: 2.385690\n",
      "Train Epoch: 3 [acc: 19%]\tLoss: 2.301321\n",
      "Train Epoch: 4 [acc: 10%]\tLoss: 2.292584\n",
      "Train Epoch: 5 [acc: 23%]\tLoss: 2.220886\n",
      "Train Epoch: 6 [acc: 25%]\tLoss: 2.212301\n",
      "Train Epoch: 7 [acc: 22%]\tLoss: 2.181154\n",
      "Train Epoch: 8 [acc: 27%]\tLoss: 2.300593\n",
      "Train Epoch: 9 [acc: 27%]\tLoss: 2.258864\n",
      "Train Epoch: 10 [acc: 25%]\tLoss: 2.210883\n",
      "Train Epoch: 11 [acc: 21%]\tLoss: 2.815822\n",
      "Train Epoch: 12 [acc: 29%]\tLoss: 2.432531\n",
      "Train Epoch: 13 [acc: 26%]\tLoss: 2.317907\n",
      "Train Epoch: 14 [acc: 28%]\tLoss: 2.853905\n",
      "Train Epoch: 15 [acc: 24%]\tLoss: 3.002356\n",
      "Train Epoch: 16 [acc: 26%]\tLoss: 2.643939\n",
      "Train Epoch: 17 [acc: 30%]\tLoss: 2.394347\n",
      "Train Epoch: 18 [acc: 21%]\tLoss: 3.229366\n",
      "Train Epoch: 19 [acc: 34%]\tLoss: 2.715906\n",
      "Train Epoch: 20 [acc: 22%]\tLoss: 3.206520\n",
      "Train Epoch: 21 [acc: 27%]\tLoss: 3.019003\n",
      "Train Epoch: 22 [acc: 26%]\tLoss: 3.069120\n",
      "Train Epoch: 23 [acc: 25%]\tLoss: 3.009127\n",
      "Train Epoch: 24 [acc: 28%]\tLoss: 2.867465\n",
      "Train Epoch: 25 [acc: 27%]\tLoss: 2.816528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/venv/torch11/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trained for 25 epochs and tested on 5 sets of 2000 images]        Avg Acc: 10.01 +- 0.10 , Avg Loss: 2.77\n",
      "Train Epoch: 26 [acc: 22%]\tLoss: 3.208790\n",
      "Train Epoch: 27 [acc: 27%]\tLoss: 3.222914\n",
      "Train Epoch: 28 [acc: 28%]\tLoss: 3.604144\n",
      "Train Epoch: 29 [acc: 27%]\tLoss: 3.571048\n",
      "Train Epoch: 30 [acc: 22%]\tLoss: 3.463038\n",
      "Train Epoch: 31 [acc: 31%]\tLoss: 3.074571\n",
      "Train Epoch: 32 [acc: 35%]\tLoss: 2.936583\n",
      "Train Epoch: 33 [acc: 22%]\tLoss: 3.945976\n",
      "Train Epoch: 34 [acc: 30%]\tLoss: 3.106127\n",
      "Train Epoch: 35 [acc: 30%]\tLoss: 3.321368\n",
      "Train Epoch: 36 [acc: 27%]\tLoss: 3.783017\n",
      "Train Epoch: 37 [acc: 26%]\tLoss: 3.514533\n",
      "Train Epoch: 38 [acc: 25%]\tLoss: 3.993915\n",
      "Train Epoch: 39 [acc: 28%]\tLoss: 4.051842\n",
      "Train Epoch: 40 [acc: 22%]\tLoss: 3.946180\n",
      "Train Epoch: 41 [acc: 34%]\tLoss: 3.761600\n",
      "Train Epoch: 42 [acc: 34%]\tLoss: 3.129751\n",
      "Train Epoch: 43 [acc: 21%]\tLoss: 4.319491\n",
      "Train Epoch: 44 [acc: 43%]\tLoss: 2.833761\n",
      "Train Epoch: 45 [acc: 29%]\tLoss: 3.866552\n",
      "Train Epoch: 46 [acc: 31%]\tLoss: 3.953522\n",
      "Train Epoch: 47 [acc: 23%]\tLoss: 4.637565\n",
      "Train Epoch: 48 [acc: 29%]\tLoss: 4.066482\n",
      "Train Epoch: 49 [acc: 23%]\tLoss: 3.803234\n",
      "Train Epoch: 50 [acc: 32%]\tLoss: 3.878431\n",
      "[Trained for 50 epochs and tested on 5 sets of 2000 images]        Avg Acc: 11.90 +- 0.30 , Avg Loss: 2.70\n",
      "Train Epoch: 51 [acc: 35%]\tLoss: 3.379072\n",
      "Train Epoch: 52 [acc: 34%]\tLoss: 3.859401\n",
      "Train Epoch: 53 [acc: 24%]\tLoss: 4.680839\n",
      "Train Epoch: 54 [acc: 34%]\tLoss: 3.998646\n",
      "Train Epoch: 55 [acc: 32%]\tLoss: 3.761777\n",
      "Train Epoch: 56 [acc: 26%]\tLoss: 4.166153\n",
      "Train Epoch: 57 [acc: 27%]\tLoss: 3.641700\n",
      "Train Epoch: 58 [acc: 30%]\tLoss: 3.708061\n",
      "Train Epoch: 59 [acc: 37%]\tLoss: 4.052446\n",
      "Train Epoch: 60 [acc: 31%]\tLoss: 3.382772\n",
      "Train Epoch: 61 [acc: 32%]\tLoss: 4.209983\n",
      "Train Epoch: 62 [acc: 28%]\tLoss: 4.565718\n",
      "Train Epoch: 63 [acc: 32%]\tLoss: 4.355746\n",
      "Train Epoch: 64 [acc: 37%]\tLoss: 3.946034\n",
      "Train Epoch: 65 [acc: 24%]\tLoss: 5.151727\n",
      "Train Epoch: 66 [acc: 26%]\tLoss: 4.267346\n",
      "Train Epoch: 67 [acc: 32%]\tLoss: 4.227029\n",
      "Train Epoch: 68 [acc: 22%]\tLoss: 4.312734\n",
      "Train Epoch: 69 [acc: 27%]\tLoss: 4.761730\n",
      "Train Epoch: 70 [acc: 33%]\tLoss: 4.052095\n",
      "Train Epoch: 71 [acc: 32%]\tLoss: 4.144601\n",
      "Train Epoch: 72 [acc: 33%]\tLoss: 3.838286\n",
      "Train Epoch: 73 [acc: 33%]\tLoss: 4.954735\n",
      "Train Epoch: 74 [acc: 34%]\tLoss: 4.736578\n",
      "Train Epoch: 75 [acc: 15%]\tLoss: 5.394700\n",
      "[Trained for 75 epochs and tested on 5 sets of 2000 images]        Avg Acc: 12.31 +- 0.44 , Avg Loss: 2.92\n",
      "Train Epoch: 76 [acc: 23%]\tLoss: 4.769939\n",
      "Train Epoch: 77 [acc: 25%]\tLoss: 4.041832\n",
      "Train Epoch: 78 [acc: 29%]\tLoss: 3.823524\n",
      "Train Epoch: 79 [acc: 34%]\tLoss: 4.469314\n",
      "Train Epoch: 80 [acc: 21%]\tLoss: 4.914093\n",
      "Train Epoch: 81 [acc: 27%]\tLoss: 4.725307\n",
      "Train Epoch: 82 [acc: 32%]\tLoss: 4.449619\n",
      "Train Epoch: 83 [acc: 35%]\tLoss: 4.407690\n",
      "Train Epoch: 84 [acc: 36%]\tLoss: 4.347905\n",
      "Train Epoch: 85 [acc: 35%]\tLoss: 4.066968\n",
      "Train Epoch: 86 [acc: 29%]\tLoss: 5.305326\n",
      "Train Epoch: 87 [acc: 30%]\tLoss: 4.550371\n",
      "Train Epoch: 88 [acc: 33%]\tLoss: 4.107629\n",
      "Train Epoch: 89 [acc: 31%]\tLoss: 4.861515\n",
      "Train Epoch: 90 [acc: 33%]\tLoss: 4.387201\n",
      "Train Epoch: 91 [acc: 28%]\tLoss: 4.819829\n",
      "Train Epoch: 92 [acc: 31%]\tLoss: 4.399664\n",
      "Train Epoch: 93 [acc: 30%]\tLoss: 4.556527\n",
      "Train Epoch: 94 [acc: 24%]\tLoss: 5.039456\n",
      "Train Epoch: 95 [acc: 35%]\tLoss: 3.730119\n",
      "Train Epoch: 96 [acc: 38%]\tLoss: 3.843297\n",
      "Train Epoch: 97 [acc: 28%]\tLoss: 4.764071\n",
      "Train Epoch: 98 [acc: 22%]\tLoss: 5.234057\n",
      "Train Epoch: 99 [acc: 27%]\tLoss: 4.701434\n",
      "Train Epoch: 100 [acc: 27%]\tLoss: 4.671388\n",
      "[Trained for 100 epochs and tested on 5 sets of 2000 images]        Avg Acc: 14.20 +- 0.40 , Avg Loss: 3.50\n",
      "Train Epoch: 101 [acc: 26%]\tLoss: 4.822112\n",
      "Train Epoch: 102 [acc: 24%]\tLoss: 5.733164\n",
      "Train Epoch: 103 [acc: 31%]\tLoss: 3.929960\n",
      "Train Epoch: 104 [acc: 36%]\tLoss: 4.832133\n",
      "Train Epoch: 105 [acc: 35%]\tLoss: 4.237535\n",
      "Train Epoch: 106 [acc: 28%]\tLoss: 4.645546\n",
      "Train Epoch: 107 [acc: 31%]\tLoss: 4.688051\n",
      "Train Epoch: 108 [acc: 26%]\tLoss: 4.942853\n",
      "Train Epoch: 109 [acc: 25%]\tLoss: 4.688005\n",
      "Train Epoch: 110 [acc: 24%]\tLoss: 5.333093\n",
      "Train Epoch: 111 [acc: 26%]\tLoss: 5.432606\n",
      "Train Epoch: 112 [acc: 28%]\tLoss: 5.012573\n",
      "Train Epoch: 113 [acc: 33%]\tLoss: 4.409339\n",
      "Train Epoch: 114 [acc: 34%]\tLoss: 4.114778\n",
      "Train Epoch: 115 [acc: 30%]\tLoss: 4.798695\n",
      "Train Epoch: 116 [acc: 28%]\tLoss: 4.620837\n",
      "Train Epoch: 117 [acc: 28%]\tLoss: 4.513058\n",
      "Train Epoch: 118 [acc: 30%]\tLoss: 5.458290\n",
      "Train Epoch: 119 [acc: 29%]\tLoss: 5.323243\n",
      "Train Epoch: 120 [acc: 32%]\tLoss: 4.123109\n",
      "Train Epoch: 121 [acc: 31%]\tLoss: 4.720150\n",
      "Train Epoch: 122 [acc: 34%]\tLoss: 4.853345\n",
      "Train Epoch: 123 [acc: 28%]\tLoss: 5.662334\n",
      "Train Epoch: 124 [acc: 34%]\tLoss: 4.016211\n",
      "Train Epoch: 125 [acc: 32%]\tLoss: 4.603313\n",
      "[Trained for 125 epochs and tested on 5 sets of 2000 images]        Avg Acc: 14.98 +- 0.17 , Avg Loss: 3.73\n",
      "Train Epoch: 126 [acc: 24%]\tLoss: 5.386576\n",
      "Train Epoch: 127 [acc: 25%]\tLoss: 5.090713\n",
      "Train Epoch: 128 [acc: 36%]\tLoss: 4.016109\n",
      "Train Epoch: 129 [acc: 33%]\tLoss: 4.725831\n",
      "Train Epoch: 130 [acc: 31%]\tLoss: 5.102915\n",
      "Train Epoch: 131 [acc: 31%]\tLoss: 5.008740\n",
      "Train Epoch: 132 [acc: 23%]\tLoss: 5.577944\n",
      "Train Epoch: 133 [acc: 26%]\tLoss: 5.433005\n",
      "Train Epoch: 134 [acc: 31%]\tLoss: 4.917333\n",
      "Train Epoch: 135 [acc: 31%]\tLoss: 5.773560\n",
      "Train Epoch: 136 [acc: 37%]\tLoss: 4.274212\n",
      "Train Epoch: 137 [acc: 28%]\tLoss: 6.332310\n",
      "Train Epoch: 138 [acc: 22%]\tLoss: 5.561044\n",
      "Train Epoch: 139 [acc: 35%]\tLoss: 4.574755\n",
      "Train Epoch: 140 [acc: 33%]\tLoss: 5.354666\n",
      "Train Epoch: 141 [acc: 27%]\tLoss: 5.057016\n",
      "Train Epoch: 142 [acc: 26%]\tLoss: 5.489932\n",
      "Train Epoch: 143 [acc: 24%]\tLoss: 5.221071\n",
      "Train Epoch: 144 [acc: 32%]\tLoss: 4.618904\n",
      "Train Epoch: 145 [acc: 29%]\tLoss: 5.601977\n",
      "Train Epoch: 146 [acc: 26%]\tLoss: 5.925809\n",
      "Train Epoch: 147 [acc: 24%]\tLoss: 6.426145\n",
      "Train Epoch: 148 [acc: 37%]\tLoss: 4.590077\n",
      "Train Epoch: 149 [acc: 24%]\tLoss: 5.389177\n",
      "Train Epoch: 150 [acc: 31%]\tLoss: 5.039900\n",
      "[Trained for 150 epochs and tested on 5 sets of 2000 images]        Avg Acc: 14.97 +- 0.50 , Avg Loss: 4.33\n",
      "Train Epoch: 151 [acc: 23%]\tLoss: 5.525358\n",
      "Train Epoch: 152 [acc: 21%]\tLoss: 5.752293\n",
      "Train Epoch: 153 [acc: 26%]\tLoss: 5.498694\n",
      "Train Epoch: 154 [acc: 28%]\tLoss: 6.192418\n",
      "Train Epoch: 155 [acc: 22%]\tLoss: 5.284330\n",
      "Train Epoch: 156 [acc: 27%]\tLoss: 5.138258\n",
      "Train Epoch: 157 [acc: 27%]\tLoss: 4.745966\n",
      "Train Epoch: 158 [acc: 34%]\tLoss: 4.741620\n",
      "Train Epoch: 159 [acc: 31%]\tLoss: 4.662382\n",
      "Train Epoch: 160 [acc: 32%]\tLoss: 5.141417\n",
      "Train Epoch: 161 [acc: 36%]\tLoss: 5.036258\n",
      "Train Epoch: 162 [acc: 28%]\tLoss: 5.643452\n",
      "Train Epoch: 163 [acc: 29%]\tLoss: 4.835431\n",
      "Train Epoch: 164 [acc: 34%]\tLoss: 5.160408\n",
      "Train Epoch: 165 [acc: 33%]\tLoss: 4.622905\n",
      "Train Epoch: 166 [acc: 35%]\tLoss: 5.145863\n",
      "Train Epoch: 167 [acc: 32%]\tLoss: 5.383511\n",
      "Train Epoch: 168 [acc: 38%]\tLoss: 4.503912\n",
      "Train Epoch: 169 [acc: 29%]\tLoss: 5.497037\n",
      "Train Epoch: 170 [acc: 21%]\tLoss: 5.877023\n",
      "Train Epoch: 171 [acc: 34%]\tLoss: 4.874346\n",
      "Train Epoch: 172 [acc: 26%]\tLoss: 5.582060\n",
      "Train Epoch: 173 [acc: 28%]\tLoss: 5.279147\n",
      "Train Epoch: 174 [acc: 34%]\tLoss: 5.504793\n",
      "Train Epoch: 175 [acc: 32%]\tLoss: 5.306684\n",
      "[Trained for 175 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.99 +- 0.29 , Avg Loss: 5.04\n",
      "Train Epoch: 176 [acc: 35%]\tLoss: 4.461656\n",
      "Train Epoch: 177 [acc: 30%]\tLoss: 4.525928\n",
      "Train Epoch: 178 [acc: 24%]\tLoss: 5.700743\n",
      "Train Epoch: 179 [acc: 31%]\tLoss: 4.831379\n",
      "Train Epoch: 180 [acc: 28%]\tLoss: 5.343607\n",
      "Train Epoch: 181 [acc: 32%]\tLoss: 5.558857\n",
      "Train Epoch: 182 [acc: 28%]\tLoss: 5.402452\n",
      "Train Epoch: 183 [acc: 33%]\tLoss: 5.328990\n",
      "Train Epoch: 184 [acc: 32%]\tLoss: 4.856063\n",
      "Train Epoch: 185 [acc: 33%]\tLoss: 5.039039\n",
      "Train Epoch: 186 [acc: 28%]\tLoss: 4.963373\n",
      "Train Epoch: 187 [acc: 36%]\tLoss: 4.304718\n",
      "Train Epoch: 188 [acc: 32%]\tLoss: 5.434841\n",
      "Train Epoch: 189 [acc: 34%]\tLoss: 5.349818\n",
      "Train Epoch: 190 [acc: 32%]\tLoss: 5.033524\n",
      "Train Epoch: 191 [acc: 26%]\tLoss: 5.473036\n",
      "Train Epoch: 192 [acc: 31%]\tLoss: 5.469003\n",
      "Train Epoch: 193 [acc: 29%]\tLoss: 5.320624\n",
      "Train Epoch: 194 [acc: 35%]\tLoss: 4.940532\n",
      "Train Epoch: 195 [acc: 26%]\tLoss: 5.462969\n",
      "Train Epoch: 196 [acc: 27%]\tLoss: 6.060360\n",
      "Train Epoch: 197 [acc: 38%]\tLoss: 4.419191\n",
      "Train Epoch: 198 [acc: 29%]\tLoss: 6.011659\n",
      "Train Epoch: 199 [acc: 28%]\tLoss: 6.069266\n",
      "Train Epoch: 200 [acc: 27%]\tLoss: 6.039221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trained for 200 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.98 +- 0.60 , Avg Loss: 5.27\n",
      "Train Epoch: 201 [acc: 35%]\tLoss: 4.902340\n",
      "Train Epoch: 202 [acc: 30%]\tLoss: 6.456171\n",
      "Train Epoch: 203 [acc: 28%]\tLoss: 5.321291\n",
      "Train Epoch: 204 [acc: 34%]\tLoss: 5.030759\n",
      "Train Epoch: 205 [acc: 34%]\tLoss: 5.365110\n",
      "Train Epoch: 206 [acc: 32%]\tLoss: 4.791469\n",
      "Train Epoch: 207 [acc: 23%]\tLoss: 5.830007\n",
      "Train Epoch: 208 [acc: 30%]\tLoss: 5.588132\n",
      "Train Epoch: 209 [acc: 34%]\tLoss: 4.534493\n",
      "Train Epoch: 210 [acc: 22%]\tLoss: 6.431704\n",
      "Train Epoch: 211 [acc: 29%]\tLoss: 4.886855\n",
      "Train Epoch: 212 [acc: 30%]\tLoss: 4.971320\n",
      "Train Epoch: 213 [acc: 27%]\tLoss: 5.743999\n",
      "Train Epoch: 214 [acc: 31%]\tLoss: 4.744389\n",
      "Train Epoch: 215 [acc: 25%]\tLoss: 5.719297\n",
      "Train Epoch: 216 [acc: 34%]\tLoss: 4.869385\n",
      "Train Epoch: 217 [acc: 30%]\tLoss: 5.091959\n",
      "Train Epoch: 218 [acc: 29%]\tLoss: 4.945020\n",
      "Train Epoch: 219 [acc: 33%]\tLoss: 5.687912\n",
      "Train Epoch: 220 [acc: 36%]\tLoss: 4.612198\n",
      "Train Epoch: 221 [acc: 32%]\tLoss: 5.393071\n",
      "Train Epoch: 222 [acc: 30%]\tLoss: 5.511123\n",
      "Train Epoch: 223 [acc: 29%]\tLoss: 4.766767\n",
      "Train Epoch: 224 [acc: 25%]\tLoss: 4.827701\n",
      "Train Epoch: 225 [acc: 34%]\tLoss: 5.017854\n",
      "[Trained for 225 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.54 +- 0.84 , Avg Loss: 5.65\n",
      "Train Epoch: 226 [acc: 25%]\tLoss: 5.698489\n",
      "Train Epoch: 227 [acc: 25%]\tLoss: 5.813309\n",
      "Train Epoch: 228 [acc: 32%]\tLoss: 4.615228\n",
      "Train Epoch: 229 [acc: 28%]\tLoss: 5.166774\n",
      "Train Epoch: 230 [acc: 31%]\tLoss: 6.204521\n",
      "Train Epoch: 231 [acc: 25%]\tLoss: 5.717518\n",
      "Train Epoch: 232 [acc: 23%]\tLoss: 6.526319\n",
      "Train Epoch: 233 [acc: 35%]\tLoss: 5.123322\n",
      "Train Epoch: 234 [acc: 32%]\tLoss: 5.410838\n",
      "Train Epoch: 235 [acc: 31%]\tLoss: 4.718154\n",
      "Train Epoch: 236 [acc: 26%]\tLoss: 5.382220\n",
      "Train Epoch: 237 [acc: 31%]\tLoss: 5.438564\n",
      "Train Epoch: 238 [acc: 34%]\tLoss: 5.320557\n",
      "Train Epoch: 239 [acc: 34%]\tLoss: 4.515757\n",
      "Train Epoch: 240 [acc: 20%]\tLoss: 6.079641\n",
      "Train Epoch: 241 [acc: 35%]\tLoss: 4.359041\n",
      "Train Epoch: 242 [acc: 29%]\tLoss: 5.909603\n",
      "Train Epoch: 243 [acc: 36%]\tLoss: 5.262866\n",
      "Train Epoch: 244 [acc: 35%]\tLoss: 5.239279\n",
      "Train Epoch: 245 [acc: 29%]\tLoss: 5.965495\n",
      "Train Epoch: 246 [acc: 32%]\tLoss: 5.439762\n",
      "Train Epoch: 247 [acc: 34%]\tLoss: 6.045505\n",
      "Train Epoch: 248 [acc: 34%]\tLoss: 4.621650\n",
      "Train Epoch: 249 [acc: 34%]\tLoss: 5.004600\n",
      "Train Epoch: 250 [acc: 25%]\tLoss: 6.162671\n",
      "[Trained for 250 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.54 +- 0.99 , Avg Loss: 6.11\n",
      "Train Epoch: 251 [acc: 25%]\tLoss: 5.855364\n",
      "Train Epoch: 252 [acc: 25%]\tLoss: 7.067181\n",
      "Train Epoch: 253 [acc: 32%]\tLoss: 5.610569\n",
      "Train Epoch: 254 [acc: 24%]\tLoss: 6.081338\n",
      "Train Epoch: 255 [acc: 31%]\tLoss: 5.164312\n",
      "Train Epoch: 256 [acc: 31%]\tLoss: 5.284429\n",
      "Train Epoch: 257 [acc: 30%]\tLoss: 5.065711\n",
      "Train Epoch: 258 [acc: 33%]\tLoss: 5.337113\n",
      "Train Epoch: 259 [acc: 23%]\tLoss: 5.914480\n",
      "Train Epoch: 260 [acc: 28%]\tLoss: 5.750369\n",
      "Train Epoch: 261 [acc: 34%]\tLoss: 5.421714\n",
      "Train Epoch: 262 [acc: 35%]\tLoss: 4.811395\n",
      "Train Epoch: 263 [acc: 24%]\tLoss: 5.728090\n",
      "Train Epoch: 264 [acc: 24%]\tLoss: 6.291842\n",
      "Train Epoch: 265 [acc: 31%]\tLoss: 5.169436\n",
      "Train Epoch: 266 [acc: 28%]\tLoss: 5.436641\n",
      "Train Epoch: 267 [acc: 29%]\tLoss: 5.504263\n",
      "Train Epoch: 268 [acc: 19%]\tLoss: 6.065586\n",
      "Train Epoch: 269 [acc: 32%]\tLoss: 4.868956\n",
      "Train Epoch: 270 [acc: 35%]\tLoss: 5.883244\n",
      "Train Epoch: 271 [acc: 36%]\tLoss: 5.820781\n",
      "Train Epoch: 272 [acc: 33%]\tLoss: 5.675395\n",
      "Train Epoch: 273 [acc: 31%]\tLoss: 5.228194\n",
      "Train Epoch: 274 [acc: 40%]\tLoss: 4.408198\n",
      "Train Epoch: 275 [acc: 41%]\tLoss: 3.725930\n",
      "[Trained for 275 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.14 +- 0.32 , Avg Loss: 6.53\n",
      "Train Epoch: 276 [acc: 33%]\tLoss: 6.589285\n",
      "Train Epoch: 277 [acc: 29%]\tLoss: 6.866619\n",
      "Train Epoch: 278 [acc: 28%]\tLoss: 5.514494\n",
      "Train Epoch: 279 [acc: 28%]\tLoss: 4.917393\n",
      "Train Epoch: 280 [acc: 18%]\tLoss: 6.817666\n",
      "Train Epoch: 281 [acc: 28%]\tLoss: 6.252668\n",
      "Train Epoch: 282 [acc: 22%]\tLoss: 6.085119\n",
      "Train Epoch: 283 [acc: 31%]\tLoss: 5.055080\n",
      "Train Epoch: 284 [acc: 30%]\tLoss: 5.687605\n",
      "Train Epoch: 285 [acc: 35%]\tLoss: 4.335641\n",
      "Train Epoch: 286 [acc: 24%]\tLoss: 6.039581\n",
      "Train Epoch: 287 [acc: 28%]\tLoss: 5.794769\n",
      "Train Epoch: 288 [acc: 25%]\tLoss: 5.506275\n",
      "Train Epoch: 289 [acc: 27%]\tLoss: 5.511856\n",
      "Train Epoch: 290 [acc: 30%]\tLoss: 5.530588\n",
      "Train Epoch: 291 [acc: 30%]\tLoss: 5.164877\n",
      "Train Epoch: 292 [acc: 32%]\tLoss: 5.883644\n",
      "Train Epoch: 293 [acc: 30%]\tLoss: 5.022783\n",
      "Train Epoch: 294 [acc: 29%]\tLoss: 5.133914\n",
      "Train Epoch: 295 [acc: 31%]\tLoss: 5.379717\n",
      "Train Epoch: 296 [acc: 23%]\tLoss: 5.897105\n",
      "Train Epoch: 297 [acc: 27%]\tLoss: 6.313099\n",
      "Train Epoch: 298 [acc: 29%]\tLoss: 5.831627\n",
      "Train Epoch: 299 [acc: 27%]\tLoss: 4.658700\n",
      "Train Epoch: 300 [acc: 29%]\tLoss: 5.570403\n",
      "[Trained for 300 epochs and tested on 5 sets of 2000 images]        Avg Acc: 17.05 +- 0.42 , Avg Loss: 6.79\n",
      "Train Epoch: 301 [acc: 39%]\tLoss: 5.236003\n",
      "Train Epoch: 302 [acc: 31%]\tLoss: 5.995608\n",
      "Train Epoch: 303 [acc: 28%]\tLoss: 6.142562\n",
      "Train Epoch: 304 [acc: 25%]\tLoss: 5.108100\n",
      "Train Epoch: 305 [acc: 25%]\tLoss: 5.283305\n",
      "Train Epoch: 306 [acc: 41%]\tLoss: 3.580451\n",
      "Train Epoch: 307 [acc: 26%]\tLoss: 6.588510\n",
      "Train Epoch: 308 [acc: 26%]\tLoss: 6.213340\n",
      "Train Epoch: 309 [acc: 33%]\tLoss: 5.355442\n",
      "Train Epoch: 310 [acc: 25%]\tLoss: 6.499196\n",
      "Train Epoch: 311 [acc: 31%]\tLoss: 5.179870\n",
      "Train Epoch: 312 [acc: 24%]\tLoss: 5.470512\n",
      "Train Epoch: 313 [acc: 26%]\tLoss: 6.163220\n",
      "Train Epoch: 314 [acc: 31%]\tLoss: 5.084610\n",
      "Train Epoch: 315 [acc: 33%]\tLoss: 5.384242\n",
      "Train Epoch: 316 [acc: 22%]\tLoss: 6.180185\n",
      "Train Epoch: 317 [acc: 25%]\tLoss: 6.351063\n",
      "Train Epoch: 318 [acc: 32%]\tLoss: 5.919174\n",
      "Train Epoch: 319 [acc: 29%]\tLoss: 5.541231\n",
      "Train Epoch: 320 [acc: 27%]\tLoss: 6.391681\n",
      "Train Epoch: 321 [acc: 30%]\tLoss: 5.065041\n",
      "Train Epoch: 322 [acc: 28%]\tLoss: 5.821817\n",
      "Train Epoch: 323 [acc: 24%]\tLoss: 5.704294\n",
      "Train Epoch: 324 [acc: 26%]\tLoss: 6.703463\n",
      "Train Epoch: 325 [acc: 27%]\tLoss: 4.809025\n",
      "[Trained for 325 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.54 +- 0.61 , Avg Loss: 6.77\n",
      "Train Epoch: 326 [acc: 26%]\tLoss: 6.341911\n",
      "Train Epoch: 327 [acc: 25%]\tLoss: 6.678921\n",
      "Train Epoch: 328 [acc: 32%]\tLoss: 5.586658\n",
      "Train Epoch: 329 [acc: 33%]\tLoss: 4.973360\n",
      "Train Epoch: 330 [acc: 32%]\tLoss: 6.117490\n",
      "Train Epoch: 331 [acc: 34%]\tLoss: 4.741021\n",
      "Train Epoch: 332 [acc: 37%]\tLoss: 5.320723\n",
      "Train Epoch: 333 [acc: 27%]\tLoss: 5.264589\n",
      "Train Epoch: 334 [acc: 24%]\tLoss: 5.871867\n",
      "Train Epoch: 335 [acc: 31%]\tLoss: 5.367762\n",
      "Train Epoch: 336 [acc: 33%]\tLoss: 5.885760\n",
      "Train Epoch: 337 [acc: 23%]\tLoss: 6.767280\n",
      "Train Epoch: 338 [acc: 27%]\tLoss: 6.529624\n",
      "Train Epoch: 339 [acc: 28%]\tLoss: 5.744717\n",
      "Train Epoch: 340 [acc: 34%]\tLoss: 4.689693\n",
      "Train Epoch: 341 [acc: 29%]\tLoss: 5.979549\n",
      "Train Epoch: 342 [acc: 24%]\tLoss: 6.301664\n",
      "Train Epoch: 343 [acc: 22%]\tLoss: 7.033020\n",
      "Train Epoch: 344 [acc: 29%]\tLoss: 5.889438\n",
      "Train Epoch: 345 [acc: 32%]\tLoss: 4.978953\n",
      "Train Epoch: 346 [acc: 28%]\tLoss: 5.912561\n",
      "Train Epoch: 347 [acc: 38%]\tLoss: 5.650335\n",
      "Train Epoch: 348 [acc: 35%]\tLoss: 4.810135\n",
      "Train Epoch: 349 [acc: 27%]\tLoss: 5.500281\n",
      "Train Epoch: 350 [acc: 26%]\tLoss: 5.784963\n",
      "[Trained for 350 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.92 +- 0.64 , Avg Loss: 7.19\n",
      "Train Epoch: 351 [acc: 25%]\tLoss: 5.874780\n",
      "Train Epoch: 352 [acc: 32%]\tLoss: 5.647204\n",
      "Train Epoch: 353 [acc: 26%]\tLoss: 5.643348\n",
      "Train Epoch: 354 [acc: 33%]\tLoss: 5.336261\n",
      "Train Epoch: 355 [acc: 32%]\tLoss: 5.838074\n",
      "Train Epoch: 356 [acc: 31%]\tLoss: 4.665318\n",
      "Train Epoch: 357 [acc: 27%]\tLoss: 6.031603\n",
      "Train Epoch: 358 [acc: 29%]\tLoss: 4.208302\n",
      "Train Epoch: 359 [acc: 25%]\tLoss: 7.116353\n",
      "Train Epoch: 360 [acc: 26%]\tLoss: 7.270416\n",
      "Train Epoch: 361 [acc: 32%]\tLoss: 5.123147\n",
      "Train Epoch: 362 [acc: 32%]\tLoss: 5.521852\n",
      "Train Epoch: 363 [acc: 34%]\tLoss: 5.211942\n",
      "Train Epoch: 364 [acc: 32%]\tLoss: 4.821687\n",
      "Train Epoch: 365 [acc: 38%]\tLoss: 5.269751\n",
      "Train Epoch: 366 [acc: 22%]\tLoss: 5.230669\n",
      "Train Epoch: 367 [acc: 40%]\tLoss: 4.953009\n",
      "Train Epoch: 368 [acc: 30%]\tLoss: 5.754872\n",
      "Train Epoch: 369 [acc: 27%]\tLoss: 5.748496\n",
      "Train Epoch: 370 [acc: 28%]\tLoss: 6.288488\n",
      "Train Epoch: 371 [acc: 32%]\tLoss: 5.367655\n",
      "Train Epoch: 372 [acc: 33%]\tLoss: 5.165285\n",
      "Train Epoch: 373 [acc: 34%]\tLoss: 4.565412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 374 [acc: 22%]\tLoss: 7.204317\n",
      "Train Epoch: 375 [acc: 30%]\tLoss: 5.186903\n",
      "[Trained for 375 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.95 +- 0.72 , Avg Loss: 7.14\n",
      "Train Epoch: 376 [acc: 34%]\tLoss: 5.838130\n",
      "Train Epoch: 377 [acc: 36%]\tLoss: 4.415599\n",
      "Train Epoch: 378 [acc: 32%]\tLoss: 5.442934\n",
      "Train Epoch: 379 [acc: 26%]\tLoss: 6.241819\n",
      "Train Epoch: 380 [acc: 30%]\tLoss: 6.048078\n",
      "Train Epoch: 381 [acc: 22%]\tLoss: 6.208910\n",
      "Train Epoch: 382 [acc: 25%]\tLoss: 6.227676\n",
      "Train Epoch: 383 [acc: 31%]\tLoss: 5.909729\n",
      "Train Epoch: 384 [acc: 25%]\tLoss: 5.870172\n",
      "Train Epoch: 385 [acc: 31%]\tLoss: 6.016154\n",
      "Train Epoch: 386 [acc: 31%]\tLoss: 5.423904\n",
      "Train Epoch: 387 [acc: 33%]\tLoss: 5.297634\n",
      "Train Epoch: 388 [acc: 39%]\tLoss: 4.890568\n",
      "Train Epoch: 389 [acc: 34%]\tLoss: 5.463426\n",
      "Train Epoch: 390 [acc: 30%]\tLoss: 5.584150\n",
      "Train Epoch: 391 [acc: 35%]\tLoss: 5.537448\n",
      "Train Epoch: 392 [acc: 37%]\tLoss: 5.044569\n",
      "Train Epoch: 393 [acc: 30%]\tLoss: 5.004819\n",
      "Train Epoch: 394 [acc: 23%]\tLoss: 6.042429\n",
      "Train Epoch: 395 [acc: 30%]\tLoss: 6.227993\n",
      "Train Epoch: 396 [acc: 32%]\tLoss: 5.500660\n",
      "Train Epoch: 397 [acc: 27%]\tLoss: 6.334043\n",
      "Train Epoch: 398 [acc: 34%]\tLoss: 4.530151\n",
      "Train Epoch: 399 [acc: 24%]\tLoss: 5.762216\n",
      "Train Epoch: 400 [acc: 30%]\tLoss: 5.256435\n",
      "[Trained for 400 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.30 +- 1.21 , Avg Loss: 7.47\n",
      "Train Epoch: 401 [acc: 28%]\tLoss: 5.983621\n",
      "Train Epoch: 402 [acc: 27%]\tLoss: 5.987753\n",
      "Train Epoch: 403 [acc: 33%]\tLoss: 5.910112\n",
      "Train Epoch: 404 [acc: 16%]\tLoss: 7.277317\n",
      "Train Epoch: 405 [acc: 38%]\tLoss: 4.812441\n",
      "Train Epoch: 406 [acc: 25%]\tLoss: 6.011738\n",
      "Train Epoch: 407 [acc: 32%]\tLoss: 6.123759\n",
      "Train Epoch: 408 [acc: 35%]\tLoss: 4.130288\n",
      "Train Epoch: 409 [acc: 27%]\tLoss: 5.660641\n",
      "Train Epoch: 410 [acc: 22%]\tLoss: 6.116715\n",
      "Train Epoch: 411 [acc: 31%]\tLoss: 5.547698\n",
      "Train Epoch: 412 [acc: 29%]\tLoss: 5.664390\n",
      "Train Epoch: 413 [acc: 41%]\tLoss: 4.945191\n",
      "Train Epoch: 414 [acc: 31%]\tLoss: 6.196075\n",
      "Train Epoch: 415 [acc: 24%]\tLoss: 5.309530\n",
      "Train Epoch: 416 [acc: 29%]\tLoss: 5.186952\n",
      "Train Epoch: 417 [acc: 30%]\tLoss: 5.159080\n",
      "Train Epoch: 418 [acc: 28%]\tLoss: 6.062111\n",
      "Train Epoch: 419 [acc: 22%]\tLoss: 5.790615\n",
      "Train Epoch: 420 [acc: 26%]\tLoss: 5.077556\n",
      "Train Epoch: 421 [acc: 27%]\tLoss: 5.742235\n",
      "Train Epoch: 422 [acc: 40%]\tLoss: 4.866357\n",
      "Train Epoch: 423 [acc: 33%]\tLoss: 6.038260\n",
      "Train Epoch: 424 [acc: 31%]\tLoss: 5.211266\n",
      "Train Epoch: 425 [acc: 31%]\tLoss: 5.238891\n",
      "[Trained for 425 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.30 +- 0.67 , Avg Loss: 7.77\n",
      "Train Epoch: 426 [acc: 27%]\tLoss: 6.299696\n",
      "Train Epoch: 427 [acc: 34%]\tLoss: 6.004719\n",
      "Train Epoch: 428 [acc: 32%]\tLoss: 5.144166\n",
      "Train Epoch: 429 [acc: 28%]\tLoss: 5.777782\n",
      "Train Epoch: 430 [acc: 33%]\tLoss: 5.139364\n",
      "Train Epoch: 431 [acc: 33%]\tLoss: 4.446151\n",
      "Train Epoch: 432 [acc: 31%]\tLoss: 6.431890\n",
      "Train Epoch: 433 [acc: 31%]\tLoss: 5.818548\n",
      "Train Epoch: 434 [acc: 24%]\tLoss: 6.267530\n",
      "Train Epoch: 435 [acc: 24%]\tLoss: 6.697213\n",
      "Train Epoch: 436 [acc: 27%]\tLoss: 5.679683\n",
      "Train Epoch: 437 [acc: 29%]\tLoss: 5.654633\n",
      "Train Epoch: 438 [acc: 36%]\tLoss: 4.315482\n",
      "Train Epoch: 439 [acc: 33%]\tLoss: 5.944561\n",
      "Train Epoch: 440 [acc: 32%]\tLoss: 5.323907\n",
      "Train Epoch: 441 [acc: 32%]\tLoss: 5.106077\n",
      "Train Epoch: 442 [acc: 28%]\tLoss: 6.616924\n",
      "Train Epoch: 443 [acc: 25%]\tLoss: 5.823052\n",
      "Train Epoch: 444 [acc: 40%]\tLoss: 4.485037\n",
      "Train Epoch: 445 [acc: 24%]\tLoss: 5.712923\n",
      "Train Epoch: 446 [acc: 29%]\tLoss: 4.928593\n",
      "Train Epoch: 447 [acc: 39%]\tLoss: 4.746284\n",
      "Train Epoch: 448 [acc: 33%]\tLoss: 5.377225\n",
      "Train Epoch: 449 [acc: 27%]\tLoss: 5.724256\n",
      "Train Epoch: 450 [acc: 28%]\tLoss: 4.984556\n",
      "[Trained for 450 epochs and tested on 5 sets of 2000 images]        Avg Acc: 14.81 +- 0.77 , Avg Loss: 7.63\n",
      "Train Epoch: 451 [acc: 29%]\tLoss: 6.344241\n",
      "Train Epoch: 452 [acc: 36%]\tLoss: 5.083496\n",
      "Train Epoch: 453 [acc: 30%]\tLoss: 5.445072\n",
      "Train Epoch: 454 [acc: 23%]\tLoss: 6.288154\n",
      "Train Epoch: 455 [acc: 28%]\tLoss: 5.496570\n",
      "Train Epoch: 456 [acc: 25%]\tLoss: 6.640794\n",
      "Train Epoch: 457 [acc: 33%]\tLoss: 4.862920\n",
      "Train Epoch: 458 [acc: 24%]\tLoss: 5.974077\n",
      "Train Epoch: 459 [acc: 29%]\tLoss: 5.600068\n",
      "Train Epoch: 460 [acc: 25%]\tLoss: 5.545514\n",
      "Train Epoch: 461 [acc: 35%]\tLoss: 5.078514\n",
      "Train Epoch: 462 [acc: 45%]\tLoss: 4.426680\n",
      "Train Epoch: 463 [acc: 24%]\tLoss: 5.496150\n",
      "Train Epoch: 464 [acc: 33%]\tLoss: 5.337215\n",
      "Train Epoch: 465 [acc: 35%]\tLoss: 5.013724\n",
      "Train Epoch: 466 [acc: 32%]\tLoss: 4.334780\n",
      "Train Epoch: 467 [acc: 33%]\tLoss: 5.036063\n",
      "Train Epoch: 468 [acc: 29%]\tLoss: 5.711918\n",
      "Train Epoch: 469 [acc: 31%]\tLoss: 5.068254\n",
      "Train Epoch: 470 [acc: 30%]\tLoss: 5.922647\n",
      "Train Epoch: 471 [acc: 29%]\tLoss: 5.626289\n",
      "Train Epoch: 472 [acc: 27%]\tLoss: 5.597408\n",
      "Train Epoch: 473 [acc: 27%]\tLoss: 4.930231\n",
      "Train Epoch: 474 [acc: 26%]\tLoss: 6.220660\n",
      "Train Epoch: 475 [acc: 34%]\tLoss: 5.725279\n",
      "[Trained for 475 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.46 +- 0.54 , Avg Loss: 7.57\n",
      "Train Epoch: 476 [acc: 29%]\tLoss: 5.955300\n",
      "Train Epoch: 477 [acc: 30%]\tLoss: 6.345458\n",
      "Train Epoch: 478 [acc: 24%]\tLoss: 5.523592\n",
      "Train Epoch: 479 [acc: 33%]\tLoss: 5.270529\n",
      "Train Epoch: 480 [acc: 26%]\tLoss: 5.375107\n",
      "Train Epoch: 481 [acc: 25%]\tLoss: 5.914217\n",
      "Train Epoch: 482 [acc: 30%]\tLoss: 6.336641\n",
      "Train Epoch: 483 [acc: 37%]\tLoss: 5.951385\n",
      "Train Epoch: 484 [acc: 31%]\tLoss: 5.557091\n",
      "Train Epoch: 485 [acc: 36%]\tLoss: 4.832002\n",
      "Train Epoch: 486 [acc: 26%]\tLoss: 5.603982\n",
      "Train Epoch: 487 [acc: 33%]\tLoss: 5.779104\n",
      "Train Epoch: 488 [acc: 31%]\tLoss: 5.082088\n",
      "Train Epoch: 489 [acc: 29%]\tLoss: 6.054692\n",
      "Train Epoch: 490 [acc: 31%]\tLoss: 5.696428\n",
      "Train Epoch: 491 [acc: 25%]\tLoss: 6.485293\n",
      "Train Epoch: 492 [acc: 32%]\tLoss: 5.093947\n",
      "Train Epoch: 493 [acc: 35%]\tLoss: 4.815851\n",
      "Train Epoch: 494 [acc: 35%]\tLoss: 4.930313\n",
      "Train Epoch: 495 [acc: 34%]\tLoss: 6.146402\n",
      "Train Epoch: 496 [acc: 29%]\tLoss: 5.851183\n",
      "Train Epoch: 497 [acc: 33%]\tLoss: 5.044890\n",
      "Train Epoch: 498 [acc: 27%]\tLoss: 6.786949\n",
      "Train Epoch: 499 [acc: 28%]\tLoss: 6.349371\n",
      "Train Epoch: 500 [acc: 30%]\tLoss: 6.312123\n",
      "[Trained for 500 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.16 +- 0.26 , Avg Loss: 7.85\n",
      "Train Epoch: 501 [acc: 32%]\tLoss: 4.981379\n",
      "Train Epoch: 502 [acc: 31%]\tLoss: 5.006866\n",
      "Train Epoch: 503 [acc: 30%]\tLoss: 5.495004\n",
      "Train Epoch: 504 [acc: 35%]\tLoss: 6.249243\n",
      "Train Epoch: 505 [acc: 29%]\tLoss: 5.862137\n",
      "Train Epoch: 506 [acc: 35%]\tLoss: 5.541580\n",
      "Train Epoch: 507 [acc: 25%]\tLoss: 5.953193\n",
      "Train Epoch: 508 [acc: 35%]\tLoss: 6.170541\n",
      "Train Epoch: 509 [acc: 35%]\tLoss: 4.951128\n",
      "Train Epoch: 510 [acc: 26%]\tLoss: 5.684704\n",
      "Train Epoch: 511 [acc: 30%]\tLoss: 6.216266\n",
      "Train Epoch: 512 [acc: 29%]\tLoss: 4.741005\n",
      "Train Epoch: 513 [acc: 22%]\tLoss: 6.694157\n",
      "Train Epoch: 514 [acc: 27%]\tLoss: 6.315722\n",
      "Train Epoch: 515 [acc: 30%]\tLoss: 5.801840\n",
      "Train Epoch: 516 [acc: 35%]\tLoss: 5.157306\n",
      "Train Epoch: 517 [acc: 34%]\tLoss: 4.602418\n",
      "Train Epoch: 518 [acc: 32%]\tLoss: 5.136565\n",
      "Train Epoch: 519 [acc: 39%]\tLoss: 5.282419\n",
      "Train Epoch: 520 [acc: 34%]\tLoss: 5.596032\n",
      "Train Epoch: 521 [acc: 28%]\tLoss: 6.211191\n",
      "Train Epoch: 522 [acc: 43%]\tLoss: 4.848238\n",
      "Train Epoch: 523 [acc: 29%]\tLoss: 6.076057\n",
      "Train Epoch: 524 [acc: 36%]\tLoss: 5.508704\n",
      "Train Epoch: 525 [acc: 20%]\tLoss: 6.285016\n",
      "[Trained for 525 epochs and tested on 5 sets of 2000 images]        Avg Acc: 14.89 +- 0.70 , Avg Loss: 8.15\n",
      "Train Epoch: 526 [acc: 31%]\tLoss: 6.394701\n",
      "Train Epoch: 527 [acc: 35%]\tLoss: 4.705716\n",
      "Train Epoch: 528 [acc: 34%]\tLoss: 5.574486\n",
      "Train Epoch: 529 [acc: 29%]\tLoss: 5.297941\n",
      "Train Epoch: 530 [acc: 35%]\tLoss: 5.533691\n",
      "Train Epoch: 531 [acc: 32%]\tLoss: 5.241884\n",
      "Train Epoch: 532 [acc: 24%]\tLoss: 6.596922\n",
      "Train Epoch: 533 [acc: 27%]\tLoss: 6.176190\n",
      "Train Epoch: 534 [acc: 31%]\tLoss: 5.997244\n",
      "Train Epoch: 535 [acc: 34%]\tLoss: 5.673952\n",
      "Train Epoch: 536 [acc: 28%]\tLoss: 5.135294\n",
      "Train Epoch: 537 [acc: 38%]\tLoss: 4.636806\n",
      "Train Epoch: 538 [acc: 36%]\tLoss: 4.641218\n",
      "Train Epoch: 539 [acc: 32%]\tLoss: 5.494003\n",
      "Train Epoch: 540 [acc: 29%]\tLoss: 6.133322\n",
      "Train Epoch: 541 [acc: 37%]\tLoss: 5.256866\n",
      "Train Epoch: 542 [acc: 28%]\tLoss: 5.948070\n",
      "Train Epoch: 543 [acc: 37%]\tLoss: 6.211402\n",
      "Train Epoch: 544 [acc: 31%]\tLoss: 5.054147\n",
      "Train Epoch: 545 [acc: 27%]\tLoss: 5.997556\n",
      "Train Epoch: 546 [acc: 26%]\tLoss: 6.345510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 547 [acc: 23%]\tLoss: 6.026980\n",
      "Train Epoch: 548 [acc: 23%]\tLoss: 6.200944\n",
      "Train Epoch: 549 [acc: 23%]\tLoss: 6.476707\n",
      "Train Epoch: 550 [acc: 31%]\tLoss: 5.508612\n",
      "[Trained for 550 epochs and tested on 5 sets of 2000 images]        Avg Acc: 14.75 +- 0.74 , Avg Loss: 7.84\n",
      "Train Epoch: 551 [acc: 35%]\tLoss: 5.464926\n",
      "Train Epoch: 552 [acc: 34%]\tLoss: 5.099392\n",
      "Train Epoch: 553 [acc: 27%]\tLoss: 5.341575\n",
      "Train Epoch: 554 [acc: 36%]\tLoss: 5.046067\n",
      "Train Epoch: 555 [acc: 24%]\tLoss: 5.382242\n",
      "Train Epoch: 556 [acc: 31%]\tLoss: 4.968020\n",
      "Train Epoch: 557 [acc: 39%]\tLoss: 5.002208\n",
      "Train Epoch: 558 [acc: 34%]\tLoss: 4.982231\n",
      "Train Epoch: 559 [acc: 37%]\tLoss: 4.947643\n",
      "Train Epoch: 560 [acc: 32%]\tLoss: 4.940932\n",
      "Train Epoch: 561 [acc: 25%]\tLoss: 6.750845\n",
      "Train Epoch: 562 [acc: 32%]\tLoss: 5.909590\n",
      "Train Epoch: 563 [acc: 23%]\tLoss: 5.721008\n",
      "Train Epoch: 564 [acc: 25%]\tLoss: 6.222853\n",
      "Train Epoch: 565 [acc: 32%]\tLoss: 5.611746\n",
      "Train Epoch: 566 [acc: 29%]\tLoss: 6.253826\n",
      "Train Epoch: 567 [acc: 30%]\tLoss: 5.330663\n",
      "Train Epoch: 568 [acc: 30%]\tLoss: 5.644078\n",
      "Train Epoch: 569 [acc: 31%]\tLoss: 4.758674\n",
      "Train Epoch: 570 [acc: 33%]\tLoss: 5.274935\n",
      "Train Epoch: 571 [acc: 35%]\tLoss: 5.206434\n",
      "Train Epoch: 572 [acc: 30%]\tLoss: 6.060246\n",
      "Train Epoch: 573 [acc: 36%]\tLoss: 4.517152\n",
      "Train Epoch: 574 [acc: 38%]\tLoss: 4.725694\n",
      "Train Epoch: 575 [acc: 41%]\tLoss: 5.146115\n",
      "[Trained for 575 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.86 +- 0.16 , Avg Loss: 7.73\n",
      "Train Epoch: 576 [acc: 31%]\tLoss: 6.277705\n",
      "Train Epoch: 577 [acc: 24%]\tLoss: 5.810876\n",
      "Train Epoch: 578 [acc: 28%]\tLoss: 6.109050\n",
      "Train Epoch: 579 [acc: 25%]\tLoss: 6.235944\n",
      "Train Epoch: 580 [acc: 29%]\tLoss: 5.855919\n",
      "Train Epoch: 581 [acc: 31%]\tLoss: 6.353623\n",
      "Train Epoch: 582 [acc: 26%]\tLoss: 6.241437\n",
      "Train Epoch: 583 [acc: 30%]\tLoss: 6.056672\n",
      "Train Epoch: 584 [acc: 29%]\tLoss: 5.988301\n",
      "Train Epoch: 585 [acc: 36%]\tLoss: 4.586371\n",
      "Train Epoch: 586 [acc: 31%]\tLoss: 5.518205\n",
      "Train Epoch: 587 [acc: 25%]\tLoss: 6.582761\n",
      "Train Epoch: 588 [acc: 23%]\tLoss: 5.446666\n",
      "Train Epoch: 589 [acc: 36%]\tLoss: 5.866766\n",
      "Train Epoch: 590 [acc: 21%]\tLoss: 6.490949\n",
      "Train Epoch: 591 [acc: 31%]\tLoss: 5.588072\n",
      "Train Epoch: 592 [acc: 30%]\tLoss: 5.717817\n",
      "Train Epoch: 593 [acc: 32%]\tLoss: 5.233815\n",
      "Train Epoch: 594 [acc: 28%]\tLoss: 6.506102\n",
      "Train Epoch: 595 [acc: 27%]\tLoss: 6.904474\n",
      "Train Epoch: 596 [acc: 28%]\tLoss: 4.934006\n",
      "Train Epoch: 597 [acc: 29%]\tLoss: 4.926274\n",
      "Train Epoch: 598 [acc: 33%]\tLoss: 5.678700\n",
      "Train Epoch: 599 [acc: 23%]\tLoss: 5.459589\n",
      "Train Epoch: 600 [acc: 31%]\tLoss: 5.553745\n",
      "[Trained for 600 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.44 +- 0.40 , Avg Loss: 7.67\n",
      "Train Epoch: 601 [acc: 25%]\tLoss: 5.189596\n",
      "Train Epoch: 602 [acc: 30%]\tLoss: 4.844158\n",
      "Train Epoch: 603 [acc: 31%]\tLoss: 4.061516\n",
      "Train Epoch: 604 [acc: 28%]\tLoss: 5.786194\n",
      "Train Epoch: 605 [acc: 36%]\tLoss: 5.243661\n",
      "Train Epoch: 606 [acc: 31%]\tLoss: 5.130495\n",
      "Train Epoch: 607 [acc: 28%]\tLoss: 5.358791\n",
      "Train Epoch: 608 [acc: 33%]\tLoss: 4.974442\n",
      "Train Epoch: 609 [acc: 28%]\tLoss: 5.939840\n",
      "Train Epoch: 610 [acc: 30%]\tLoss: 5.114358\n",
      "Train Epoch: 611 [acc: 26%]\tLoss: 5.454504\n",
      "Train Epoch: 612 [acc: 36%]\tLoss: 4.318612\n",
      "Train Epoch: 613 [acc: 33%]\tLoss: 5.850473\n",
      "Train Epoch: 614 [acc: 23%]\tLoss: 6.469622\n",
      "Train Epoch: 615 [acc: 31%]\tLoss: 4.477606\n",
      "Train Epoch: 616 [acc: 33%]\tLoss: 5.786595\n",
      "Train Epoch: 617 [acc: 36%]\tLoss: 5.367746\n",
      "Train Epoch: 618 [acc: 34%]\tLoss: 5.317287\n",
      "Train Epoch: 619 [acc: 26%]\tLoss: 5.842050\n",
      "Train Epoch: 620 [acc: 31%]\tLoss: 5.134778\n",
      "Train Epoch: 621 [acc: 37%]\tLoss: 4.983645\n",
      "Train Epoch: 622 [acc: 29%]\tLoss: 6.341362\n",
      "Train Epoch: 623 [acc: 31%]\tLoss: 5.394363\n",
      "Train Epoch: 624 [acc: 29%]\tLoss: 5.603779\n",
      "Train Epoch: 625 [acc: 29%]\tLoss: 5.515723\n",
      "[Trained for 625 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.44 +- 0.83 , Avg Loss: 7.58\n",
      "Train Epoch: 626 [acc: 26%]\tLoss: 5.136453\n",
      "Train Epoch: 627 [acc: 31%]\tLoss: 5.661613\n",
      "Train Epoch: 628 [acc: 26%]\tLoss: 5.817946\n",
      "Train Epoch: 629 [acc: 27%]\tLoss: 6.273818\n",
      "Train Epoch: 630 [acc: 37%]\tLoss: 4.103340\n",
      "Train Epoch: 631 [acc: 31%]\tLoss: 5.385999\n",
      "Train Epoch: 632 [acc: 35%]\tLoss: 4.999708\n",
      "Train Epoch: 633 [acc: 30%]\tLoss: 4.801932\n",
      "Train Epoch: 634 [acc: 29%]\tLoss: 5.315343\n",
      "Train Epoch: 635 [acc: 30%]\tLoss: 5.241598\n",
      "Train Epoch: 636 [acc: 34%]\tLoss: 6.643069\n",
      "Train Epoch: 637 [acc: 39%]\tLoss: 4.272429\n",
      "Train Epoch: 638 [acc: 20%]\tLoss: 5.743554\n",
      "Train Epoch: 639 [acc: 33%]\tLoss: 4.492208\n",
      "Train Epoch: 640 [acc: 32%]\tLoss: 5.104325\n",
      "Train Epoch: 641 [acc: 24%]\tLoss: 7.324398\n",
      "Train Epoch: 642 [acc: 38%]\tLoss: 5.806481\n",
      "Train Epoch: 643 [acc: 27%]\tLoss: 6.374927\n",
      "Train Epoch: 644 [acc: 24%]\tLoss: 5.998501\n",
      "Train Epoch: 645 [acc: 34%]\tLoss: 5.219173\n",
      "Train Epoch: 646 [acc: 34%]\tLoss: 5.340915\n",
      "Train Epoch: 647 [acc: 30%]\tLoss: 6.223491\n",
      "Train Epoch: 648 [acc: 33%]\tLoss: 5.181690\n",
      "Train Epoch: 649 [acc: 26%]\tLoss: 5.589151\n",
      "Train Epoch: 650 [acc: 31%]\tLoss: 5.546627\n",
      "[Trained for 650 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.35 +- 0.52 , Avg Loss: 7.72\n",
      "Train Epoch: 651 [acc: 29%]\tLoss: 4.737842\n",
      "Train Epoch: 652 [acc: 32%]\tLoss: 5.379456\n",
      "Train Epoch: 653 [acc: 32%]\tLoss: 5.215221\n",
      "Train Epoch: 654 [acc: 25%]\tLoss: 5.933372\n",
      "Train Epoch: 655 [acc: 33%]\tLoss: 5.271680\n",
      "Train Epoch: 656 [acc: 24%]\tLoss: 5.689958\n",
      "Train Epoch: 657 [acc: 24%]\tLoss: 5.488514\n",
      "Train Epoch: 658 [acc: 37%]\tLoss: 5.531111\n",
      "Train Epoch: 659 [acc: 27%]\tLoss: 5.609415\n",
      "Train Epoch: 660 [acc: 26%]\tLoss: 5.352577\n",
      "Train Epoch: 661 [acc: 38%]\tLoss: 5.856870\n",
      "Train Epoch: 662 [acc: 34%]\tLoss: 4.627305\n",
      "Train Epoch: 663 [acc: 39%]\tLoss: 4.815835\n",
      "Train Epoch: 664 [acc: 26%]\tLoss: 5.669969\n",
      "Train Epoch: 665 [acc: 32%]\tLoss: 6.093768\n",
      "Train Epoch: 666 [acc: 30%]\tLoss: 5.835121\n",
      "Train Epoch: 667 [acc: 26%]\tLoss: 5.602812\n",
      "Train Epoch: 668 [acc: 30%]\tLoss: 5.916215\n",
      "Train Epoch: 669 [acc: 28%]\tLoss: 4.891223\n",
      "Train Epoch: 670 [acc: 28%]\tLoss: 5.555931\n",
      "Train Epoch: 671 [acc: 42%]\tLoss: 4.878332\n",
      "Train Epoch: 672 [acc: 33%]\tLoss: 5.256512\n",
      "Train Epoch: 673 [acc: 34%]\tLoss: 5.466807\n",
      "Train Epoch: 674 [acc: 37%]\tLoss: 4.817673\n",
      "Train Epoch: 675 [acc: 26%]\tLoss: 5.764020\n",
      "[Trained for 675 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.45 +- 0.45 , Avg Loss: 7.75\n",
      "Train Epoch: 676 [acc: 31%]\tLoss: 6.088224\n",
      "Train Epoch: 677 [acc: 27%]\tLoss: 5.285632\n",
      "Train Epoch: 678 [acc: 27%]\tLoss: 6.472251\n",
      "Train Epoch: 679 [acc: 23%]\tLoss: 5.788257\n",
      "Train Epoch: 680 [acc: 33%]\tLoss: 5.006743\n",
      "Train Epoch: 681 [acc: 24%]\tLoss: 5.766735\n",
      "Train Epoch: 682 [acc: 28%]\tLoss: 5.648976\n",
      "Train Epoch: 683 [acc: 28%]\tLoss: 5.714756\n",
      "Train Epoch: 684 [acc: 31%]\tLoss: 5.434257\n",
      "Train Epoch: 685 [acc: 21%]\tLoss: 7.524863\n",
      "Train Epoch: 686 [acc: 25%]\tLoss: 6.123705\n",
      "Train Epoch: 687 [acc: 36%]\tLoss: 5.908210\n",
      "Train Epoch: 688 [acc: 31%]\tLoss: 4.862901\n",
      "Train Epoch: 689 [acc: 43%]\tLoss: 3.946272\n",
      "Train Epoch: 690 [acc: 33%]\tLoss: 5.155575\n",
      "Train Epoch: 691 [acc: 33%]\tLoss: 5.796871\n",
      "Train Epoch: 692 [acc: 28%]\tLoss: 5.743750\n",
      "Train Epoch: 693 [acc: 27%]\tLoss: 5.993302\n",
      "Train Epoch: 694 [acc: 20%]\tLoss: 6.713533\n",
      "Train Epoch: 695 [acc: 25%]\tLoss: 5.837365\n",
      "Train Epoch: 696 [acc: 21%]\tLoss: 5.312492\n",
      "Train Epoch: 697 [acc: 33%]\tLoss: 5.615564\n",
      "Train Epoch: 698 [acc: 25%]\tLoss: 5.133656\n",
      "Train Epoch: 699 [acc: 38%]\tLoss: 4.830028\n",
      "Train Epoch: 700 [acc: 39%]\tLoss: 5.186372\n",
      "[Trained for 700 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.78 +- 0.65 , Avg Loss: 7.83\n",
      "Train Epoch: 701 [acc: 35%]\tLoss: 5.248784\n",
      "Train Epoch: 702 [acc: 37%]\tLoss: 4.735232\n",
      "Train Epoch: 703 [acc: 28%]\tLoss: 6.041486\n",
      "Train Epoch: 704 [acc: 26%]\tLoss: 6.222589\n",
      "Train Epoch: 705 [acc: 42%]\tLoss: 5.090592\n",
      "Train Epoch: 706 [acc: 34%]\tLoss: 5.046110\n",
      "Train Epoch: 707 [acc: 36%]\tLoss: 4.802289\n",
      "Train Epoch: 708 [acc: 35%]\tLoss: 3.851596\n",
      "Train Epoch: 709 [acc: 26%]\tLoss: 5.469327\n",
      "Train Epoch: 710 [acc: 26%]\tLoss: 6.783805\n",
      "Train Epoch: 711 [acc: 35%]\tLoss: 5.314368\n",
      "Train Epoch: 712 [acc: 30%]\tLoss: 6.779050\n",
      "Train Epoch: 713 [acc: 35%]\tLoss: 5.410645\n",
      "Train Epoch: 714 [acc: 32%]\tLoss: 5.373406\n",
      "Train Epoch: 715 [acc: 35%]\tLoss: 4.608362\n",
      "Train Epoch: 716 [acc: 29%]\tLoss: 6.103158\n",
      "Train Epoch: 717 [acc: 26%]\tLoss: 6.108387\n",
      "Train Epoch: 718 [acc: 31%]\tLoss: 4.976498\n",
      "Train Epoch: 719 [acc: 33%]\tLoss: 5.123276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 720 [acc: 33%]\tLoss: 5.728374\n",
      "Train Epoch: 721 [acc: 31%]\tLoss: 6.164769\n",
      "Train Epoch: 722 [acc: 35%]\tLoss: 6.107947\n",
      "Train Epoch: 723 [acc: 39%]\tLoss: 5.438971\n",
      "Train Epoch: 724 [acc: 38%]\tLoss: 3.989581\n",
      "Train Epoch: 725 [acc: 31%]\tLoss: 5.882509\n",
      "[Trained for 725 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.69 +- 0.69 , Avg Loss: 7.89\n",
      "Train Epoch: 726 [acc: 39%]\tLoss: 4.911623\n",
      "Train Epoch: 727 [acc: 31%]\tLoss: 6.136773\n",
      "Train Epoch: 728 [acc: 30%]\tLoss: 5.279458\n",
      "Train Epoch: 729 [acc: 28%]\tLoss: 5.650531\n",
      "Train Epoch: 730 [acc: 34%]\tLoss: 5.287696\n",
      "Train Epoch: 731 [acc: 27%]\tLoss: 7.136525\n",
      "Train Epoch: 732 [acc: 35%]\tLoss: 4.185211\n",
      "Train Epoch: 733 [acc: 26%]\tLoss: 5.091441\n",
      "Train Epoch: 734 [acc: 24%]\tLoss: 6.134154\n",
      "Train Epoch: 735 [acc: 38%]\tLoss: 4.947012\n",
      "Train Epoch: 736 [acc: 28%]\tLoss: 5.636404\n",
      "Train Epoch: 737 [acc: 31%]\tLoss: 5.341041\n",
      "Train Epoch: 738 [acc: 33%]\tLoss: 6.374817\n",
      "Train Epoch: 739 [acc: 26%]\tLoss: 5.848241\n",
      "Train Epoch: 740 [acc: 35%]\tLoss: 5.399270\n",
      "Train Epoch: 741 [acc: 26%]\tLoss: 5.697360\n",
      "Train Epoch: 742 [acc: 28%]\tLoss: 5.208354\n",
      "Train Epoch: 743 [acc: 40%]\tLoss: 5.954337\n",
      "Train Epoch: 744 [acc: 27%]\tLoss: 5.305352\n",
      "Train Epoch: 745 [acc: 36%]\tLoss: 5.003558\n",
      "Train Epoch: 746 [acc: 40%]\tLoss: 4.587513\n",
      "Train Epoch: 747 [acc: 37%]\tLoss: 5.425934\n",
      "Train Epoch: 748 [acc: 37%]\tLoss: 5.447854\n",
      "Train Epoch: 749 [acc: 34%]\tLoss: 5.990723\n",
      "Train Epoch: 750 [acc: 27%]\tLoss: 5.393430\n",
      "[Trained for 750 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.55 +- 0.75 , Avg Loss: 8.19\n",
      "Train Epoch: 751 [acc: 30%]\tLoss: 5.963536\n",
      "Train Epoch: 752 [acc: 35%]\tLoss: 4.794393\n",
      "Train Epoch: 753 [acc: 28%]\tLoss: 5.912120\n",
      "Train Epoch: 754 [acc: 26%]\tLoss: 6.946429\n",
      "Train Epoch: 755 [acc: 35%]\tLoss: 5.090463\n",
      "Train Epoch: 756 [acc: 20%]\tLoss: 6.373662\n",
      "Train Epoch: 757 [acc: 30%]\tLoss: 6.633448\n",
      "Train Epoch: 758 [acc: 32%]\tLoss: 5.262923\n",
      "Train Epoch: 759 [acc: 27%]\tLoss: 6.331779\n",
      "Train Epoch: 760 [acc: 28%]\tLoss: 5.154430\n",
      "Train Epoch: 761 [acc: 30%]\tLoss: 4.936262\n",
      "Train Epoch: 762 [acc: 37%]\tLoss: 5.018253\n",
      "Train Epoch: 763 [acc: 27%]\tLoss: 5.846652\n",
      "Train Epoch: 764 [acc: 29%]\tLoss: 5.356617\n",
      "Train Epoch: 765 [acc: 33%]\tLoss: 5.105091\n",
      "Train Epoch: 766 [acc: 32%]\tLoss: 4.917439\n",
      "Train Epoch: 767 [acc: 26%]\tLoss: 5.508434\n",
      "Train Epoch: 768 [acc: 35%]\tLoss: 5.380015\n",
      "Train Epoch: 769 [acc: 31%]\tLoss: 5.681028\n",
      "Train Epoch: 770 [acc: 32%]\tLoss: 4.653872\n",
      "Train Epoch: 771 [acc: 35%]\tLoss: 3.934998\n",
      "Train Epoch: 772 [acc: 25%]\tLoss: 5.901805\n",
      "Train Epoch: 773 [acc: 33%]\tLoss: 5.590757\n",
      "Train Epoch: 774 [acc: 26%]\tLoss: 5.049411\n",
      "Train Epoch: 775 [acc: 33%]\tLoss: 6.348119\n",
      "[Trained for 775 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.68 +- 0.41 , Avg Loss: 7.89\n",
      "Train Epoch: 776 [acc: 31%]\tLoss: 5.524891\n",
      "Train Epoch: 777 [acc: 38%]\tLoss: 5.671539\n",
      "Train Epoch: 778 [acc: 40%]\tLoss: 4.531760\n",
      "Train Epoch: 779 [acc: 33%]\tLoss: 5.087575\n",
      "Train Epoch: 780 [acc: 25%]\tLoss: 5.988811\n",
      "Train Epoch: 781 [acc: 31%]\tLoss: 5.824701\n",
      "Train Epoch: 782 [acc: 31%]\tLoss: 5.666897\n",
      "Train Epoch: 783 [acc: 26%]\tLoss: 4.988775\n",
      "Train Epoch: 784 [acc: 28%]\tLoss: 5.760049\n",
      "Train Epoch: 785 [acc: 28%]\tLoss: 5.294159\n",
      "Train Epoch: 786 [acc: 31%]\tLoss: 5.363846\n",
      "Train Epoch: 787 [acc: 28%]\tLoss: 6.000820\n",
      "Train Epoch: 788 [acc: 26%]\tLoss: 6.934284\n",
      "Train Epoch: 789 [acc: 31%]\tLoss: 5.489550\n",
      "Train Epoch: 790 [acc: 35%]\tLoss: 4.862936\n",
      "Train Epoch: 791 [acc: 35%]\tLoss: 5.473022\n",
      "Train Epoch: 792 [acc: 25%]\tLoss: 5.855761\n",
      "Train Epoch: 793 [acc: 29%]\tLoss: 5.581483\n",
      "Train Epoch: 794 [acc: 39%]\tLoss: 5.904628\n",
      "Train Epoch: 795 [acc: 30%]\tLoss: 5.379226\n",
      "Train Epoch: 796 [acc: 33%]\tLoss: 4.375423\n",
      "Train Epoch: 797 [acc: 26%]\tLoss: 5.541183\n",
      "Train Epoch: 798 [acc: 35%]\tLoss: 5.429111\n",
      "Train Epoch: 799 [acc: 30%]\tLoss: 5.647923\n",
      "Train Epoch: 800 [acc: 36%]\tLoss: 4.452779\n",
      "[Trained for 800 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.79 +- 0.67 , Avg Loss: 7.86\n",
      "Train Epoch: 801 [acc: 30%]\tLoss: 5.348899\n",
      "Train Epoch: 802 [acc: 22%]\tLoss: 6.286591\n",
      "Train Epoch: 803 [acc: 29%]\tLoss: 5.800491\n",
      "Train Epoch: 804 [acc: 29%]\tLoss: 5.891930\n",
      "Train Epoch: 805 [acc: 29%]\tLoss: 6.016696\n",
      "Train Epoch: 806 [acc: 37%]\tLoss: 5.416078\n",
      "Train Epoch: 807 [acc: 30%]\tLoss: 5.403760\n",
      "Train Epoch: 808 [acc: 31%]\tLoss: 5.830493\n",
      "Train Epoch: 809 [acc: 34%]\tLoss: 5.214909\n",
      "Train Epoch: 810 [acc: 34%]\tLoss: 5.379181\n",
      "Train Epoch: 811 [acc: 31%]\tLoss: 5.016971\n",
      "Train Epoch: 812 [acc: 21%]\tLoss: 5.793778\n",
      "Train Epoch: 813 [acc: 22%]\tLoss: 6.224906\n",
      "Train Epoch: 814 [acc: 31%]\tLoss: 5.325415\n",
      "Train Epoch: 815 [acc: 27%]\tLoss: 6.354237\n",
      "Train Epoch: 816 [acc: 35%]\tLoss: 4.542963\n",
      "Train Epoch: 817 [acc: 25%]\tLoss: 6.189245\n",
      "Train Epoch: 818 [acc: 26%]\tLoss: 5.453117\n",
      "Train Epoch: 819 [acc: 37%]\tLoss: 5.325225\n",
      "Train Epoch: 820 [acc: 28%]\tLoss: 6.267016\n",
      "Train Epoch: 821 [acc: 30%]\tLoss: 4.902081\n",
      "Train Epoch: 822 [acc: 26%]\tLoss: 6.096710\n",
      "Train Epoch: 823 [acc: 29%]\tLoss: 5.760520\n",
      "Train Epoch: 824 [acc: 26%]\tLoss: 5.904484\n",
      "Train Epoch: 825 [acc: 32%]\tLoss: 6.995305\n",
      "[Trained for 825 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.28 +- 1.07 , Avg Loss: 7.82\n",
      "Train Epoch: 826 [acc: 30%]\tLoss: 5.253343\n",
      "Train Epoch: 827 [acc: 30%]\tLoss: 5.911896\n",
      "Train Epoch: 828 [acc: 28%]\tLoss: 5.911126\n",
      "Train Epoch: 829 [acc: 26%]\tLoss: 6.355714\n",
      "Train Epoch: 830 [acc: 35%]\tLoss: 5.459577\n",
      "Train Epoch: 831 [acc: 24%]\tLoss: 6.288881\n",
      "Train Epoch: 832 [acc: 36%]\tLoss: 5.588891\n",
      "Train Epoch: 833 [acc: 25%]\tLoss: 5.794229\n",
      "Train Epoch: 834 [acc: 29%]\tLoss: 4.752357\n",
      "Train Epoch: 835 [acc: 31%]\tLoss: 5.843369\n",
      "Train Epoch: 836 [acc: 36%]\tLoss: 5.080678\n",
      "Train Epoch: 837 [acc: 24%]\tLoss: 6.055309\n",
      "Train Epoch: 838 [acc: 34%]\tLoss: 5.654778\n",
      "Train Epoch: 839 [acc: 26%]\tLoss: 5.877117\n",
      "Train Epoch: 840 [acc: 40%]\tLoss: 5.095019\n",
      "Train Epoch: 841 [acc: 35%]\tLoss: 5.723037\n",
      "Train Epoch: 842 [acc: 28%]\tLoss: 5.764947\n",
      "Train Epoch: 843 [acc: 37%]\tLoss: 4.727619\n",
      "Train Epoch: 844 [acc: 24%]\tLoss: 6.562476\n",
      "Train Epoch: 845 [acc: 31%]\tLoss: 5.525732\n",
      "Train Epoch: 846 [acc: 24%]\tLoss: 5.972517\n",
      "Train Epoch: 847 [acc: 29%]\tLoss: 5.334137\n",
      "Train Epoch: 848 [acc: 32%]\tLoss: 4.970870\n",
      "Train Epoch: 849 [acc: 34%]\tLoss: 5.783809\n",
      "Train Epoch: 850 [acc: 34%]\tLoss: 5.130748\n",
      "[Trained for 850 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.67 +- 0.95 , Avg Loss: 7.90\n",
      "Train Epoch: 851 [acc: 35%]\tLoss: 5.039859\n",
      "Train Epoch: 852 [acc: 36%]\tLoss: 4.906016\n",
      "Train Epoch: 853 [acc: 21%]\tLoss: 5.001200\n",
      "Train Epoch: 854 [acc: 34%]\tLoss: 5.223145\n",
      "Train Epoch: 855 [acc: 32%]\tLoss: 5.968506\n",
      "Train Epoch: 856 [acc: 36%]\tLoss: 4.600701\n",
      "Train Epoch: 857 [acc: 24%]\tLoss: 5.988481\n",
      "Train Epoch: 858 [acc: 39%]\tLoss: 4.250429\n",
      "Train Epoch: 859 [acc: 24%]\tLoss: 6.771675\n",
      "Train Epoch: 860 [acc: 33%]\tLoss: 6.176106\n",
      "Train Epoch: 861 [acc: 32%]\tLoss: 5.287410\n",
      "Train Epoch: 862 [acc: 31%]\tLoss: 5.476748\n",
      "Train Epoch: 863 [acc: 33%]\tLoss: 5.387133\n",
      "Train Epoch: 864 [acc: 29%]\tLoss: 6.043077\n",
      "Train Epoch: 865 [acc: 22%]\tLoss: 6.457833\n",
      "Train Epoch: 866 [acc: 29%]\tLoss: 5.378045\n",
      "Train Epoch: 867 [acc: 29%]\tLoss: 4.681765\n",
      "Train Epoch: 868 [acc: 27%]\tLoss: 6.401980\n",
      "Train Epoch: 869 [acc: 38%]\tLoss: 4.809263\n",
      "Train Epoch: 870 [acc: 35%]\tLoss: 5.836521\n",
      "Train Epoch: 871 [acc: 31%]\tLoss: 5.030119\n",
      "Train Epoch: 872 [acc: 23%]\tLoss: 4.940647\n",
      "Train Epoch: 873 [acc: 27%]\tLoss: 5.421517\n",
      "Train Epoch: 874 [acc: 23%]\tLoss: 6.802671\n",
      "Train Epoch: 875 [acc: 33%]\tLoss: 6.099876\n",
      "[Trained for 875 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.19 +- 0.50 , Avg Loss: 8.33\n",
      "Train Epoch: 876 [acc: 26%]\tLoss: 5.613764\n",
      "Train Epoch: 877 [acc: 29%]\tLoss: 6.087188\n",
      "Train Epoch: 878 [acc: 31%]\tLoss: 5.687016\n",
      "Train Epoch: 879 [acc: 23%]\tLoss: 7.159202\n",
      "Train Epoch: 880 [acc: 32%]\tLoss: 5.848675\n",
      "Train Epoch: 881 [acc: 30%]\tLoss: 5.097929\n",
      "Train Epoch: 882 [acc: 35%]\tLoss: 6.517138\n",
      "Train Epoch: 883 [acc: 32%]\tLoss: 5.613455\n",
      "Train Epoch: 884 [acc: 37%]\tLoss: 4.868242\n",
      "Train Epoch: 885 [acc: 31%]\tLoss: 5.598080\n",
      "Train Epoch: 886 [acc: 34%]\tLoss: 5.358801\n",
      "Train Epoch: 887 [acc: 33%]\tLoss: 5.051442\n",
      "Train Epoch: 888 [acc: 28%]\tLoss: 6.192961\n",
      "Train Epoch: 889 [acc: 25%]\tLoss: 5.912496\n",
      "Train Epoch: 890 [acc: 32%]\tLoss: 5.321249\n",
      "Train Epoch: 891 [acc: 27%]\tLoss: 6.426752\n",
      "Train Epoch: 892 [acc: 28%]\tLoss: 5.858488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 893 [acc: 30%]\tLoss: 5.328732\n",
      "Train Epoch: 894 [acc: 36%]\tLoss: 5.009834\n",
      "Train Epoch: 895 [acc: 32%]\tLoss: 5.371693\n",
      "Train Epoch: 896 [acc: 27%]\tLoss: 5.778114\n",
      "Train Epoch: 897 [acc: 25%]\tLoss: 6.864463\n",
      "Train Epoch: 898 [acc: 29%]\tLoss: 5.675755\n",
      "Train Epoch: 899 [acc: 29%]\tLoss: 5.810035\n",
      "Train Epoch: 900 [acc: 22%]\tLoss: 7.059555\n",
      "[Trained for 900 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.92 +- 0.53 , Avg Loss: 8.17\n",
      "Train Epoch: 901 [acc: 33%]\tLoss: 5.577340\n",
      "Train Epoch: 902 [acc: 27%]\tLoss: 6.607209\n",
      "Train Epoch: 903 [acc: 30%]\tLoss: 5.393483\n",
      "Train Epoch: 904 [acc: 36%]\tLoss: 4.986413\n",
      "Train Epoch: 905 [acc: 40%]\tLoss: 4.472353\n",
      "Train Epoch: 906 [acc: 31%]\tLoss: 6.350215\n",
      "Train Epoch: 907 [acc: 34%]\tLoss: 5.545057\n",
      "Train Epoch: 908 [acc: 28%]\tLoss: 6.301169\n",
      "Train Epoch: 909 [acc: 35%]\tLoss: 5.371226\n",
      "Train Epoch: 910 [acc: 31%]\tLoss: 5.457590\n",
      "Train Epoch: 911 [acc: 37%]\tLoss: 5.913306\n",
      "Train Epoch: 912 [acc: 28%]\tLoss: 6.361075\n",
      "Train Epoch: 913 [acc: 36%]\tLoss: 6.041842\n",
      "Train Epoch: 914 [acc: 25%]\tLoss: 5.718917\n",
      "Train Epoch: 915 [acc: 32%]\tLoss: 5.881112\n",
      "Train Epoch: 916 [acc: 31%]\tLoss: 5.334761\n",
      "Train Epoch: 917 [acc: 30%]\tLoss: 6.337689\n",
      "Train Epoch: 918 [acc: 24%]\tLoss: 7.453694\n",
      "Train Epoch: 919 [acc: 28%]\tLoss: 7.213663\n",
      "Train Epoch: 920 [acc: 35%]\tLoss: 4.642690\n",
      "Train Epoch: 921 [acc: 26%]\tLoss: 5.924162\n",
      "Train Epoch: 922 [acc: 23%]\tLoss: 6.478357\n",
      "Train Epoch: 923 [acc: 31%]\tLoss: 5.537302\n",
      "Train Epoch: 924 [acc: 28%]\tLoss: 5.803389\n",
      "Train Epoch: 925 [acc: 33%]\tLoss: 5.630766\n",
      "[Trained for 925 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.61 +- 0.24 , Avg Loss: 7.95\n",
      "Train Epoch: 926 [acc: 33%]\tLoss: 6.153347\n",
      "Train Epoch: 927 [acc: 33%]\tLoss: 5.588792\n",
      "Train Epoch: 928 [acc: 35%]\tLoss: 5.314263\n",
      "Train Epoch: 929 [acc: 33%]\tLoss: 5.572303\n",
      "Train Epoch: 930 [acc: 29%]\tLoss: 4.960170\n",
      "Train Epoch: 931 [acc: 23%]\tLoss: 7.111210\n",
      "Train Epoch: 932 [acc: 24%]\tLoss: 5.939890\n",
      "Train Epoch: 933 [acc: 31%]\tLoss: 6.538668\n",
      "Train Epoch: 934 [acc: 34%]\tLoss: 5.274971\n",
      "Train Epoch: 935 [acc: 25%]\tLoss: 5.599053\n",
      "Train Epoch: 936 [acc: 37%]\tLoss: 5.203509\n",
      "Train Epoch: 937 [acc: 21%]\tLoss: 5.352780\n",
      "Train Epoch: 938 [acc: 32%]\tLoss: 5.672339\n",
      "Train Epoch: 939 [acc: 33%]\tLoss: 5.382784\n",
      "Train Epoch: 940 [acc: 37%]\tLoss: 5.705620\n",
      "Train Epoch: 941 [acc: 25%]\tLoss: 5.961241\n",
      "Train Epoch: 942 [acc: 38%]\tLoss: 6.468348\n",
      "Train Epoch: 943 [acc: 32%]\tLoss: 5.720995\n",
      "Train Epoch: 944 [acc: 31%]\tLoss: 5.968728\n",
      "Train Epoch: 945 [acc: 29%]\tLoss: 6.108929\n",
      "Train Epoch: 946 [acc: 35%]\tLoss: 5.814416\n",
      "Train Epoch: 947 [acc: 25%]\tLoss: 6.179680\n",
      "Train Epoch: 948 [acc: 35%]\tLoss: 4.763665\n",
      "Train Epoch: 949 [acc: 31%]\tLoss: 5.439992\n",
      "Train Epoch: 950 [acc: 32%]\tLoss: 5.294892\n",
      "[Trained for 950 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.49 +- 0.67 , Avg Loss: 7.74\n",
      "Train Epoch: 951 [acc: 27%]\tLoss: 6.230348\n",
      "Train Epoch: 952 [acc: 20%]\tLoss: 6.248873\n",
      "Train Epoch: 953 [acc: 30%]\tLoss: 5.817526\n",
      "Train Epoch: 954 [acc: 28%]\tLoss: 5.385629\n",
      "Train Epoch: 955 [acc: 32%]\tLoss: 5.648046\n",
      "Train Epoch: 956 [acc: 23%]\tLoss: 6.266595\n",
      "Train Epoch: 957 [acc: 30%]\tLoss: 5.636595\n",
      "Train Epoch: 958 [acc: 24%]\tLoss: 6.069307\n",
      "Train Epoch: 959 [acc: 35%]\tLoss: 4.988849\n",
      "Train Epoch: 960 [acc: 26%]\tLoss: 6.283942\n",
      "Train Epoch: 961 [acc: 32%]\tLoss: 5.299381\n",
      "Train Epoch: 962 [acc: 36%]\tLoss: 5.821767\n",
      "Train Epoch: 963 [acc: 21%]\tLoss: 6.774035\n",
      "Train Epoch: 964 [acc: 30%]\tLoss: 6.250573\n",
      "Train Epoch: 965 [acc: 26%]\tLoss: 4.975407\n",
      "Train Epoch: 966 [acc: 30%]\tLoss: 4.964378\n",
      "Train Epoch: 967 [acc: 29%]\tLoss: 5.552041\n",
      "Train Epoch: 968 [acc: 24%]\tLoss: 5.617207\n",
      "Train Epoch: 969 [acc: 29%]\tLoss: 5.935142\n",
      "Train Epoch: 970 [acc: 33%]\tLoss: 4.878736\n",
      "Train Epoch: 971 [acc: 26%]\tLoss: 6.720494\n",
      "Train Epoch: 972 [acc: 29%]\tLoss: 6.341514\n",
      "Train Epoch: 973 [acc: 27%]\tLoss: 6.201058\n",
      "Train Epoch: 974 [acc: 35%]\tLoss: 4.910735\n",
      "Train Epoch: 975 [acc: 40%]\tLoss: 5.102232\n",
      "[Trained for 975 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.01 +- 0.42 , Avg Loss: 7.60\n",
      "Train Epoch: 976 [acc: 24%]\tLoss: 6.349163\n",
      "Train Epoch: 977 [acc: 26%]\tLoss: 6.542869\n",
      "Train Epoch: 978 [acc: 34%]\tLoss: 4.845462\n",
      "Train Epoch: 979 [acc: 32%]\tLoss: 4.877924\n",
      "Train Epoch: 980 [acc: 28%]\tLoss: 6.100132\n",
      "Train Epoch: 981 [acc: 33%]\tLoss: 5.509033\n",
      "Train Epoch: 982 [acc: 24%]\tLoss: 6.269988\n",
      "Train Epoch: 983 [acc: 27%]\tLoss: 5.771181\n",
      "Train Epoch: 984 [acc: 25%]\tLoss: 5.148790\n",
      "Train Epoch: 985 [acc: 22%]\tLoss: 5.932818\n",
      "Train Epoch: 986 [acc: 37%]\tLoss: 5.303604\n",
      "Train Epoch: 987 [acc: 35%]\tLoss: 5.367661\n",
      "Train Epoch: 988 [acc: 29%]\tLoss: 5.186687\n",
      "Train Epoch: 989 [acc: 29%]\tLoss: 5.969832\n",
      "Train Epoch: 990 [acc: 23%]\tLoss: 6.265476\n",
      "Train Epoch: 991 [acc: 32%]\tLoss: 5.286605\n",
      "Train Epoch: 992 [acc: 25%]\tLoss: 5.354444\n",
      "Train Epoch: 993 [acc: 28%]\tLoss: 6.344816\n",
      "Train Epoch: 994 [acc: 25%]\tLoss: 5.347011\n",
      "Train Epoch: 995 [acc: 28%]\tLoss: 5.665519\n",
      "Train Epoch: 996 [acc: 31%]\tLoss: 6.087873\n",
      "Train Epoch: 997 [acc: 33%]\tLoss: 5.498529\n",
      "Train Epoch: 998 [acc: 31%]\tLoss: 5.096569\n",
      "Train Epoch: 999 [acc: 29%]\tLoss: 5.735974\n",
      "Train Epoch: 1000 [acc: 27%]\tLoss: 6.990483\n",
      "[Trained for 1000 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.62 +- 0.86 , Avg Loss: 7.72\n",
      "Train Epoch: 1001 [acc: 32%]\tLoss: 5.674963\n",
      "Train Epoch: 1002 [acc: 34%]\tLoss: 5.373993\n",
      "Train Epoch: 1003 [acc: 34%]\tLoss: 5.270743\n",
      "Train Epoch: 1004 [acc: 23%]\tLoss: 6.496559\n",
      "Train Epoch: 1005 [acc: 24%]\tLoss: 5.492977\n",
      "Train Epoch: 1006 [acc: 23%]\tLoss: 6.391268\n",
      "Train Epoch: 1007 [acc: 31%]\tLoss: 5.853535\n",
      "Train Epoch: 1008 [acc: 31%]\tLoss: 4.221985\n",
      "Train Epoch: 1009 [acc: 28%]\tLoss: 4.931376\n",
      "Train Epoch: 1010 [acc: 26%]\tLoss: 5.289703\n",
      "Train Epoch: 1011 [acc: 30%]\tLoss: 5.094242\n",
      "Train Epoch: 1012 [acc: 31%]\tLoss: 5.032646\n",
      "Train Epoch: 1013 [acc: 35%]\tLoss: 4.765832\n",
      "Train Epoch: 1014 [acc: 33%]\tLoss: 5.834212\n",
      "Train Epoch: 1015 [acc: 29%]\tLoss: 5.475923\n",
      "Train Epoch: 1016 [acc: 30%]\tLoss: 5.184396\n",
      "Train Epoch: 1017 [acc: 36%]\tLoss: 4.845483\n",
      "Train Epoch: 1018 [acc: 30%]\tLoss: 5.124520\n",
      "Train Epoch: 1019 [acc: 33%]\tLoss: 5.979379\n",
      "Train Epoch: 1020 [acc: 38%]\tLoss: 3.912100\n",
      "Train Epoch: 1021 [acc: 30%]\tLoss: 5.471143\n",
      "Train Epoch: 1022 [acc: 29%]\tLoss: 5.653962\n",
      "Train Epoch: 1023 [acc: 27%]\tLoss: 5.323866\n",
      "Train Epoch: 1024 [acc: 23%]\tLoss: 6.717157\n",
      "Train Epoch: 1025 [acc: 43%]\tLoss: 5.123346\n",
      "[Trained for 1025 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.54 +- 0.37 , Avg Loss: 8.29\n",
      "Train Epoch: 1026 [acc: 30%]\tLoss: 6.198053\n",
      "Train Epoch: 1027 [acc: 30%]\tLoss: 5.950540\n",
      "Train Epoch: 1028 [acc: 30%]\tLoss: 5.196669\n",
      "Train Epoch: 1029 [acc: 29%]\tLoss: 5.779402\n",
      "Train Epoch: 1030 [acc: 42%]\tLoss: 5.717068\n",
      "Train Epoch: 1031 [acc: 31%]\tLoss: 6.095479\n",
      "Train Epoch: 1032 [acc: 28%]\tLoss: 6.038864\n",
      "Train Epoch: 1033 [acc: 33%]\tLoss: 5.239447\n",
      "Train Epoch: 1034 [acc: 34%]\tLoss: 5.808964\n",
      "Train Epoch: 1035 [acc: 30%]\tLoss: 6.128926\n",
      "Train Epoch: 1036 [acc: 32%]\tLoss: 5.628148\n",
      "Train Epoch: 1037 [acc: 36%]\tLoss: 5.600765\n",
      "Train Epoch: 1038 [acc: 31%]\tLoss: 5.712001\n",
      "Train Epoch: 1039 [acc: 35%]\tLoss: 4.718445\n",
      "Train Epoch: 1040 [acc: 25%]\tLoss: 6.461390\n",
      "Train Epoch: 1041 [acc: 28%]\tLoss: 7.257299\n",
      "Train Epoch: 1042 [acc: 24%]\tLoss: 5.253253\n",
      "Train Epoch: 1043 [acc: 25%]\tLoss: 6.185292\n",
      "Train Epoch: 1044 [acc: 24%]\tLoss: 6.269217\n",
      "Train Epoch: 1045 [acc: 36%]\tLoss: 5.032055\n",
      "Train Epoch: 1046 [acc: 29%]\tLoss: 5.608858\n",
      "Train Epoch: 1047 [acc: 31%]\tLoss: 6.004655\n",
      "Train Epoch: 1048 [acc: 33%]\tLoss: 6.094104\n",
      "Train Epoch: 1049 [acc: 29%]\tLoss: 5.033194\n",
      "Train Epoch: 1050 [acc: 31%]\tLoss: 6.283785\n",
      "[Trained for 1050 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.28 +- 0.54 , Avg Loss: 7.95\n",
      "Train Epoch: 1051 [acc: 25%]\tLoss: 5.602020\n",
      "Train Epoch: 1052 [acc: 35%]\tLoss: 4.986457\n",
      "Train Epoch: 1053 [acc: 29%]\tLoss: 6.019193\n",
      "Train Epoch: 1054 [acc: 27%]\tLoss: 5.569706\n",
      "Train Epoch: 1055 [acc: 30%]\tLoss: 5.999514\n",
      "Train Epoch: 1056 [acc: 24%]\tLoss: 6.759635\n",
      "Train Epoch: 1057 [acc: 31%]\tLoss: 5.313228\n",
      "Train Epoch: 1058 [acc: 29%]\tLoss: 6.115498\n",
      "Train Epoch: 1059 [acc: 27%]\tLoss: 5.601630\n",
      "Train Epoch: 1060 [acc: 25%]\tLoss: 5.243555\n",
      "Train Epoch: 1061 [acc: 33%]\tLoss: 5.322414\n",
      "Train Epoch: 1062 [acc: 37%]\tLoss: 5.938477\n",
      "Train Epoch: 1063 [acc: 26%]\tLoss: 6.335169\n",
      "Train Epoch: 1064 [acc: 29%]\tLoss: 5.536305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1065 [acc: 32%]\tLoss: 5.290791\n",
      "Train Epoch: 1066 [acc: 27%]\tLoss: 5.954533\n",
      "Train Epoch: 1067 [acc: 33%]\tLoss: 5.731196\n",
      "Train Epoch: 1068 [acc: 36%]\tLoss: 4.940327\n",
      "Train Epoch: 1069 [acc: 35%]\tLoss: 5.409179\n",
      "Train Epoch: 1070 [acc: 27%]\tLoss: 5.133503\n",
      "Train Epoch: 1071 [acc: 34%]\tLoss: 4.817735\n",
      "Train Epoch: 1072 [acc: 31%]\tLoss: 5.786676\n",
      "Train Epoch: 1073 [acc: 37%]\tLoss: 5.471253\n",
      "Train Epoch: 1074 [acc: 24%]\tLoss: 6.300633\n",
      "Train Epoch: 1075 [acc: 40%]\tLoss: 4.915961\n",
      "[Trained for 1075 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.87 +- 0.62 , Avg Loss: 7.85\n",
      "Train Epoch: 1076 [acc: 22%]\tLoss: 5.596968\n",
      "Train Epoch: 1077 [acc: 29%]\tLoss: 5.681083\n",
      "Train Epoch: 1078 [acc: 39%]\tLoss: 4.549195\n",
      "Train Epoch: 1079 [acc: 26%]\tLoss: 6.414946\n",
      "Train Epoch: 1080 [acc: 32%]\tLoss: 4.786110\n",
      "Train Epoch: 1081 [acc: 33%]\tLoss: 4.669915\n",
      "Train Epoch: 1082 [acc: 26%]\tLoss: 5.735251\n",
      "Train Epoch: 1083 [acc: 36%]\tLoss: 4.814287\n",
      "Train Epoch: 1084 [acc: 30%]\tLoss: 6.350363\n",
      "Train Epoch: 1085 [acc: 37%]\tLoss: 4.872077\n",
      "Train Epoch: 1086 [acc: 31%]\tLoss: 6.017385\n",
      "Train Epoch: 1087 [acc: 35%]\tLoss: 4.797166\n",
      "Train Epoch: 1088 [acc: 32%]\tLoss: 5.444746\n",
      "Train Epoch: 1089 [acc: 31%]\tLoss: 4.847761\n",
      "Train Epoch: 1090 [acc: 27%]\tLoss: 6.683881\n",
      "Train Epoch: 1091 [acc: 32%]\tLoss: 5.579599\n",
      "Train Epoch: 1092 [acc: 34%]\tLoss: 5.809861\n",
      "Train Epoch: 1093 [acc: 29%]\tLoss: 6.354592\n",
      "Train Epoch: 1094 [acc: 33%]\tLoss: 5.408584\n",
      "Train Epoch: 1095 [acc: 31%]\tLoss: 4.588608\n",
      "Train Epoch: 1096 [acc: 33%]\tLoss: 5.450180\n",
      "Train Epoch: 1097 [acc: 31%]\tLoss: 5.838645\n",
      "Train Epoch: 1098 [acc: 33%]\tLoss: 4.580828\n",
      "Train Epoch: 1099 [acc: 35%]\tLoss: 5.191448\n",
      "Train Epoch: 1100 [acc: 33%]\tLoss: 5.293060\n",
      "[Trained for 1100 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.00 +- 0.60 , Avg Loss: 7.90\n",
      "Train Epoch: 1101 [acc: 33%]\tLoss: 5.522139\n",
      "Train Epoch: 1102 [acc: 22%]\tLoss: 5.367211\n",
      "Train Epoch: 1103 [acc: 29%]\tLoss: 6.388988\n",
      "Train Epoch: 1104 [acc: 41%]\tLoss: 5.601386\n",
      "Train Epoch: 1105 [acc: 31%]\tLoss: 6.081473\n",
      "Train Epoch: 1106 [acc: 31%]\tLoss: 6.110496\n",
      "Train Epoch: 1107 [acc: 35%]\tLoss: 4.394533\n",
      "Train Epoch: 1108 [acc: 30%]\tLoss: 5.872918\n",
      "Train Epoch: 1109 [acc: 36%]\tLoss: 5.273346\n",
      "Train Epoch: 1110 [acc: 34%]\tLoss: 5.515235\n",
      "Train Epoch: 1111 [acc: 32%]\tLoss: 5.733524\n",
      "Train Epoch: 1112 [acc: 23%]\tLoss: 6.885217\n",
      "Train Epoch: 1113 [acc: 32%]\tLoss: 5.329321\n",
      "Train Epoch: 1114 [acc: 34%]\tLoss: 5.312498\n",
      "Train Epoch: 1115 [acc: 26%]\tLoss: 5.517395\n",
      "Train Epoch: 1116 [acc: 34%]\tLoss: 5.190654\n",
      "Train Epoch: 1117 [acc: 40%]\tLoss: 4.466465\n",
      "Train Epoch: 1118 [acc: 30%]\tLoss: 5.874521\n",
      "Train Epoch: 1119 [acc: 39%]\tLoss: 5.221196\n",
      "Train Epoch: 1120 [acc: 30%]\tLoss: 4.766294\n",
      "Train Epoch: 1121 [acc: 27%]\tLoss: 5.548800\n",
      "Train Epoch: 1122 [acc: 38%]\tLoss: 5.018516\n",
      "Train Epoch: 1123 [acc: 30%]\tLoss: 5.947055\n",
      "Train Epoch: 1124 [acc: 28%]\tLoss: 6.403407\n",
      "Train Epoch: 1125 [acc: 25%]\tLoss: 6.683649\n",
      "[Trained for 1125 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.44 +- 0.72 , Avg Loss: 7.99\n",
      "Train Epoch: 1126 [acc: 31%]\tLoss: 5.256965\n",
      "Train Epoch: 1127 [acc: 31%]\tLoss: 5.493078\n",
      "Train Epoch: 1128 [acc: 36%]\tLoss: 5.069783\n",
      "Train Epoch: 1129 [acc: 38%]\tLoss: 4.737464\n",
      "Train Epoch: 1130 [acc: 35%]\tLoss: 5.016262\n",
      "Train Epoch: 1131 [acc: 28%]\tLoss: 5.163570\n",
      "Train Epoch: 1132 [acc: 30%]\tLoss: 5.340572\n",
      "Train Epoch: 1133 [acc: 30%]\tLoss: 6.072438\n",
      "Train Epoch: 1134 [acc: 32%]\tLoss: 5.446958\n",
      "Train Epoch: 1135 [acc: 24%]\tLoss: 5.084517\n",
      "Train Epoch: 1136 [acc: 26%]\tLoss: 6.358939\n",
      "Train Epoch: 1137 [acc: 33%]\tLoss: 5.795382\n",
      "Train Epoch: 1138 [acc: 33%]\tLoss: 5.549132\n",
      "Train Epoch: 1139 [acc: 38%]\tLoss: 5.063886\n",
      "Train Epoch: 1140 [acc: 38%]\tLoss: 4.306161\n",
      "Train Epoch: 1141 [acc: 24%]\tLoss: 6.523915\n",
      "Train Epoch: 1142 [acc: 35%]\tLoss: 4.467701\n",
      "Train Epoch: 1143 [acc: 36%]\tLoss: 4.644920\n",
      "Train Epoch: 1144 [acc: 35%]\tLoss: 5.681177\n",
      "Train Epoch: 1145 [acc: 29%]\tLoss: 5.559697\n",
      "Train Epoch: 1146 [acc: 28%]\tLoss: 5.863218\n",
      "Train Epoch: 1147 [acc: 22%]\tLoss: 4.985384\n",
      "Train Epoch: 1148 [acc: 31%]\tLoss: 5.342839\n",
      "Train Epoch: 1149 [acc: 24%]\tLoss: 6.430906\n",
      "Train Epoch: 1150 [acc: 37%]\tLoss: 5.444830\n",
      "[Trained for 1150 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.58 +- 0.28 , Avg Loss: 7.93\n",
      "Train Epoch: 1151 [acc: 30%]\tLoss: 5.689562\n",
      "Train Epoch: 1152 [acc: 29%]\tLoss: 5.882608\n",
      "Train Epoch: 1153 [acc: 29%]\tLoss: 5.912696\n",
      "Train Epoch: 1154 [acc: 34%]\tLoss: 4.334917\n",
      "Train Epoch: 1155 [acc: 28%]\tLoss: 5.803703\n",
      "Train Epoch: 1156 [acc: 29%]\tLoss: 5.795815\n",
      "Train Epoch: 1157 [acc: 42%]\tLoss: 4.452688\n",
      "Train Epoch: 1158 [acc: 34%]\tLoss: 6.111265\n",
      "Train Epoch: 1159 [acc: 34%]\tLoss: 4.371150\n",
      "Train Epoch: 1160 [acc: 31%]\tLoss: 5.479358\n",
      "Train Epoch: 1161 [acc: 35%]\tLoss: 4.553446\n",
      "Train Epoch: 1162 [acc: 30%]\tLoss: 5.561980\n",
      "Train Epoch: 1163 [acc: 27%]\tLoss: 6.141188\n",
      "Train Epoch: 1164 [acc: 29%]\tLoss: 5.400327\n",
      "Train Epoch: 1165 [acc: 30%]\tLoss: 5.722918\n",
      "Train Epoch: 1166 [acc: 28%]\tLoss: 5.472898\n",
      "Train Epoch: 1167 [acc: 29%]\tLoss: 5.872418\n",
      "Train Epoch: 1168 [acc: 38%]\tLoss: 5.948749\n",
      "Train Epoch: 1169 [acc: 26%]\tLoss: 6.000001\n",
      "Train Epoch: 1170 [acc: 24%]\tLoss: 5.510095\n",
      "Train Epoch: 1171 [acc: 26%]\tLoss: 5.900128\n",
      "Train Epoch: 1172 [acc: 36%]\tLoss: 5.634974\n",
      "Train Epoch: 1173 [acc: 30%]\tLoss: 5.528314\n",
      "Train Epoch: 1174 [acc: 32%]\tLoss: 5.836184\n",
      "Train Epoch: 1175 [acc: 22%]\tLoss: 6.158293\n",
      "[Trained for 1175 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.41 +- 0.38 , Avg Loss: 7.77\n",
      "Train Epoch: 1176 [acc: 33%]\tLoss: 5.529848\n",
      "Train Epoch: 1177 [acc: 31%]\tLoss: 5.608224\n",
      "Train Epoch: 1178 [acc: 28%]\tLoss: 5.480536\n",
      "Train Epoch: 1179 [acc: 37%]\tLoss: 5.196659\n",
      "Train Epoch: 1180 [acc: 32%]\tLoss: 5.371939\n",
      "Train Epoch: 1181 [acc: 33%]\tLoss: 5.576851\n",
      "Train Epoch: 1182 [acc: 23%]\tLoss: 7.127040\n",
      "Train Epoch: 1183 [acc: 21%]\tLoss: 6.068130\n",
      "Train Epoch: 1184 [acc: 34%]\tLoss: 5.728171\n",
      "Train Epoch: 1185 [acc: 31%]\tLoss: 5.539501\n",
      "Train Epoch: 1186 [acc: 36%]\tLoss: 5.420002\n",
      "Train Epoch: 1187 [acc: 27%]\tLoss: 5.666752\n",
      "Train Epoch: 1188 [acc: 28%]\tLoss: 5.164470\n",
      "Train Epoch: 1189 [acc: 27%]\tLoss: 6.399946\n",
      "Train Epoch: 1190 [acc: 30%]\tLoss: 5.234199\n",
      "Train Epoch: 1191 [acc: 36%]\tLoss: 4.629154\n",
      "Train Epoch: 1192 [acc: 25%]\tLoss: 6.398216\n",
      "Train Epoch: 1193 [acc: 28%]\tLoss: 5.591668\n",
      "Train Epoch: 1194 [acc: 29%]\tLoss: 6.147607\n",
      "Train Epoch: 1195 [acc: 41%]\tLoss: 4.859876\n",
      "Train Epoch: 1196 [acc: 35%]\tLoss: 5.261356\n",
      "Train Epoch: 1197 [acc: 27%]\tLoss: 6.083762\n",
      "Train Epoch: 1198 [acc: 23%]\tLoss: 5.936510\n",
      "Train Epoch: 1199 [acc: 26%]\tLoss: 6.351758\n",
      "Train Epoch: 1200 [acc: 25%]\tLoss: 5.577173\n",
      "[Trained for 1200 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.49 +- 0.36 , Avg Loss: 7.87\n",
      "Train Epoch: 1201 [acc: 24%]\tLoss: 5.950699\n",
      "Train Epoch: 1202 [acc: 24%]\tLoss: 6.030122\n",
      "Train Epoch: 1203 [acc: 25%]\tLoss: 6.233649\n",
      "Train Epoch: 1204 [acc: 27%]\tLoss: 5.734648\n",
      "Train Epoch: 1205 [acc: 33%]\tLoss: 4.983249\n",
      "Train Epoch: 1206 [acc: 26%]\tLoss: 5.501552\n",
      "Train Epoch: 1207 [acc: 39%]\tLoss: 4.747400\n",
      "Train Epoch: 1208 [acc: 34%]\tLoss: 5.380431\n",
      "Train Epoch: 1209 [acc: 32%]\tLoss: 4.684037\n",
      "Train Epoch: 1210 [acc: 31%]\tLoss: 5.641431\n",
      "Train Epoch: 1211 [acc: 30%]\tLoss: 6.595493\n",
      "Train Epoch: 1212 [acc: 28%]\tLoss: 6.332418\n",
      "Train Epoch: 1213 [acc: 34%]\tLoss: 5.307619\n",
      "Train Epoch: 1214 [acc: 34%]\tLoss: 6.941643\n",
      "Train Epoch: 1215 [acc: 31%]\tLoss: 6.401042\n",
      "Train Epoch: 1216 [acc: 26%]\tLoss: 6.040890\n",
      "Train Epoch: 1217 [acc: 25%]\tLoss: 5.891794\n",
      "Train Epoch: 1218 [acc: 35%]\tLoss: 6.376154\n",
      "Train Epoch: 1219 [acc: 19%]\tLoss: 5.884404\n",
      "Train Epoch: 1220 [acc: 27%]\tLoss: 6.092593\n",
      "Train Epoch: 1221 [acc: 22%]\tLoss: 6.729564\n",
      "Train Epoch: 1222 [acc: 23%]\tLoss: 6.486029\n",
      "Train Epoch: 1223 [acc: 30%]\tLoss: 5.433142\n",
      "Train Epoch: 1224 [acc: 34%]\tLoss: 5.971277\n",
      "Train Epoch: 1225 [acc: 27%]\tLoss: 5.746278\n",
      "[Trained for 1225 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.87 +- 0.44 , Avg Loss: 7.72\n",
      "Train Epoch: 1226 [acc: 31%]\tLoss: 5.007128\n",
      "Train Epoch: 1227 [acc: 34%]\tLoss: 4.895967\n",
      "Train Epoch: 1228 [acc: 32%]\tLoss: 5.575758\n",
      "Train Epoch: 1229 [acc: 24%]\tLoss: 6.233996\n",
      "Train Epoch: 1230 [acc: 37%]\tLoss: 4.452727\n",
      "Train Epoch: 1231 [acc: 26%]\tLoss: 6.328089\n",
      "Train Epoch: 1232 [acc: 39%]\tLoss: 5.071299\n",
      "Train Epoch: 1233 [acc: 23%]\tLoss: 5.489639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1234 [acc: 31%]\tLoss: 5.821721\n",
      "Train Epoch: 1235 [acc: 30%]\tLoss: 6.016754\n",
      "Train Epoch: 1236 [acc: 40%]\tLoss: 5.386507\n",
      "Train Epoch: 1237 [acc: 34%]\tLoss: 5.581277\n",
      "Train Epoch: 1238 [acc: 43%]\tLoss: 4.307306\n",
      "Train Epoch: 1239 [acc: 30%]\tLoss: 6.053410\n",
      "Train Epoch: 1240 [acc: 24%]\tLoss: 6.021787\n",
      "Train Epoch: 1241 [acc: 39%]\tLoss: 5.056051\n",
      "Train Epoch: 1242 [acc: 32%]\tLoss: 5.550295\n",
      "Train Epoch: 1243 [acc: 24%]\tLoss: 6.316788\n",
      "Train Epoch: 1244 [acc: 31%]\tLoss: 5.480353\n",
      "Train Epoch: 1245 [acc: 20%]\tLoss: 6.968160\n",
      "Train Epoch: 1246 [acc: 37%]\tLoss: 4.993532\n",
      "Train Epoch: 1247 [acc: 33%]\tLoss: 5.480715\n",
      "Train Epoch: 1248 [acc: 31%]\tLoss: 5.671327\n",
      "Train Epoch: 1249 [acc: 34%]\tLoss: 4.727278\n",
      "Train Epoch: 1250 [acc: 35%]\tLoss: 5.087755\n",
      "[Trained for 1250 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.51 +- 0.38 , Avg Loss: 7.71\n",
      "Train Epoch: 1251 [acc: 27%]\tLoss: 6.001099\n",
      "Train Epoch: 1252 [acc: 33%]\tLoss: 5.953068\n",
      "Train Epoch: 1253 [acc: 30%]\tLoss: 4.927003\n",
      "Train Epoch: 1254 [acc: 32%]\tLoss: 5.657472\n",
      "Train Epoch: 1255 [acc: 29%]\tLoss: 6.245798\n",
      "Train Epoch: 1256 [acc: 27%]\tLoss: 5.615190\n",
      "Train Epoch: 1257 [acc: 33%]\tLoss: 4.760206\n",
      "Train Epoch: 1258 [acc: 27%]\tLoss: 5.676311\n",
      "Train Epoch: 1259 [acc: 38%]\tLoss: 4.699558\n",
      "Train Epoch: 1260 [acc: 33%]\tLoss: 5.149150\n",
      "Train Epoch: 1261 [acc: 33%]\tLoss: 6.084282\n",
      "Train Epoch: 1262 [acc: 34%]\tLoss: 5.102656\n",
      "Train Epoch: 1263 [acc: 31%]\tLoss: 5.967111\n",
      "Train Epoch: 1264 [acc: 34%]\tLoss: 5.154670\n",
      "Train Epoch: 1265 [acc: 23%]\tLoss: 6.614682\n",
      "Train Epoch: 1266 [acc: 27%]\tLoss: 5.426227\n",
      "Train Epoch: 1267 [acc: 35%]\tLoss: 5.192361\n",
      "Train Epoch: 1268 [acc: 39%]\tLoss: 4.452414\n",
      "Train Epoch: 1269 [acc: 36%]\tLoss: 5.173595\n",
      "Train Epoch: 1270 [acc: 26%]\tLoss: 5.659410\n",
      "Train Epoch: 1271 [acc: 26%]\tLoss: 5.444161\n",
      "Train Epoch: 1272 [acc: 26%]\tLoss: 5.564354\n",
      "Train Epoch: 1273 [acc: 23%]\tLoss: 5.849777\n",
      "Train Epoch: 1274 [acc: 29%]\tLoss: 5.522960\n",
      "Train Epoch: 1275 [acc: 29%]\tLoss: 5.846830\n",
      "[Trained for 1275 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.10 +- 0.41 , Avg Loss: 7.64\n",
      "Train Epoch: 1276 [acc: 36%]\tLoss: 5.154037\n",
      "Train Epoch: 1277 [acc: 35%]\tLoss: 5.940613\n",
      "Train Epoch: 1278 [acc: 31%]\tLoss: 6.475981\n",
      "Train Epoch: 1279 [acc: 30%]\tLoss: 5.622831\n",
      "Train Epoch: 1280 [acc: 36%]\tLoss: 5.478814\n",
      "Train Epoch: 1281 [acc: 30%]\tLoss: 5.250159\n",
      "Train Epoch: 1282 [acc: 27%]\tLoss: 5.725730\n",
      "Train Epoch: 1283 [acc: 29%]\tLoss: 5.020400\n",
      "Train Epoch: 1284 [acc: 32%]\tLoss: 5.431830\n",
      "Train Epoch: 1285 [acc: 28%]\tLoss: 5.944216\n",
      "Train Epoch: 1286 [acc: 34%]\tLoss: 6.160997\n",
      "Train Epoch: 1287 [acc: 22%]\tLoss: 6.519961\n",
      "Train Epoch: 1288 [acc: 32%]\tLoss: 5.489945\n",
      "Train Epoch: 1289 [acc: 30%]\tLoss: 5.176333\n",
      "Train Epoch: 1290 [acc: 31%]\tLoss: 4.941779\n",
      "Train Epoch: 1291 [acc: 23%]\tLoss: 5.425798\n",
      "Train Epoch: 1292 [acc: 37%]\tLoss: 4.223856\n",
      "Train Epoch: 1293 [acc: 29%]\tLoss: 5.235206\n",
      "Train Epoch: 1294 [acc: 26%]\tLoss: 6.411221\n",
      "Train Epoch: 1295 [acc: 39%]\tLoss: 4.932714\n",
      "Train Epoch: 1296 [acc: 38%]\tLoss: 5.308771\n",
      "Train Epoch: 1297 [acc: 29%]\tLoss: 5.597033\n",
      "Train Epoch: 1298 [acc: 42%]\tLoss: 5.216215\n",
      "Train Epoch: 1299 [acc: 28%]\tLoss: 5.498387\n",
      "Train Epoch: 1300 [acc: 32%]\tLoss: 5.376208\n",
      "[Trained for 1300 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.25 +- 0.42 , Avg Loss: 7.79\n",
      "Train Epoch: 1301 [acc: 33%]\tLoss: 5.501314\n",
      "Train Epoch: 1302 [acc: 39%]\tLoss: 5.097439\n",
      "Train Epoch: 1303 [acc: 31%]\tLoss: 5.302283\n",
      "Train Epoch: 1304 [acc: 38%]\tLoss: 5.204723\n",
      "Train Epoch: 1305 [acc: 29%]\tLoss: 5.600880\n",
      "Train Epoch: 1306 [acc: 35%]\tLoss: 5.973519\n",
      "Train Epoch: 1307 [acc: 35%]\tLoss: 5.253903\n",
      "Train Epoch: 1308 [acc: 29%]\tLoss: 5.625857\n",
      "Train Epoch: 1309 [acc: 29%]\tLoss: 6.712562\n",
      "Train Epoch: 1310 [acc: 31%]\tLoss: 4.909629\n",
      "Train Epoch: 1311 [acc: 31%]\tLoss: 5.833417\n",
      "Train Epoch: 1312 [acc: 37%]\tLoss: 5.129197\n",
      "Train Epoch: 1313 [acc: 32%]\tLoss: 5.956430\n",
      "Train Epoch: 1314 [acc: 32%]\tLoss: 5.354988\n",
      "Train Epoch: 1315 [acc: 31%]\tLoss: 5.923670\n",
      "Train Epoch: 1316 [acc: 20%]\tLoss: 6.290615\n",
      "Train Epoch: 1317 [acc: 26%]\tLoss: 6.269853\n",
      "Train Epoch: 1318 [acc: 38%]\tLoss: 5.814623\n",
      "Train Epoch: 1319 [acc: 40%]\tLoss: 4.108661\n",
      "Train Epoch: 1320 [acc: 36%]\tLoss: 4.783360\n",
      "Train Epoch: 1321 [acc: 39%]\tLoss: 5.326128\n",
      "Train Epoch: 1322 [acc: 29%]\tLoss: 5.708165\n",
      "Train Epoch: 1323 [acc: 41%]\tLoss: 4.185274\n",
      "Train Epoch: 1324 [acc: 29%]\tLoss: 5.841453\n",
      "Train Epoch: 1325 [acc: 35%]\tLoss: 4.311010\n",
      "[Trained for 1325 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.57 +- 0.46 , Avg Loss: 7.85\n",
      "Train Epoch: 1326 [acc: 29%]\tLoss: 5.409324\n",
      "Train Epoch: 1327 [acc: 23%]\tLoss: 6.061357\n",
      "Train Epoch: 1328 [acc: 31%]\tLoss: 5.457412\n",
      "Train Epoch: 1329 [acc: 35%]\tLoss: 5.677307\n",
      "Train Epoch: 1330 [acc: 29%]\tLoss: 5.633511\n",
      "Train Epoch: 1331 [acc: 35%]\tLoss: 5.386267\n",
      "Train Epoch: 1332 [acc: 24%]\tLoss: 6.928815\n",
      "Train Epoch: 1333 [acc: 32%]\tLoss: 6.049354\n",
      "Train Epoch: 1334 [acc: 30%]\tLoss: 5.309427\n",
      "Train Epoch: 1335 [acc: 26%]\tLoss: 6.162718\n",
      "Train Epoch: 1336 [acc: 27%]\tLoss: 5.359903\n",
      "Train Epoch: 1337 [acc: 33%]\tLoss: 5.588245\n",
      "Train Epoch: 1338 [acc: 31%]\tLoss: 5.682748\n",
      "Train Epoch: 1339 [acc: 36%]\tLoss: 5.220993\n",
      "Train Epoch: 1340 [acc: 37%]\tLoss: 4.440281\n",
      "Train Epoch: 1341 [acc: 33%]\tLoss: 6.041327\n",
      "Train Epoch: 1342 [acc: 22%]\tLoss: 6.816365\n",
      "Train Epoch: 1343 [acc: 31%]\tLoss: 6.198501\n",
      "Train Epoch: 1344 [acc: 31%]\tLoss: 5.643036\n",
      "Train Epoch: 1345 [acc: 24%]\tLoss: 5.624850\n",
      "Train Epoch: 1346 [acc: 34%]\tLoss: 5.796645\n",
      "Train Epoch: 1347 [acc: 28%]\tLoss: 5.942165\n",
      "Train Epoch: 1348 [acc: 30%]\tLoss: 5.580052\n",
      "Train Epoch: 1349 [acc: 34%]\tLoss: 4.651159\n",
      "Train Epoch: 1350 [acc: 31%]\tLoss: 5.236503\n",
      "[Trained for 1350 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.29 +- 0.95 , Avg Loss: 7.83\n",
      "Train Epoch: 1351 [acc: 29%]\tLoss: 6.260094\n",
      "Train Epoch: 1352 [acc: 37%]\tLoss: 5.748633\n",
      "Train Epoch: 1353 [acc: 32%]\tLoss: 5.331327\n",
      "Train Epoch: 1354 [acc: 27%]\tLoss: 5.862104\n",
      "Train Epoch: 1355 [acc: 25%]\tLoss: 6.769290\n",
      "Train Epoch: 1356 [acc: 30%]\tLoss: 5.516854\n",
      "Train Epoch: 1357 [acc: 24%]\tLoss: 5.821659\n",
      "Train Epoch: 1358 [acc: 32%]\tLoss: 5.714644\n",
      "Train Epoch: 1359 [acc: 34%]\tLoss: 6.090635\n",
      "Train Epoch: 1360 [acc: 25%]\tLoss: 5.190514\n",
      "Train Epoch: 1361 [acc: 29%]\tLoss: 5.956153\n",
      "Train Epoch: 1362 [acc: 32%]\tLoss: 5.651952\n",
      "Train Epoch: 1363 [acc: 39%]\tLoss: 4.486326\n",
      "Train Epoch: 1364 [acc: 28%]\tLoss: 5.599895\n",
      "Train Epoch: 1365 [acc: 33%]\tLoss: 4.850034\n",
      "Train Epoch: 1366 [acc: 25%]\tLoss: 6.853662\n",
      "Train Epoch: 1367 [acc: 28%]\tLoss: 5.620998\n",
      "Train Epoch: 1368 [acc: 22%]\tLoss: 6.466712\n",
      "Train Epoch: 1369 [acc: 30%]\tLoss: 5.515915\n",
      "Train Epoch: 1370 [acc: 28%]\tLoss: 5.554871\n",
      "Train Epoch: 1371 [acc: 39%]\tLoss: 4.038568\n",
      "Train Epoch: 1372 [acc: 26%]\tLoss: 6.102629\n",
      "Train Epoch: 1373 [acc: 33%]\tLoss: 5.371054\n",
      "Train Epoch: 1374 [acc: 33%]\tLoss: 5.904427\n",
      "Train Epoch: 1375 [acc: 36%]\tLoss: 5.533248\n",
      "[Trained for 1375 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.89 +- 0.46 , Avg Loss: 8.18\n",
      "Train Epoch: 1376 [acc: 35%]\tLoss: 6.165224\n",
      "Train Epoch: 1377 [acc: 35%]\tLoss: 5.027568\n",
      "Train Epoch: 1378 [acc: 34%]\tLoss: 6.130160\n",
      "Train Epoch: 1379 [acc: 31%]\tLoss: 5.636752\n",
      "Train Epoch: 1380 [acc: 28%]\tLoss: 5.855152\n",
      "Train Epoch: 1381 [acc: 19%]\tLoss: 6.732531\n",
      "Train Epoch: 1382 [acc: 34%]\tLoss: 5.009437\n",
      "Train Epoch: 1383 [acc: 26%]\tLoss: 5.899368\n",
      "Train Epoch: 1384 [acc: 29%]\tLoss: 5.557695\n",
      "Train Epoch: 1385 [acc: 29%]\tLoss: 5.685594\n",
      "Train Epoch: 1386 [acc: 32%]\tLoss: 5.650813\n",
      "Train Epoch: 1387 [acc: 34%]\tLoss: 4.879107\n",
      "Train Epoch: 1388 [acc: 29%]\tLoss: 5.134981\n",
      "Train Epoch: 1389 [acc: 28%]\tLoss: 5.841231\n",
      "Train Epoch: 1390 [acc: 33%]\tLoss: 5.941680\n",
      "Train Epoch: 1391 [acc: 30%]\tLoss: 5.880102\n",
      "Train Epoch: 1392 [acc: 29%]\tLoss: 5.659264\n",
      "Train Epoch: 1393 [acc: 33%]\tLoss: 5.147515\n",
      "Train Epoch: 1394 [acc: 24%]\tLoss: 5.758993\n",
      "Train Epoch: 1395 [acc: 32%]\tLoss: 5.603693\n",
      "Train Epoch: 1396 [acc: 28%]\tLoss: 6.317666\n",
      "Train Epoch: 1397 [acc: 32%]\tLoss: 6.288325\n",
      "Train Epoch: 1398 [acc: 29%]\tLoss: 5.697473\n",
      "Train Epoch: 1399 [acc: 35%]\tLoss: 5.135157\n",
      "Train Epoch: 1400 [acc: 26%]\tLoss: 4.781137\n",
      "[Trained for 1400 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.02 +- 0.76 , Avg Loss: 7.74\n",
      "Train Epoch: 1401 [acc: 26%]\tLoss: 6.003313\n",
      "Train Epoch: 1402 [acc: 40%]\tLoss: 4.560290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1403 [acc: 33%]\tLoss: 5.719269\n",
      "Train Epoch: 1404 [acc: 25%]\tLoss: 6.612069\n",
      "Train Epoch: 1405 [acc: 26%]\tLoss: 5.757385\n",
      "Train Epoch: 1406 [acc: 27%]\tLoss: 5.836544\n",
      "Train Epoch: 1407 [acc: 27%]\tLoss: 5.927109\n",
      "Train Epoch: 1408 [acc: 30%]\tLoss: 6.251940\n",
      "Train Epoch: 1409 [acc: 27%]\tLoss: 5.697063\n",
      "Train Epoch: 1410 [acc: 39%]\tLoss: 4.480721\n",
      "Train Epoch: 1411 [acc: 28%]\tLoss: 5.808690\n",
      "Train Epoch: 1412 [acc: 25%]\tLoss: 5.943625\n",
      "Train Epoch: 1413 [acc: 32%]\tLoss: 5.178919\n",
      "Train Epoch: 1414 [acc: 32%]\tLoss: 5.466706\n",
      "Train Epoch: 1415 [acc: 30%]\tLoss: 6.351055\n",
      "Train Epoch: 1416 [acc: 26%]\tLoss: 6.153141\n",
      "Train Epoch: 1417 [acc: 39%]\tLoss: 4.812960\n",
      "Train Epoch: 1418 [acc: 30%]\tLoss: 5.907178\n",
      "Train Epoch: 1419 [acc: 34%]\tLoss: 5.558563\n",
      "Train Epoch: 1420 [acc: 37%]\tLoss: 5.302495\n",
      "Train Epoch: 1421 [acc: 30%]\tLoss: 6.222645\n",
      "Train Epoch: 1422 [acc: 29%]\tLoss: 6.443831\n",
      "Train Epoch: 1423 [acc: 23%]\tLoss: 6.493947\n",
      "Train Epoch: 1424 [acc: 30%]\tLoss: 4.764655\n",
      "Train Epoch: 1425 [acc: 29%]\tLoss: 5.191943\n",
      "[Trained for 1425 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.38 +- 0.37 , Avg Loss: 7.89\n",
      "Train Epoch: 1426 [acc: 33%]\tLoss: 4.770085\n",
      "Train Epoch: 1427 [acc: 24%]\tLoss: 7.240812\n",
      "Train Epoch: 1428 [acc: 32%]\tLoss: 5.973929\n",
      "Train Epoch: 1429 [acc: 24%]\tLoss: 5.615860\n",
      "Train Epoch: 1430 [acc: 37%]\tLoss: 4.698458\n",
      "Train Epoch: 1431 [acc: 38%]\tLoss: 5.564955\n",
      "Train Epoch: 1432 [acc: 32%]\tLoss: 5.316339\n",
      "Train Epoch: 1433 [acc: 37%]\tLoss: 5.883870\n",
      "Train Epoch: 1434 [acc: 32%]\tLoss: 5.655892\n",
      "Train Epoch: 1435 [acc: 29%]\tLoss: 5.800426\n",
      "Train Epoch: 1436 [acc: 29%]\tLoss: 5.054083\n",
      "Train Epoch: 1437 [acc: 27%]\tLoss: 6.135539\n",
      "Train Epoch: 1438 [acc: 27%]\tLoss: 6.473742\n",
      "Train Epoch: 1439 [acc: 31%]\tLoss: 5.231929\n",
      "Train Epoch: 1440 [acc: 31%]\tLoss: 5.263934\n",
      "Train Epoch: 1441 [acc: 24%]\tLoss: 6.658807\n",
      "Train Epoch: 1442 [acc: 37%]\tLoss: 5.293999\n",
      "Train Epoch: 1443 [acc: 31%]\tLoss: 5.714616\n",
      "Train Epoch: 1444 [acc: 28%]\tLoss: 6.255166\n",
      "Train Epoch: 1445 [acc: 29%]\tLoss: 5.613211\n",
      "Train Epoch: 1446 [acc: 31%]\tLoss: 5.234303\n",
      "Train Epoch: 1447 [acc: 33%]\tLoss: 5.772038\n",
      "Train Epoch: 1448 [acc: 34%]\tLoss: 5.083676\n",
      "Train Epoch: 1449 [acc: 29%]\tLoss: 5.532100\n",
      "Train Epoch: 1450 [acc: 37%]\tLoss: 5.469745\n",
      "[Trained for 1450 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.69 +- 0.29 , Avg Loss: 7.78\n",
      "Train Epoch: 1451 [acc: 31%]\tLoss: 5.906763\n",
      "Train Epoch: 1452 [acc: 32%]\tLoss: 5.486119\n",
      "Train Epoch: 1453 [acc: 33%]\tLoss: 6.437411\n",
      "Train Epoch: 1454 [acc: 34%]\tLoss: 5.436625\n",
      "Train Epoch: 1455 [acc: 30%]\tLoss: 6.484114\n",
      "Train Epoch: 1456 [acc: 31%]\tLoss: 5.870420\n",
      "Train Epoch: 1457 [acc: 25%]\tLoss: 6.462245\n",
      "Train Epoch: 1458 [acc: 30%]\tLoss: 5.765239\n",
      "Train Epoch: 1459 [acc: 33%]\tLoss: 4.991070\n",
      "Train Epoch: 1460 [acc: 42%]\tLoss: 4.813951\n",
      "Train Epoch: 1461 [acc: 29%]\tLoss: 5.052454\n",
      "Train Epoch: 1462 [acc: 39%]\tLoss: 5.143040\n",
      "Train Epoch: 1463 [acc: 40%]\tLoss: 4.923858\n",
      "Train Epoch: 1464 [acc: 31%]\tLoss: 5.762286\n",
      "Train Epoch: 1465 [acc: 32%]\tLoss: 5.963990\n",
      "Train Epoch: 1466 [acc: 25%]\tLoss: 6.474073\n",
      "Train Epoch: 1467 [acc: 26%]\tLoss: 5.474318\n",
      "Train Epoch: 1468 [acc: 31%]\tLoss: 5.935534\n",
      "Train Epoch: 1469 [acc: 27%]\tLoss: 5.051034\n",
      "Train Epoch: 1470 [acc: 32%]\tLoss: 4.821948\n",
      "Train Epoch: 1471 [acc: 33%]\tLoss: 5.346983\n",
      "Train Epoch: 1472 [acc: 24%]\tLoss: 5.603079\n",
      "Train Epoch: 1473 [acc: 29%]\tLoss: 5.442648\n",
      "Train Epoch: 1474 [acc: 33%]\tLoss: 5.982207\n",
      "Train Epoch: 1475 [acc: 27%]\tLoss: 6.172479\n",
      "[Trained for 1475 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.62 +- 0.78 , Avg Loss: 7.78\n",
      "Train Epoch: 1476 [acc: 36%]\tLoss: 5.666891\n",
      "Train Epoch: 1477 [acc: 34%]\tLoss: 6.115784\n",
      "Train Epoch: 1478 [acc: 27%]\tLoss: 5.466482\n",
      "Train Epoch: 1479 [acc: 39%]\tLoss: 5.715205\n",
      "Train Epoch: 1480 [acc: 27%]\tLoss: 6.028708\n",
      "Train Epoch: 1481 [acc: 39%]\tLoss: 5.319915\n",
      "Train Epoch: 1482 [acc: 32%]\tLoss: 5.615791\n",
      "Train Epoch: 1483 [acc: 29%]\tLoss: 5.491572\n",
      "Train Epoch: 1484 [acc: 30%]\tLoss: 4.999315\n",
      "Train Epoch: 1485 [acc: 24%]\tLoss: 6.190734\n",
      "Train Epoch: 1486 [acc: 23%]\tLoss: 6.274866\n",
      "Train Epoch: 1487 [acc: 35%]\tLoss: 5.662722\n",
      "Train Epoch: 1488 [acc: 32%]\tLoss: 5.960318\n",
      "Train Epoch: 1489 [acc: 30%]\tLoss: 6.193902\n",
      "Train Epoch: 1490 [acc: 31%]\tLoss: 4.911294\n",
      "Train Epoch: 1491 [acc: 32%]\tLoss: 6.130840\n",
      "Train Epoch: 1492 [acc: 34%]\tLoss: 4.506683\n",
      "Train Epoch: 1493 [acc: 25%]\tLoss: 5.744251\n",
      "Train Epoch: 1494 [acc: 33%]\tLoss: 5.059464\n",
      "Train Epoch: 1495 [acc: 23%]\tLoss: 6.128913\n",
      "Train Epoch: 1496 [acc: 44%]\tLoss: 5.398688\n",
      "Train Epoch: 1497 [acc: 25%]\tLoss: 5.905741\n",
      "Train Epoch: 1498 [acc: 24%]\tLoss: 5.804440\n",
      "Train Epoch: 1499 [acc: 35%]\tLoss: 5.839013\n",
      "Train Epoch: 1500 [acc: 31%]\tLoss: 5.294586\n",
      "[Trained for 1500 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.43 +- 0.52 , Avg Loss: 7.71\n",
      "Train Epoch: 1501 [acc: 33%]\tLoss: 5.493327\n",
      "Train Epoch: 1502 [acc: 33%]\tLoss: 5.779114\n",
      "Train Epoch: 1503 [acc: 33%]\tLoss: 5.018064\n",
      "Train Epoch: 1504 [acc: 30%]\tLoss: 5.554036\n",
      "Train Epoch: 1505 [acc: 23%]\tLoss: 5.569787\n",
      "Train Epoch: 1506 [acc: 29%]\tLoss: 6.421711\n",
      "Train Epoch: 1507 [acc: 23%]\tLoss: 6.726462\n",
      "Train Epoch: 1508 [acc: 27%]\tLoss: 5.100368\n",
      "Train Epoch: 1509 [acc: 38%]\tLoss: 5.002377\n",
      "Train Epoch: 1510 [acc: 26%]\tLoss: 5.760295\n",
      "Train Epoch: 1511 [acc: 31%]\tLoss: 5.024531\n",
      "Train Epoch: 1512 [acc: 26%]\tLoss: 5.248651\n",
      "Train Epoch: 1513 [acc: 34%]\tLoss: 4.092661\n",
      "Train Epoch: 1514 [acc: 27%]\tLoss: 5.998741\n",
      "Train Epoch: 1515 [acc: 29%]\tLoss: 5.284347\n",
      "Train Epoch: 1516 [acc: 28%]\tLoss: 6.033691\n",
      "Train Epoch: 1517 [acc: 26%]\tLoss: 6.361636\n",
      "Train Epoch: 1518 [acc: 31%]\tLoss: 5.827554\n",
      "Train Epoch: 1519 [acc: 39%]\tLoss: 4.757805\n",
      "Train Epoch: 1520 [acc: 25%]\tLoss: 4.740955\n",
      "Train Epoch: 1521 [acc: 34%]\tLoss: 4.978024\n",
      "Train Epoch: 1522 [acc: 29%]\tLoss: 5.593256\n",
      "Train Epoch: 1523 [acc: 25%]\tLoss: 5.856774\n",
      "Train Epoch: 1524 [acc: 36%]\tLoss: 5.531909\n",
      "Train Epoch: 1525 [acc: 28%]\tLoss: 6.140744\n",
      "[Trained for 1525 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.21 +- 1.10 , Avg Loss: 7.82\n",
      "Train Epoch: 1526 [acc: 26%]\tLoss: 5.581015\n",
      "Train Epoch: 1527 [acc: 30%]\tLoss: 5.958989\n",
      "Train Epoch: 1528 [acc: 33%]\tLoss: 5.541928\n",
      "Train Epoch: 1529 [acc: 36%]\tLoss: 4.914277\n",
      "Train Epoch: 1530 [acc: 38%]\tLoss: 5.228813\n",
      "Train Epoch: 1531 [acc: 21%]\tLoss: 6.429976\n",
      "Train Epoch: 1532 [acc: 26%]\tLoss: 6.183609\n",
      "Train Epoch: 1533 [acc: 27%]\tLoss: 6.522268\n",
      "Train Epoch: 1534 [acc: 21%]\tLoss: 6.063104\n",
      "Train Epoch: 1535 [acc: 22%]\tLoss: 6.131633\n",
      "Train Epoch: 1536 [acc: 29%]\tLoss: 5.098198\n",
      "Train Epoch: 1537 [acc: 22%]\tLoss: 5.840775\n",
      "Train Epoch: 1538 [acc: 40%]\tLoss: 4.397461\n",
      "Train Epoch: 1539 [acc: 30%]\tLoss: 5.874773\n",
      "Train Epoch: 1540 [acc: 38%]\tLoss: 5.663960\n",
      "Train Epoch: 1541 [acc: 25%]\tLoss: 6.407371\n",
      "Train Epoch: 1542 [acc: 34%]\tLoss: 5.212929\n",
      "Train Epoch: 1543 [acc: 32%]\tLoss: 5.768139\n",
      "Train Epoch: 1544 [acc: 25%]\tLoss: 5.883423\n",
      "Train Epoch: 1545 [acc: 34%]\tLoss: 5.445148\n",
      "Train Epoch: 1546 [acc: 29%]\tLoss: 6.147110\n",
      "Train Epoch: 1547 [acc: 26%]\tLoss: 6.308982\n",
      "Train Epoch: 1548 [acc: 24%]\tLoss: 5.137014\n",
      "Train Epoch: 1549 [acc: 16%]\tLoss: 7.189794\n",
      "Train Epoch: 1550 [acc: 34%]\tLoss: 5.212544\n",
      "[Trained for 1550 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.58 +- 1.31 , Avg Loss: 7.70\n",
      "Train Epoch: 1551 [acc: 36%]\tLoss: 5.709133\n",
      "Train Epoch: 1552 [acc: 38%]\tLoss: 4.986363\n",
      "Train Epoch: 1553 [acc: 29%]\tLoss: 5.534789\n",
      "Train Epoch: 1554 [acc: 29%]\tLoss: 6.403170\n",
      "Train Epoch: 1555 [acc: 31%]\tLoss: 5.159130\n",
      "Train Epoch: 1556 [acc: 32%]\tLoss: 5.491448\n",
      "Train Epoch: 1557 [acc: 32%]\tLoss: 6.378664\n",
      "Train Epoch: 1558 [acc: 27%]\tLoss: 5.807129\n",
      "Train Epoch: 1559 [acc: 30%]\tLoss: 5.504280\n",
      "Train Epoch: 1560 [acc: 27%]\tLoss: 5.614293\n",
      "Train Epoch: 1561 [acc: 33%]\tLoss: 5.706489\n",
      "Train Epoch: 1562 [acc: 27%]\tLoss: 6.408443\n",
      "Train Epoch: 1563 [acc: 34%]\tLoss: 5.068139\n",
      "Train Epoch: 1564 [acc: 33%]\tLoss: 6.145336\n",
      "Train Epoch: 1565 [acc: 29%]\tLoss: 4.961877\n",
      "Train Epoch: 1566 [acc: 34%]\tLoss: 5.308190\n",
      "Train Epoch: 1567 [acc: 32%]\tLoss: 5.235188\n",
      "Train Epoch: 1568 [acc: 31%]\tLoss: 5.551587\n",
      "Train Epoch: 1569 [acc: 33%]\tLoss: 5.485690\n",
      "Train Epoch: 1570 [acc: 33%]\tLoss: 5.902215\n",
      "Train Epoch: 1571 [acc: 34%]\tLoss: 5.724194\n",
      "Train Epoch: 1572 [acc: 37%]\tLoss: 5.293655\n",
      "Train Epoch: 1573 [acc: 22%]\tLoss: 6.709122\n",
      "Train Epoch: 1574 [acc: 27%]\tLoss: 6.370480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1575 [acc: 27%]\tLoss: 6.632133\n",
      "[Trained for 1575 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.73 +- 0.53 , Avg Loss: 7.97\n",
      "Train Epoch: 1576 [acc: 24%]\tLoss: 5.399893\n",
      "Train Epoch: 1577 [acc: 33%]\tLoss: 5.516624\n",
      "Train Epoch: 1578 [acc: 34%]\tLoss: 5.200974\n",
      "Train Epoch: 1579 [acc: 39%]\tLoss: 5.214938\n",
      "Train Epoch: 1580 [acc: 33%]\tLoss: 5.493948\n",
      "Train Epoch: 1581 [acc: 28%]\tLoss: 6.102060\n",
      "Train Epoch: 1582 [acc: 39%]\tLoss: 5.422990\n",
      "Train Epoch: 1583 [acc: 24%]\tLoss: 5.372546\n",
      "Train Epoch: 1584 [acc: 32%]\tLoss: 5.196605\n",
      "Train Epoch: 1585 [acc: 29%]\tLoss: 5.849141\n",
      "Train Epoch: 1586 [acc: 27%]\tLoss: 5.403432\n",
      "Train Epoch: 1587 [acc: 24%]\tLoss: 5.152740\n",
      "Train Epoch: 1588 [acc: 36%]\tLoss: 5.048390\n",
      "Train Epoch: 1589 [acc: 27%]\tLoss: 5.794664\n",
      "Train Epoch: 1590 [acc: 34%]\tLoss: 5.413836\n",
      "Train Epoch: 1591 [acc: 29%]\tLoss: 6.181556\n",
      "Train Epoch: 1592 [acc: 26%]\tLoss: 5.472163\n",
      "Train Epoch: 1593 [acc: 34%]\tLoss: 5.555897\n",
      "Train Epoch: 1594 [acc: 36%]\tLoss: 4.835930\n",
      "Train Epoch: 1595 [acc: 31%]\tLoss: 5.251681\n",
      "Train Epoch: 1596 [acc: 28%]\tLoss: 6.370709\n",
      "Train Epoch: 1597 [acc: 31%]\tLoss: 5.099781\n",
      "Train Epoch: 1598 [acc: 32%]\tLoss: 5.434492\n",
      "Train Epoch: 1599 [acc: 30%]\tLoss: 5.871137\n",
      "Train Epoch: 1600 [acc: 33%]\tLoss: 4.382317\n",
      "[Trained for 1600 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.96 +- 1.18 , Avg Loss: 7.94\n",
      "Train Epoch: 1601 [acc: 31%]\tLoss: 4.831676\n",
      "Train Epoch: 1602 [acc: 42%]\tLoss: 4.919753\n",
      "Train Epoch: 1603 [acc: 33%]\tLoss: 5.311051\n",
      "Train Epoch: 1604 [acc: 27%]\tLoss: 5.754880\n",
      "Train Epoch: 1605 [acc: 44%]\tLoss: 4.110292\n",
      "Train Epoch: 1606 [acc: 35%]\tLoss: 6.172016\n",
      "Train Epoch: 1607 [acc: 28%]\tLoss: 5.803371\n",
      "Train Epoch: 1608 [acc: 21%]\tLoss: 6.954672\n",
      "Train Epoch: 1609 [acc: 37%]\tLoss: 5.152601\n",
      "Train Epoch: 1610 [acc: 31%]\tLoss: 5.837177\n",
      "Train Epoch: 1611 [acc: 37%]\tLoss: 6.077501\n",
      "Train Epoch: 1612 [acc: 33%]\tLoss: 5.733142\n",
      "Train Epoch: 1613 [acc: 32%]\tLoss: 6.193241\n",
      "Train Epoch: 1614 [acc: 33%]\tLoss: 5.433441\n",
      "Train Epoch: 1615 [acc: 26%]\tLoss: 5.760708\n",
      "Train Epoch: 1616 [acc: 31%]\tLoss: 6.069188\n",
      "Train Epoch: 1617 [acc: 32%]\tLoss: 5.410565\n",
      "Train Epoch: 1618 [acc: 37%]\tLoss: 4.647873\n",
      "Train Epoch: 1619 [acc: 35%]\tLoss: 4.316121\n",
      "Train Epoch: 1620 [acc: 32%]\tLoss: 5.636952\n",
      "Train Epoch: 1621 [acc: 30%]\tLoss: 5.246429\n",
      "Train Epoch: 1622 [acc: 39%]\tLoss: 4.657831\n",
      "Train Epoch: 1623 [acc: 32%]\tLoss: 6.106318\n",
      "Train Epoch: 1624 [acc: 35%]\tLoss: 5.488092\n",
      "Train Epoch: 1625 [acc: 26%]\tLoss: 6.921990\n",
      "[Trained for 1625 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.92 +- 0.92 , Avg Loss: 7.79\n",
      "Train Epoch: 1626 [acc: 32%]\tLoss: 5.597323\n",
      "Train Epoch: 1627 [acc: 31%]\tLoss: 6.058332\n",
      "Train Epoch: 1628 [acc: 35%]\tLoss: 5.461840\n",
      "Train Epoch: 1629 [acc: 28%]\tLoss: 5.846132\n",
      "Train Epoch: 1630 [acc: 32%]\tLoss: 5.334102\n",
      "Train Epoch: 1631 [acc: 30%]\tLoss: 5.542637\n",
      "Train Epoch: 1632 [acc: 33%]\tLoss: 6.210470\n",
      "Train Epoch: 1633 [acc: 39%]\tLoss: 5.128551\n",
      "Train Epoch: 1634 [acc: 34%]\tLoss: 5.494709\n",
      "Train Epoch: 1635 [acc: 30%]\tLoss: 6.481177\n",
      "Train Epoch: 1636 [acc: 33%]\tLoss: 6.681388\n",
      "Train Epoch: 1637 [acc: 37%]\tLoss: 5.356406\n",
      "Train Epoch: 1638 [acc: 36%]\tLoss: 5.349607\n",
      "Train Epoch: 1639 [acc: 25%]\tLoss: 5.326891\n",
      "Train Epoch: 1640 [acc: 20%]\tLoss: 6.367327\n",
      "Train Epoch: 1641 [acc: 23%]\tLoss: 6.934539\n",
      "Train Epoch: 1642 [acc: 24%]\tLoss: 7.202628\n",
      "Train Epoch: 1643 [acc: 21%]\tLoss: 6.150572\n",
      "Train Epoch: 1644 [acc: 29%]\tLoss: 5.248984\n",
      "Train Epoch: 1645 [acc: 36%]\tLoss: 5.682304\n",
      "Train Epoch: 1646 [acc: 32%]\tLoss: 4.943852\n",
      "Train Epoch: 1647 [acc: 40%]\tLoss: 3.955938\n",
      "Train Epoch: 1648 [acc: 35%]\tLoss: 4.936544\n",
      "Train Epoch: 1649 [acc: 32%]\tLoss: 4.399629\n",
      "Train Epoch: 1650 [acc: 39%]\tLoss: 4.747761\n",
      "[Trained for 1650 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.28 +- 0.60 , Avg Loss: 7.55\n",
      "Train Epoch: 1651 [acc: 32%]\tLoss: 6.051677\n",
      "Train Epoch: 1652 [acc: 25%]\tLoss: 5.744739\n",
      "Train Epoch: 1653 [acc: 31%]\tLoss: 5.051749\n",
      "Train Epoch: 1654 [acc: 34%]\tLoss: 5.151758\n",
      "Train Epoch: 1655 [acc: 32%]\tLoss: 5.553885\n",
      "Train Epoch: 1656 [acc: 24%]\tLoss: 5.055950\n",
      "Train Epoch: 1657 [acc: 38%]\tLoss: 4.367990\n",
      "Train Epoch: 1658 [acc: 27%]\tLoss: 6.165147\n",
      "Train Epoch: 1659 [acc: 24%]\tLoss: 5.710669\n",
      "Train Epoch: 1660 [acc: 29%]\tLoss: 6.420355\n",
      "Train Epoch: 1661 [acc: 36%]\tLoss: 5.120296\n",
      "Train Epoch: 1662 [acc: 30%]\tLoss: 6.278043\n",
      "Train Epoch: 1663 [acc: 26%]\tLoss: 5.913133\n",
      "Train Epoch: 1664 [acc: 21%]\tLoss: 7.057704\n",
      "Train Epoch: 1665 [acc: 32%]\tLoss: 5.387207\n",
      "Train Epoch: 1666 [acc: 36%]\tLoss: 5.127692\n",
      "Train Epoch: 1667 [acc: 30%]\tLoss: 5.019234\n",
      "Train Epoch: 1668 [acc: 21%]\tLoss: 6.422135\n",
      "Train Epoch: 1669 [acc: 24%]\tLoss: 5.472225\n",
      "Train Epoch: 1670 [acc: 36%]\tLoss: 4.696104\n",
      "Train Epoch: 1671 [acc: 31%]\tLoss: 5.100019\n",
      "Train Epoch: 1672 [acc: 22%]\tLoss: 5.280766\n",
      "Train Epoch: 1673 [acc: 26%]\tLoss: 5.658629\n",
      "Train Epoch: 1674 [acc: 25%]\tLoss: 5.804421\n",
      "Train Epoch: 1675 [acc: 34%]\tLoss: 5.231240\n",
      "[Trained for 1675 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.37 +- 0.54 , Avg Loss: 7.78\n",
      "Train Epoch: 1676 [acc: 29%]\tLoss: 6.281985\n",
      "Train Epoch: 1677 [acc: 31%]\tLoss: 5.750116\n",
      "Train Epoch: 1678 [acc: 27%]\tLoss: 5.341403\n",
      "Train Epoch: 1679 [acc: 31%]\tLoss: 5.372696\n",
      "Train Epoch: 1680 [acc: 28%]\tLoss: 5.228371\n",
      "Train Epoch: 1681 [acc: 29%]\tLoss: 6.422859\n",
      "Train Epoch: 1682 [acc: 31%]\tLoss: 5.667842\n",
      "Train Epoch: 1683 [acc: 38%]\tLoss: 4.648555\n",
      "Train Epoch: 1684 [acc: 29%]\tLoss: 5.578116\n",
      "Train Epoch: 1685 [acc: 31%]\tLoss: 6.234680\n",
      "Train Epoch: 1686 [acc: 28%]\tLoss: 5.976942\n",
      "Train Epoch: 1687 [acc: 42%]\tLoss: 4.386579\n",
      "Train Epoch: 1688 [acc: 35%]\tLoss: 6.010159\n",
      "Train Epoch: 1689 [acc: 37%]\tLoss: 5.063963\n",
      "Train Epoch: 1690 [acc: 29%]\tLoss: 5.303097\n",
      "Train Epoch: 1691 [acc: 24%]\tLoss: 6.282556\n",
      "Train Epoch: 1692 [acc: 27%]\tLoss: 6.311310\n",
      "Train Epoch: 1693 [acc: 32%]\tLoss: 5.209524\n",
      "Train Epoch: 1694 [acc: 30%]\tLoss: 6.472924\n",
      "Train Epoch: 1695 [acc: 31%]\tLoss: 6.135634\n",
      "Train Epoch: 1696 [acc: 34%]\tLoss: 4.742602\n",
      "Train Epoch: 1697 [acc: 29%]\tLoss: 5.777875\n",
      "Train Epoch: 1698 [acc: 41%]\tLoss: 5.541963\n",
      "Train Epoch: 1699 [acc: 24%]\tLoss: 7.004625\n",
      "Train Epoch: 1700 [acc: 35%]\tLoss: 4.807535\n",
      "[Trained for 1700 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.12 +- 0.62 , Avg Loss: 7.93\n",
      "Train Epoch: 1701 [acc: 33%]\tLoss: 5.610689\n",
      "Train Epoch: 1702 [acc: 28%]\tLoss: 5.622313\n",
      "Train Epoch: 1703 [acc: 36%]\tLoss: 5.032139\n",
      "Train Epoch: 1704 [acc: 30%]\tLoss: 6.411224\n",
      "Train Epoch: 1705 [acc: 42%]\tLoss: 4.061182\n",
      "Train Epoch: 1706 [acc: 33%]\tLoss: 5.115729\n",
      "Train Epoch: 1707 [acc: 34%]\tLoss: 4.953609\n",
      "Train Epoch: 1708 [acc: 31%]\tLoss: 4.861042\n",
      "Train Epoch: 1709 [acc: 31%]\tLoss: 5.469025\n",
      "Train Epoch: 1710 [acc: 27%]\tLoss: 5.916139\n",
      "Train Epoch: 1711 [acc: 40%]\tLoss: 4.003830\n",
      "Train Epoch: 1712 [acc: 27%]\tLoss: 6.490572\n",
      "Train Epoch: 1713 [acc: 29%]\tLoss: 5.547441\n",
      "Train Epoch: 1714 [acc: 25%]\tLoss: 6.873625\n",
      "Train Epoch: 1715 [acc: 29%]\tLoss: 5.354836\n",
      "Train Epoch: 1716 [acc: 30%]\tLoss: 5.069875\n",
      "Train Epoch: 1717 [acc: 36%]\tLoss: 4.859943\n",
      "Train Epoch: 1718 [acc: 30%]\tLoss: 5.971852\n",
      "Train Epoch: 1719 [acc: 23%]\tLoss: 6.431213\n",
      "Train Epoch: 1720 [acc: 28%]\tLoss: 6.146832\n",
      "Train Epoch: 1721 [acc: 34%]\tLoss: 5.072213\n",
      "Train Epoch: 1722 [acc: 34%]\tLoss: 5.742395\n",
      "Train Epoch: 1723 [acc: 32%]\tLoss: 5.763316\n",
      "Train Epoch: 1724 [acc: 24%]\tLoss: 6.284044\n",
      "Train Epoch: 1725 [acc: 27%]\tLoss: 5.439119\n",
      "[Trained for 1725 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.88 +- 0.67 , Avg Loss: 8.16\n",
      "Train Epoch: 1726 [acc: 30%]\tLoss: 5.017535\n",
      "Train Epoch: 1727 [acc: 30%]\tLoss: 6.485746\n",
      "Train Epoch: 1728 [acc: 35%]\tLoss: 4.440334\n",
      "Train Epoch: 1729 [acc: 34%]\tLoss: 5.000949\n",
      "Train Epoch: 1730 [acc: 29%]\tLoss: 5.398108\n",
      "Train Epoch: 1731 [acc: 30%]\tLoss: 5.827842\n",
      "Train Epoch: 1732 [acc: 22%]\tLoss: 6.461570\n",
      "Train Epoch: 1733 [acc: 29%]\tLoss: 6.019355\n",
      "Train Epoch: 1734 [acc: 23%]\tLoss: 6.309228\n",
      "Train Epoch: 1735 [acc: 30%]\tLoss: 5.439671\n",
      "Train Epoch: 1736 [acc: 37%]\tLoss: 4.800851\n",
      "Train Epoch: 1737 [acc: 34%]\tLoss: 5.258271\n",
      "Train Epoch: 1738 [acc: 23%]\tLoss: 5.821901\n",
      "Train Epoch: 1739 [acc: 38%]\tLoss: 5.054422\n",
      "Train Epoch: 1740 [acc: 28%]\tLoss: 6.243870\n",
      "Train Epoch: 1741 [acc: 30%]\tLoss: 5.217980\n",
      "Train Epoch: 1742 [acc: 35%]\tLoss: 4.588837\n",
      "Train Epoch: 1743 [acc: 32%]\tLoss: 4.735361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1744 [acc: 34%]\tLoss: 5.191627\n",
      "Train Epoch: 1745 [acc: 27%]\tLoss: 6.296119\n",
      "Train Epoch: 1746 [acc: 34%]\tLoss: 6.177954\n",
      "Train Epoch: 1747 [acc: 31%]\tLoss: 5.091797\n",
      "Train Epoch: 1748 [acc: 27%]\tLoss: 6.571234\n",
      "Train Epoch: 1749 [acc: 22%]\tLoss: 6.859026\n",
      "Train Epoch: 1750 [acc: 26%]\tLoss: 5.917472\n",
      "[Trained for 1750 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.30 +- 0.69 , Avg Loss: 8.05\n",
      "Train Epoch: 1751 [acc: 31%]\tLoss: 5.359730\n",
      "Train Epoch: 1752 [acc: 29%]\tLoss: 5.505345\n",
      "Train Epoch: 1753 [acc: 27%]\tLoss: 6.412131\n",
      "Train Epoch: 1754 [acc: 33%]\tLoss: 5.113166\n",
      "Train Epoch: 1755 [acc: 31%]\tLoss: 6.007956\n",
      "Train Epoch: 1756 [acc: 27%]\tLoss: 5.723130\n",
      "Train Epoch: 1757 [acc: 30%]\tLoss: 6.620576\n",
      "Train Epoch: 1758 [acc: 28%]\tLoss: 6.232605\n",
      "Train Epoch: 1759 [acc: 23%]\tLoss: 6.849936\n",
      "Train Epoch: 1760 [acc: 33%]\tLoss: 5.330819\n",
      "Train Epoch: 1761 [acc: 20%]\tLoss: 6.500650\n",
      "Train Epoch: 1762 [acc: 29%]\tLoss: 5.776295\n",
      "Train Epoch: 1763 [acc: 29%]\tLoss: 4.590209\n",
      "Train Epoch: 1764 [acc: 36%]\tLoss: 5.000076\n",
      "Train Epoch: 1765 [acc: 27%]\tLoss: 6.189034\n",
      "Train Epoch: 1766 [acc: 32%]\tLoss: 5.748618\n",
      "Train Epoch: 1767 [acc: 39%]\tLoss: 5.073773\n",
      "Train Epoch: 1768 [acc: 29%]\tLoss: 5.255740\n",
      "Train Epoch: 1769 [acc: 32%]\tLoss: 5.377700\n",
      "Train Epoch: 1770 [acc: 38%]\tLoss: 5.043987\n",
      "Train Epoch: 1771 [acc: 34%]\tLoss: 5.504700\n",
      "Train Epoch: 1772 [acc: 34%]\tLoss: 5.425394\n",
      "Train Epoch: 1773 [acc: 34%]\tLoss: 4.812738\n",
      "Train Epoch: 1774 [acc: 20%]\tLoss: 5.995697\n",
      "Train Epoch: 1775 [acc: 30%]\tLoss: 5.573449\n",
      "[Trained for 1775 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.98 +- 0.67 , Avg Loss: 7.72\n",
      "Train Epoch: 1776 [acc: 34%]\tLoss: 5.343507\n",
      "Train Epoch: 1777 [acc: 32%]\tLoss: 5.106504\n",
      "Train Epoch: 1778 [acc: 34%]\tLoss: 5.985078\n",
      "Train Epoch: 1779 [acc: 31%]\tLoss: 5.298300\n",
      "Train Epoch: 1780 [acc: 27%]\tLoss: 6.235730\n",
      "Train Epoch: 1781 [acc: 30%]\tLoss: 5.728270\n",
      "Train Epoch: 1782 [acc: 29%]\tLoss: 6.520465\n",
      "Train Epoch: 1783 [acc: 33%]\tLoss: 5.443680\n",
      "Train Epoch: 1784 [acc: 33%]\tLoss: 5.758046\n",
      "Train Epoch: 1785 [acc: 33%]\tLoss: 5.796025\n",
      "Train Epoch: 1786 [acc: 32%]\tLoss: 5.298822\n",
      "Train Epoch: 1787 [acc: 32%]\tLoss: 5.466082\n",
      "Train Epoch: 1788 [acc: 28%]\tLoss: 5.722339\n",
      "Train Epoch: 1789 [acc: 36%]\tLoss: 4.756289\n",
      "Train Epoch: 1790 [acc: 29%]\tLoss: 5.975843\n",
      "Train Epoch: 1791 [acc: 33%]\tLoss: 5.743566\n",
      "Train Epoch: 1792 [acc: 32%]\tLoss: 5.727501\n",
      "Train Epoch: 1793 [acc: 31%]\tLoss: 6.308046\n",
      "Train Epoch: 1794 [acc: 28%]\tLoss: 5.404514\n",
      "Train Epoch: 1795 [acc: 23%]\tLoss: 6.146258\n",
      "Train Epoch: 1796 [acc: 31%]\tLoss: 5.466052\n",
      "Train Epoch: 1797 [acc: 27%]\tLoss: 6.103682\n",
      "Train Epoch: 1798 [acc: 28%]\tLoss: 6.262906\n",
      "Train Epoch: 1799 [acc: 26%]\tLoss: 5.412907\n",
      "Train Epoch: 1800 [acc: 34%]\tLoss: 5.163868\n",
      "[Trained for 1800 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.20 +- 0.47 , Avg Loss: 7.81\n",
      "Train Epoch: 1801 [acc: 30%]\tLoss: 4.949391\n",
      "Train Epoch: 1802 [acc: 28%]\tLoss: 5.671843\n",
      "Train Epoch: 1803 [acc: 35%]\tLoss: 6.043089\n",
      "Train Epoch: 1804 [acc: 35%]\tLoss: 5.388576\n",
      "Train Epoch: 1805 [acc: 39%]\tLoss: 5.244728\n",
      "Train Epoch: 1806 [acc: 25%]\tLoss: 7.129059\n",
      "Train Epoch: 1807 [acc: 30%]\tLoss: 6.027904\n",
      "Train Epoch: 1808 [acc: 28%]\tLoss: 5.439106\n",
      "Train Epoch: 1809 [acc: 24%]\tLoss: 6.770566\n",
      "Train Epoch: 1810 [acc: 27%]\tLoss: 6.046218\n",
      "Train Epoch: 1811 [acc: 34%]\tLoss: 5.209761\n",
      "Train Epoch: 1812 [acc: 35%]\tLoss: 4.476066\n",
      "Train Epoch: 1813 [acc: 26%]\tLoss: 5.924269\n",
      "Train Epoch: 1814 [acc: 32%]\tLoss: 5.127686\n",
      "Train Epoch: 1815 [acc: 31%]\tLoss: 5.259225\n",
      "Train Epoch: 1816 [acc: 25%]\tLoss: 5.947421\n",
      "Train Epoch: 1817 [acc: 23%]\tLoss: 6.145465\n",
      "Train Epoch: 1818 [acc: 29%]\tLoss: 5.654383\n",
      "Train Epoch: 1819 [acc: 25%]\tLoss: 5.530112\n",
      "Train Epoch: 1820 [acc: 24%]\tLoss: 6.348867\n",
      "Train Epoch: 1821 [acc: 25%]\tLoss: 6.598460\n",
      "Train Epoch: 1822 [acc: 29%]\tLoss: 5.367663\n",
      "Train Epoch: 1823 [acc: 25%]\tLoss: 6.647474\n",
      "Train Epoch: 1824 [acc: 26%]\tLoss: 6.495321\n",
      "Train Epoch: 1825 [acc: 34%]\tLoss: 6.236135\n",
      "[Trained for 1825 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.33 +- 0.44 , Avg Loss: 7.82\n",
      "Train Epoch: 1826 [acc: 26%]\tLoss: 5.945231\n",
      "Train Epoch: 1827 [acc: 27%]\tLoss: 5.957268\n",
      "Train Epoch: 1828 [acc: 34%]\tLoss: 5.504055\n",
      "Train Epoch: 1829 [acc: 28%]\tLoss: 5.795470\n",
      "Train Epoch: 1830 [acc: 34%]\tLoss: 5.547304\n",
      "Train Epoch: 1831 [acc: 33%]\tLoss: 5.727454\n",
      "Train Epoch: 1832 [acc: 26%]\tLoss: 6.931503\n",
      "Train Epoch: 1833 [acc: 24%]\tLoss: 6.473319\n",
      "Train Epoch: 1834 [acc: 28%]\tLoss: 5.649921\n",
      "Train Epoch: 1835 [acc: 27%]\tLoss: 6.032167\n",
      "Train Epoch: 1836 [acc: 29%]\tLoss: 5.906928\n",
      "Train Epoch: 1837 [acc: 29%]\tLoss: 5.451904\n",
      "Train Epoch: 1838 [acc: 30%]\tLoss: 5.228381\n",
      "Train Epoch: 1839 [acc: 23%]\tLoss: 6.673272\n",
      "Train Epoch: 1840 [acc: 31%]\tLoss: 5.544033\n",
      "Train Epoch: 1841 [acc: 32%]\tLoss: 5.618182\n",
      "Train Epoch: 1842 [acc: 38%]\tLoss: 5.282788\n",
      "Train Epoch: 1843 [acc: 26%]\tLoss: 5.838336\n",
      "Train Epoch: 1844 [acc: 38%]\tLoss: 5.120498\n",
      "Train Epoch: 1845 [acc: 32%]\tLoss: 5.033309\n",
      "Train Epoch: 1846 [acc: 26%]\tLoss: 5.151700\n",
      "Train Epoch: 1847 [acc: 27%]\tLoss: 5.768386\n",
      "Train Epoch: 1848 [acc: 30%]\tLoss: 5.799502\n",
      "Train Epoch: 1849 [acc: 27%]\tLoss: 6.236821\n",
      "Train Epoch: 1850 [acc: 26%]\tLoss: 5.546857\n",
      "[Trained for 1850 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.53 +- 0.49 , Avg Loss: 8.24\n",
      "Train Epoch: 1851 [acc: 36%]\tLoss: 5.118753\n",
      "Train Epoch: 1852 [acc: 32%]\tLoss: 5.620543\n",
      "Train Epoch: 1853 [acc: 33%]\tLoss: 4.896469\n",
      "Train Epoch: 1854 [acc: 30%]\tLoss: 6.309777\n",
      "Train Epoch: 1855 [acc: 27%]\tLoss: 6.481066\n",
      "Train Epoch: 1856 [acc: 35%]\tLoss: 4.830886\n",
      "Train Epoch: 1857 [acc: 29%]\tLoss: 6.708162\n",
      "Train Epoch: 1858 [acc: 33%]\tLoss: 5.667552\n",
      "Train Epoch: 1859 [acc: 25%]\tLoss: 5.760118\n",
      "Train Epoch: 1860 [acc: 29%]\tLoss: 5.905581\n",
      "Train Epoch: 1861 [acc: 36%]\tLoss: 5.589517\n",
      "Train Epoch: 1862 [acc: 37%]\tLoss: 5.202025\n",
      "Train Epoch: 1863 [acc: 25%]\tLoss: 6.581810\n",
      "Train Epoch: 1864 [acc: 30%]\tLoss: 5.948107\n",
      "Train Epoch: 1865 [acc: 30%]\tLoss: 5.974195\n",
      "Train Epoch: 1866 [acc: 34%]\tLoss: 5.039457\n",
      "Train Epoch: 1867 [acc: 32%]\tLoss: 5.409312\n",
      "Train Epoch: 1868 [acc: 28%]\tLoss: 4.736436\n",
      "Train Epoch: 1869 [acc: 31%]\tLoss: 5.471906\n",
      "Train Epoch: 1870 [acc: 27%]\tLoss: 5.810231\n",
      "Train Epoch: 1871 [acc: 27%]\tLoss: 5.626844\n",
      "Train Epoch: 1872 [acc: 33%]\tLoss: 6.049035\n",
      "Train Epoch: 1873 [acc: 24%]\tLoss: 5.382246\n",
      "Train Epoch: 1874 [acc: 38%]\tLoss: 5.707754\n",
      "Train Epoch: 1875 [acc: 40%]\tLoss: 5.563587\n",
      "[Trained for 1875 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.65 +- 0.89 , Avg Loss: 7.87\n",
      "Train Epoch: 1876 [acc: 32%]\tLoss: 5.606282\n",
      "Train Epoch: 1877 [acc: 29%]\tLoss: 5.614155\n",
      "Train Epoch: 1878 [acc: 34%]\tLoss: 5.042091\n",
      "Train Epoch: 1879 [acc: 29%]\tLoss: 4.744966\n",
      "Train Epoch: 1880 [acc: 34%]\tLoss: 5.938982\n",
      "Train Epoch: 1881 [acc: 25%]\tLoss: 5.496896\n",
      "Train Epoch: 1882 [acc: 22%]\tLoss: 5.817858\n",
      "Train Epoch: 1883 [acc: 21%]\tLoss: 5.072691\n",
      "Train Epoch: 1884 [acc: 30%]\tLoss: 5.319850\n",
      "Train Epoch: 1885 [acc: 31%]\tLoss: 4.617630\n",
      "Train Epoch: 1886 [acc: 29%]\tLoss: 5.331107\n",
      "Train Epoch: 1887 [acc: 31%]\tLoss: 5.596653\n",
      "Train Epoch: 1888 [acc: 40%]\tLoss: 6.112256\n",
      "Train Epoch: 1889 [acc: 33%]\tLoss: 5.981095\n",
      "Train Epoch: 1890 [acc: 29%]\tLoss: 5.718030\n",
      "Train Epoch: 1891 [acc: 40%]\tLoss: 4.111375\n",
      "Train Epoch: 1892 [acc: 19%]\tLoss: 6.674207\n",
      "Train Epoch: 1893 [acc: 35%]\tLoss: 4.795647\n",
      "Train Epoch: 1894 [acc: 28%]\tLoss: 5.775321\n",
      "Train Epoch: 1895 [acc: 26%]\tLoss: 6.024161\n",
      "Train Epoch: 1896 [acc: 28%]\tLoss: 5.267951\n",
      "Train Epoch: 1897 [acc: 22%]\tLoss: 7.362546\n",
      "Train Epoch: 1898 [acc: 34%]\tLoss: 5.678630\n",
      "Train Epoch: 1899 [acc: 26%]\tLoss: 5.643613\n",
      "Train Epoch: 1900 [acc: 29%]\tLoss: 5.339763\n",
      "[Trained for 1900 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.20 +- 0.66 , Avg Loss: 8.07\n",
      "Train Epoch: 1901 [acc: 33%]\tLoss: 5.685782\n",
      "Train Epoch: 1902 [acc: 29%]\tLoss: 6.071248\n",
      "Train Epoch: 1903 [acc: 38%]\tLoss: 5.386636\n",
      "Train Epoch: 1904 [acc: 32%]\tLoss: 5.770556\n",
      "Train Epoch: 1905 [acc: 34%]\tLoss: 4.849834\n",
      "Train Epoch: 1906 [acc: 34%]\tLoss: 5.330495\n",
      "Train Epoch: 1907 [acc: 28%]\tLoss: 5.066216\n",
      "Train Epoch: 1908 [acc: 32%]\tLoss: 4.920501\n",
      "Train Epoch: 1909 [acc: 37%]\tLoss: 5.211968\n",
      "Train Epoch: 1910 [acc: 39%]\tLoss: 5.355126\n",
      "Train Epoch: 1911 [acc: 36%]\tLoss: 5.714783\n",
      "Train Epoch: 1912 [acc: 25%]\tLoss: 6.068628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1913 [acc: 35%]\tLoss: 5.855463\n",
      "Train Epoch: 1914 [acc: 29%]\tLoss: 7.066701\n",
      "Train Epoch: 1915 [acc: 26%]\tLoss: 6.308536\n",
      "Train Epoch: 1916 [acc: 31%]\tLoss: 5.559658\n",
      "Train Epoch: 1917 [acc: 32%]\tLoss: 5.013192\n",
      "Train Epoch: 1918 [acc: 25%]\tLoss: 6.203205\n",
      "Train Epoch: 1919 [acc: 28%]\tLoss: 6.220907\n",
      "Train Epoch: 1920 [acc: 33%]\tLoss: 5.895517\n",
      "Train Epoch: 1921 [acc: 42%]\tLoss: 4.601152\n",
      "Train Epoch: 1922 [acc: 38%]\tLoss: 4.828623\n",
      "Train Epoch: 1923 [acc: 28%]\tLoss: 4.516589\n",
      "Train Epoch: 1924 [acc: 33%]\tLoss: 4.602806\n",
      "Train Epoch: 1925 [acc: 21%]\tLoss: 6.525775\n",
      "[Trained for 1925 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.54 +- 0.90 , Avg Loss: 8.04\n",
      "Train Epoch: 1926 [acc: 33%]\tLoss: 5.510846\n",
      "Train Epoch: 1927 [acc: 29%]\tLoss: 6.193460\n",
      "Train Epoch: 1928 [acc: 35%]\tLoss: 6.041289\n",
      "Train Epoch: 1929 [acc: 33%]\tLoss: 5.550560\n",
      "Train Epoch: 1930 [acc: 25%]\tLoss: 6.310377\n",
      "Train Epoch: 1931 [acc: 37%]\tLoss: 5.538084\n",
      "Train Epoch: 1932 [acc: 28%]\tLoss: 6.530467\n",
      "Train Epoch: 1933 [acc: 36%]\tLoss: 5.923190\n",
      "Train Epoch: 1934 [acc: 31%]\tLoss: 5.603276\n",
      "Train Epoch: 1935 [acc: 24%]\tLoss: 5.463582\n",
      "Train Epoch: 1936 [acc: 34%]\tLoss: 4.850651\n",
      "Train Epoch: 1937 [acc: 34%]\tLoss: 5.126169\n",
      "Train Epoch: 1938 [acc: 34%]\tLoss: 6.156058\n",
      "Train Epoch: 1939 [acc: 29%]\tLoss: 5.398147\n",
      "Train Epoch: 1940 [acc: 39%]\tLoss: 5.312390\n",
      "Train Epoch: 1941 [acc: 38%]\tLoss: 4.783660\n",
      "Train Epoch: 1942 [acc: 30%]\tLoss: 5.516420\n",
      "Train Epoch: 1943 [acc: 26%]\tLoss: 5.970878\n",
      "Train Epoch: 1944 [acc: 26%]\tLoss: 5.483129\n",
      "Train Epoch: 1945 [acc: 24%]\tLoss: 5.834551\n",
      "Train Epoch: 1946 [acc: 29%]\tLoss: 4.569090\n",
      "Train Epoch: 1947 [acc: 22%]\tLoss: 5.357043\n",
      "Train Epoch: 1948 [acc: 30%]\tLoss: 4.685247\n",
      "Train Epoch: 1949 [acc: 39%]\tLoss: 4.511428\n",
      "Train Epoch: 1950 [acc: 21%]\tLoss: 5.117613\n",
      "[Trained for 1950 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.86 +- 0.55 , Avg Loss: 7.94\n",
      "Train Epoch: 1951 [acc: 20%]\tLoss: 6.498448\n",
      "Train Epoch: 1952 [acc: 29%]\tLoss: 5.768240\n",
      "Train Epoch: 1953 [acc: 36%]\tLoss: 4.767585\n",
      "Train Epoch: 1954 [acc: 23%]\tLoss: 6.076617\n",
      "Train Epoch: 1955 [acc: 27%]\tLoss: 5.724165\n",
      "Train Epoch: 1956 [acc: 20%]\tLoss: 7.293810\n",
      "Train Epoch: 1957 [acc: 30%]\tLoss: 5.981527\n",
      "Train Epoch: 1958 [acc: 41%]\tLoss: 4.546758\n",
      "Train Epoch: 1959 [acc: 29%]\tLoss: 6.506368\n",
      "Train Epoch: 1960 [acc: 27%]\tLoss: 5.740158\n",
      "Train Epoch: 1961 [acc: 31%]\tLoss: 5.603891\n",
      "Train Epoch: 1962 [acc: 34%]\tLoss: 4.483538\n",
      "Train Epoch: 1963 [acc: 33%]\tLoss: 5.449152\n",
      "Train Epoch: 1964 [acc: 35%]\tLoss: 5.129076\n",
      "Train Epoch: 1965 [acc: 35%]\tLoss: 5.396574\n",
      "Train Epoch: 1966 [acc: 25%]\tLoss: 5.370804\n",
      "Train Epoch: 1967 [acc: 35%]\tLoss: 5.470357\n",
      "Train Epoch: 1968 [acc: 35%]\tLoss: 5.664146\n",
      "Train Epoch: 1969 [acc: 30%]\tLoss: 5.771295\n",
      "Train Epoch: 1970 [acc: 29%]\tLoss: 6.243912\n",
      "Train Epoch: 1971 [acc: 26%]\tLoss: 5.826514\n",
      "Train Epoch: 1972 [acc: 37%]\tLoss: 4.810343\n",
      "Train Epoch: 1973 [acc: 35%]\tLoss: 5.334283\n",
      "Train Epoch: 1974 [acc: 26%]\tLoss: 6.669226\n",
      "Train Epoch: 1975 [acc: 30%]\tLoss: 5.325750\n",
      "[Trained for 1975 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.39 +- 0.59 , Avg Loss: 7.63\n",
      "Train Epoch: 1976 [acc: 40%]\tLoss: 5.368851\n",
      "Train Epoch: 1977 [acc: 30%]\tLoss: 6.038427\n",
      "Train Epoch: 1978 [acc: 34%]\tLoss: 5.014480\n",
      "Train Epoch: 1979 [acc: 30%]\tLoss: 5.932255\n",
      "Train Epoch: 1980 [acc: 33%]\tLoss: 5.049636\n",
      "Train Epoch: 1981 [acc: 30%]\tLoss: 5.528852\n",
      "Train Epoch: 1982 [acc: 32%]\tLoss: 5.230849\n",
      "Train Epoch: 1983 [acc: 33%]\tLoss: 5.027086\n",
      "Train Epoch: 1984 [acc: 37%]\tLoss: 5.292058\n",
      "Train Epoch: 1985 [acc: 35%]\tLoss: 4.778573\n",
      "Train Epoch: 1986 [acc: 32%]\tLoss: 6.246780\n",
      "Train Epoch: 1987 [acc: 35%]\tLoss: 5.398762\n",
      "Train Epoch: 1988 [acc: 34%]\tLoss: 4.788843\n",
      "Train Epoch: 1989 [acc: 39%]\tLoss: 5.261461\n",
      "Train Epoch: 1990 [acc: 28%]\tLoss: 6.039504\n",
      "Train Epoch: 1991 [acc: 33%]\tLoss: 5.382230\n",
      "Train Epoch: 1992 [acc: 28%]\tLoss: 5.012612\n",
      "Train Epoch: 1993 [acc: 32%]\tLoss: 6.675214\n",
      "Train Epoch: 1994 [acc: 31%]\tLoss: 5.021407\n",
      "Train Epoch: 1995 [acc: 26%]\tLoss: 6.672753\n",
      "Train Epoch: 1996 [acc: 29%]\tLoss: 6.398698\n",
      "Train Epoch: 1997 [acc: 25%]\tLoss: 6.005851\n",
      "Train Epoch: 1998 [acc: 37%]\tLoss: 5.102007\n",
      "Train Epoch: 1999 [acc: 28%]\tLoss: 5.729562\n",
      "Train Epoch: 2000 [acc: 38%]\tLoss: 4.966297\n",
      "[Trained for 2000 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.07 +- 0.67 , Avg Loss: 7.85\n",
      "Train Epoch: 2001 [acc: 28%]\tLoss: 6.452151\n",
      "Train Epoch: 2002 [acc: 27%]\tLoss: 5.845729\n",
      "Train Epoch: 2003 [acc: 25%]\tLoss: 4.533408\n",
      "Train Epoch: 2004 [acc: 30%]\tLoss: 5.448546\n",
      "Train Epoch: 2005 [acc: 40%]\tLoss: 5.426264\n",
      "Train Epoch: 2006 [acc: 29%]\tLoss: 5.980707\n",
      "Train Epoch: 2007 [acc: 25%]\tLoss: 6.477953\n",
      "Train Epoch: 2008 [acc: 30%]\tLoss: 5.519384\n",
      "Train Epoch: 2009 [acc: 27%]\tLoss: 7.042294\n",
      "Train Epoch: 2010 [acc: 27%]\tLoss: 6.374328\n",
      "Train Epoch: 2011 [acc: 30%]\tLoss: 5.524952\n",
      "Train Epoch: 2012 [acc: 31%]\tLoss: 5.257460\n",
      "Train Epoch: 2013 [acc: 31%]\tLoss: 5.967087\n",
      "Train Epoch: 2014 [acc: 41%]\tLoss: 4.148410\n",
      "Train Epoch: 2015 [acc: 32%]\tLoss: 5.139612\n",
      "Train Epoch: 2016 [acc: 28%]\tLoss: 6.607128\n",
      "Train Epoch: 2017 [acc: 39%]\tLoss: 5.470301\n",
      "Train Epoch: 2018 [acc: 33%]\tLoss: 5.171525\n",
      "Train Epoch: 2019 [acc: 31%]\tLoss: 6.067531\n",
      "Train Epoch: 2020 [acc: 31%]\tLoss: 5.446842\n",
      "Train Epoch: 2021 [acc: 26%]\tLoss: 5.425208\n",
      "Train Epoch: 2022 [acc: 38%]\tLoss: 4.410601\n",
      "Train Epoch: 2023 [acc: 30%]\tLoss: 4.948689\n",
      "Train Epoch: 2024 [acc: 30%]\tLoss: 6.013817\n",
      "Train Epoch: 2025 [acc: 37%]\tLoss: 5.921483\n",
      "[Trained for 2025 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.50 +- 0.93 , Avg Loss: 7.88\n",
      "Train Epoch: 2026 [acc: 36%]\tLoss: 5.996670\n",
      "Train Epoch: 2027 [acc: 35%]\tLoss: 4.494238\n",
      "Train Epoch: 2028 [acc: 29%]\tLoss: 5.890713\n",
      "Train Epoch: 2029 [acc: 34%]\tLoss: 5.477578\n",
      "Train Epoch: 2030 [acc: 25%]\tLoss: 7.133962\n",
      "Train Epoch: 2031 [acc: 18%]\tLoss: 6.918247\n",
      "Train Epoch: 2032 [acc: 38%]\tLoss: 4.363168\n",
      "Train Epoch: 2033 [acc: 35%]\tLoss: 4.676122\n",
      "Train Epoch: 2034 [acc: 31%]\tLoss: 6.188828\n",
      "Train Epoch: 2035 [acc: 28%]\tLoss: 6.137059\n",
      "Train Epoch: 2036 [acc: 37%]\tLoss: 5.104370\n",
      "Train Epoch: 2037 [acc: 36%]\tLoss: 5.154586\n",
      "Train Epoch: 2038 [acc: 32%]\tLoss: 5.046217\n",
      "Train Epoch: 2039 [acc: 33%]\tLoss: 6.164755\n",
      "Train Epoch: 2040 [acc: 27%]\tLoss: 5.520996\n",
      "Train Epoch: 2041 [acc: 27%]\tLoss: 5.699206\n",
      "Train Epoch: 2042 [acc: 34%]\tLoss: 6.027982\n",
      "Train Epoch: 2043 [acc: 33%]\tLoss: 5.175430\n",
      "Train Epoch: 2044 [acc: 40%]\tLoss: 5.235398\n",
      "Train Epoch: 2045 [acc: 34%]\tLoss: 5.264493\n",
      "Train Epoch: 2046 [acc: 23%]\tLoss: 5.550773\n",
      "Train Epoch: 2047 [acc: 26%]\tLoss: 5.826540\n",
      "Train Epoch: 2048 [acc: 32%]\tLoss: 5.729419\n",
      "Train Epoch: 2049 [acc: 24%]\tLoss: 6.650298\n",
      "Train Epoch: 2050 [acc: 30%]\tLoss: 4.991968\n",
      "[Trained for 2050 epochs and tested on 5 sets of 2000 images]        Avg Acc: 16.31 +- 0.69 , Avg Loss: 8.07\n",
      "Train Epoch: 2051 [acc: 28%]\tLoss: 6.147206\n",
      "Train Epoch: 2052 [acc: 27%]\tLoss: 5.043141\n",
      "Train Epoch: 2053 [acc: 25%]\tLoss: 5.844140\n",
      "Train Epoch: 2054 [acc: 33%]\tLoss: 5.203548\n",
      "Train Epoch: 2055 [acc: 39%]\tLoss: 4.965908\n",
      "Train Epoch: 2056 [acc: 29%]\tLoss: 6.119287\n",
      "Train Epoch: 2057 [acc: 33%]\tLoss: 5.623303\n",
      "Train Epoch: 2058 [acc: 32%]\tLoss: 6.009299\n",
      "Train Epoch: 2059 [acc: 28%]\tLoss: 4.808066\n",
      "Train Epoch: 2060 [acc: 35%]\tLoss: 5.106697\n",
      "Train Epoch: 2061 [acc: 31%]\tLoss: 5.360308\n",
      "Train Epoch: 2062 [acc: 30%]\tLoss: 6.071281\n",
      "Train Epoch: 2063 [acc: 32%]\tLoss: 5.599457\n",
      "Train Epoch: 2064 [acc: 27%]\tLoss: 5.991172\n",
      "Train Epoch: 2065 [acc: 33%]\tLoss: 5.753621\n",
      "Train Epoch: 2066 [acc: 23%]\tLoss: 6.543045\n",
      "Train Epoch: 2067 [acc: 33%]\tLoss: 5.192812\n",
      "Train Epoch: 2068 [acc: 27%]\tLoss: 6.024937\n",
      "Train Epoch: 2069 [acc: 33%]\tLoss: 4.554389\n",
      "Train Epoch: 2070 [acc: 32%]\tLoss: 5.326417\n",
      "Train Epoch: 2071 [acc: 26%]\tLoss: 5.928369\n",
      "Train Epoch: 2072 [acc: 37%]\tLoss: 5.902631\n",
      "Train Epoch: 2073 [acc: 37%]\tLoss: 5.330954\n",
      "Train Epoch: 2074 [acc: 27%]\tLoss: 5.092567\n",
      "Train Epoch: 2075 [acc: 40%]\tLoss: 3.810201\n",
      "[Trained for 2075 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.73 +- 0.86 , Avg Loss: 7.92\n",
      "Train Epoch: 2076 [acc: 38%]\tLoss: 4.594730\n",
      "Train Epoch: 2077 [acc: 22%]\tLoss: 5.887928\n",
      "Train Epoch: 2078 [acc: 37%]\tLoss: 4.946099\n",
      "Train Epoch: 2079 [acc: 27%]\tLoss: 5.973397\n",
      "Train Epoch: 2080 [acc: 33%]\tLoss: 5.648308\n",
      "Train Epoch: 2081 [acc: 29%]\tLoss: 4.735500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2082 [acc: 37%]\tLoss: 4.135371\n",
      "Train Epoch: 2083 [acc: 35%]\tLoss: 5.366210\n",
      "Train Epoch: 2084 [acc: 30%]\tLoss: 5.751951\n",
      "Train Epoch: 2085 [acc: 30%]\tLoss: 6.191255\n",
      "Train Epoch: 2086 [acc: 38%]\tLoss: 5.003034\n",
      "Train Epoch: 2087 [acc: 31%]\tLoss: 6.419865\n",
      "Train Epoch: 2088 [acc: 30%]\tLoss: 5.257380\n",
      "Train Epoch: 2089 [acc: 30%]\tLoss: 5.219092\n",
      "Train Epoch: 2090 [acc: 29%]\tLoss: 5.340225\n",
      "Train Epoch: 2091 [acc: 26%]\tLoss: 5.734572\n",
      "Train Epoch: 2092 [acc: 36%]\tLoss: 4.636786\n",
      "Train Epoch: 2093 [acc: 33%]\tLoss: 4.777450\n",
      "Train Epoch: 2094 [acc: 31%]\tLoss: 5.444011\n",
      "Train Epoch: 2095 [acc: 29%]\tLoss: 5.352147\n",
      "Train Epoch: 2096 [acc: 24%]\tLoss: 6.865884\n",
      "Train Epoch: 2097 [acc: 30%]\tLoss: 5.348666\n",
      "Train Epoch: 2098 [acc: 30%]\tLoss: 5.644678\n",
      "Train Epoch: 2099 [acc: 35%]\tLoss: 5.117139\n",
      "Train Epoch: 2100 [acc: 34%]\tLoss: 5.369485\n",
      "[Trained for 2100 epochs and tested on 5 sets of 2000 images]        Avg Acc: 15.68 +- 1.03 , Avg Loss: 8.21\n",
      "Train Epoch: 2101 [acc: 30%]\tLoss: 6.410943\n",
      "Train Epoch: 2102 [acc: 29%]\tLoss: 5.905829\n",
      "Train Epoch: 2103 [acc: 30%]\tLoss: 5.461273\n",
      "Train Epoch: 2104 [acc: 28%]\tLoss: 5.708866\n",
      "Train Epoch: 2105 [acc: 31%]\tLoss: 5.926446\n",
      "Train Epoch: 2106 [acc: 26%]\tLoss: 6.331093\n",
      "Train Epoch: 2107 [acc: 40%]\tLoss: 4.876941\n",
      "Train Epoch: 2108 [acc: 28%]\tLoss: 6.130722\n",
      "Train Epoch: 2109 [acc: 30%]\tLoss: 5.561134\n",
      "Train Epoch: 2110 [acc: 30%]\tLoss: 6.376075\n",
      "Train Epoch: 2111 [acc: 28%]\tLoss: 5.822333\n",
      "Train Epoch: 2112 [acc: 36%]\tLoss: 5.524259\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "#     resnet18 = models.resnet18()\n",
    "#     alexnet = models.alexnet()\n",
    "#     vgg16 = models.vgg16()\n",
    "#     squeezenet = models.squeezenet1_0()\n",
    "#     densenet = models.densenet161()\n",
    "#     inception = models.inception_v3()\n",
    "#     googlenet = models.googlenet()\n",
    "#     shufflenet = models.shufflenet_v2_x1_0()\n",
    "#     mobilenet = models.mobilenet_v2()\n",
    "#     resnext50_32x4d = models.resnext50_32x4d()\n",
    "#     wide_resnet50_2 = models.wide_resnet50_2()\n",
    "#     mnasnet = models.mnasnet1_0()\n",
    "\n",
    "OPTIM = \"rmsprop\"\n",
    "MODEL = \"efficientnet-b2\"\n",
    "EPOCH_NUM = 100000\n",
    "TRAIN_SAMPLE_NUM = 100\n",
    "VAL_SAMPLE_NUM = 2000\n",
    "BATCH_SIZE = 128\n",
    "VALIDATION_SET_NUM = 5\n",
    "AUGMENT = True\n",
    "VAL_DISPLAY_DIVISOR =25\n",
    "CIFAR_TRAIN = True\n",
    "\n",
    "#cifar-10:\n",
    "#mean = (0.4914, 0.4822, 0.4465)\n",
    "#std = (0.247, 0.243, 0.261)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "if AUGMENT:\n",
    "    dataAugmentation = [ \n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        AutoAugment(),\n",
    "        Cutout()\n",
    "    ]\n",
    "    augment = \"Crop,Flip,AutoAugment,Cutout\"\n",
    "else: \n",
    "    dataAugmentation = []\n",
    "    augment = \"Nothing\"\n",
    "\n",
    "\n",
    "# We resize images to allow using imagenet pre-trained models, is there a better way?\n",
    "resize = transforms.Resize(160) \n",
    "\n",
    "transform_train = transforms.Compose(dataAugmentation + [transforms.ToTensor(), normalize]) \n",
    "transform_val = transforms.Compose([transforms.ToTensor(), normalize]) #careful to keep this one same\n",
    "\n",
    "cifar_train = datasets.CIFAR10(root='.',train=CIFAR_TRAIN, transform=transform_train, download=True)\n",
    "cifar_val = datasets.CIFAR10(root='.',train=CIFAR_TRAIN, transform=transform_val, download=True)\n",
    "\n",
    "ss = SmallSampleController(numClasses=10,trainSampleNum=TRAIN_SAMPLE_NUM, # abstract the data-loading procedure\n",
    "                           valSampleNum=VAL_SAMPLE_NUM, batchSize=BATCH_SIZE, \n",
    "                           multiplier=VALIDATION_SET_NUM, trainDataset=cifar_train, \n",
    "                           valDataset=cifar_val)\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data, valSets, seed = ss.generateNewSet(device,valMultiplier = VALIDATION_SET_NUM) #Sample from datasets\n",
    "\n",
    "\n",
    "\n",
    "eNet = getModel(MODEL).cuda()\n",
    "\n",
    "# for param in eNet.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "# model = featureExtractor(eNet).cuda()\n",
    "model = eNet\n",
    "    \n",
    "optimizer,LR = getOptimizer128(OPTIM,model._fc.parameters())\n",
    "\n",
    "print(model.parameters)\n",
    "\n",
    "print(' => Total trainable parameters: %.2fM' % (sum(p.numel() for p in model.parameters()) / 1000000.0))        \n",
    "\n",
    "trainTracker = {\"meanLoss\":[],\"accuracy\":[]}\n",
    "valTracker = {\"allLoss\":[],\"allAcc\":[],\"meanLoss\":[],\"meanAcc\":[],\"stdAcc\":[]}\n",
    "latexTracker = []\n",
    "\n",
    "print(\"Begin Train for {} epochs\".format(EPOCH_NUM))\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    acc, loss = train(model, device, train_data[0], optimizer, epoch+1, display=True)\n",
    "    trainTracker[\"meanLoss\"].append(loss)\n",
    "    trainTracker[\"accuracy\"].append(acc)\n",
    "    \n",
    "    if (epoch+1) % VAL_DISPLAY_DIVISOR == 0:\n",
    "        checkTest(model,device,valSets,valTracker,latexTracker,epoch+1,\n",
    "              model_name=MODEL,optim_name=OPTIM,lr=LR,totalTestSamples=VAL_SAMPLE_NUM*VALIDATION_SET_NUM,\n",
    "                  seed=seed,verbose=True)\n",
    "        \n",
    "          \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = latexTracker[-1][:-2] \n",
    "\n",
    "def writeTex(latexTracker,dirname):\n",
    "    if not os.path.isdir(dirname):\n",
    "        os.mkdir(dirname)\n",
    "        \n",
    "    f= open(os.path.join(dirname,\"latexTable.txt\"),\"w\")\n",
    "    for x in latexTracker:\n",
    "        f.write(x)\n",
    "    f.close()\n",
    "\n",
    "writeTex(latexTracker,dirname)\n",
    "\n",
    "for x in latexTracker:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochList = [x+1 for x in range(len(trainTracker[\"meanLoss\"]))]\n",
    "\n",
    "plot(xlist=epochList,ylist=trainTracker[\"meanLoss\"],xlab=\"Mean Train Loss\",\n",
    "    ylab=\"Epochs\",title=\"Mean Train Loss over Epochs\",\n",
    "    color=\"#243A92\",label=\"mean train loss\",savedir=dirname,save=True)\n",
    "\n",
    "plot(xlist=epochList,ylist=trainTracker[\"accuracy\"],xlab=\"Train Accuracy\",\n",
    "    ylab=\"Epochs\",title=\"Train Accuracy Over Epochs\",\n",
    "    color=\"#34267E\",label=\"Train Accuracy\",savedir=dirname,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochList = [VAL_DISPLAY_DIVISOR*(x+1) for x in range(len(valTracker[\"meanLoss\"]))]\n",
    "\n",
    "plot(xlist=epochList,ylist=valTracker[\"meanLoss\"],xlab=\"Epochs\",\n",
    "    ylab=\"Mean Val Loss\",title=\"Mean Val Loss over Epochs\",\n",
    "    color=\"#243A92\",label=\"mean val loss\",savedir=dirname,save=True)\n",
    "\n",
    "plot(xlist=epochList,ylist=valTracker[\"meanAcc\"],xlab=\"Epochs\",\n",
    "    ylab=\"Val Accuracy\",title=\"Val Accuracy Over Epochs\",\n",
    "    color=\"#34267E\",label=\"Val Accuracy\",savedir=dirname,save=True)\n",
    "\n",
    "plot(xlist=epochList,ylist=valTracker[\"stdAcc\"],xlab=\"Epochs\",\n",
    "    ylab=\"Val Accuracy Standard Deviation\",title=\"Val Accuracy Standard Deviation Over Epochs\",\n",
    "    color=\"#34267E\",label=\"Val Accuracy SD\",savedir=dirname,save=True)\n",
    "\n",
    "\n",
    "valSetEvalCount = VAL_DISPLAY_DIVISOR * EPOCH_NUM * VALIDATION_SET_NUM\n",
    "epochList = [VAL_DISPLAY_DIVISOR*(x+1) for x in range(len(valTracker[\"meanLoss\"]))\\\n",
    "             for y in range(VALIDATION_SET_NUM)]\n",
    "\n",
    "\n",
    "plot(xlist=epochList,ylist=valTracker[\"allLoss\"],xlab=\"Val Set Evaluations\",\n",
    "    ylab=\"Val Loss\",title=\"Val loss over val set evaluations ({} \\\n",
    "every {} epochs)\".format(VALIDATION_SET_NUM,VAL_DISPLAY_DIVISOR),\n",
    "    color=\"#34267E\",label=\"Val Loss\",savedir=dirname,save=True)\n",
    "\n",
    "plot(xlist=epochList,ylist=valTracker[\"allAcc\"],xlab=\"Val Set Evaluations\",\n",
    "    ylab=\"Val Accuracy\",title=\"Val loss over val set evaluations ({} \\\n",
    "every {} epochs) \".format(VALIDATION_SET_NUM,VAL_DISPLAY_DIVISOR),\n",
    "    color=\"#34267E\",label=\"Val Accuracy\",savedir=dirname,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
