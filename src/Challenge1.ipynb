{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:40% !important; }</style>\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, display=True):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if display:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "          100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader,verbose=True):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    if verbose: print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "    return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValSets(num,random_permute,cifar_data,cifar_data_val):\n",
    "    sets = []\n",
    "    for x in range(num):\n",
    "        s = 200*x \n",
    "        indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == class_)[0][random_permute[s:s+200]] for class_ in range(0, 10)])\n",
    "        subset = Subset(cifar_data_val, indx_val)\n",
    "        sets.append(torch.utils.data.DataLoader(subset,\n",
    "                                           batch_size=128, \n",
    "                                           shuffle=False))\n",
    "    return sets\n",
    "\n",
    "def getLatexRow(seed,net,acc,epoch,lr,dataAug=\"Nothing\"):\n",
    "    categories = [\"seed\",\"network\",\"Accuracy\",\"Epochs\",\"learning rate\",\"data augmentation\"]\n",
    "    row = [str(seed),str(net),str(round(acc,3)),str(epoch),str(lr),str(dataAug)]\n",
    "    \n",
    "    c = \"&\".join(categories)\n",
    "    r = \"&\".join(row)\n",
    "    return \"{}\\\\\\\\\\n{}\\\\\\\\\\n\".format(c,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Num Samples For Training 100 Num Samples For Val 100\n",
      "<generator object Module.parameters at 0x7f15e84cd190>\n",
      "Begin Train for 375 epochs\n",
      "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.333071\n",
      "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.294249\n",
      "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.332584\n",
      "Train Epoch: 3 [0/100 (0%)]\tLoss: 2.300163\n",
      "Train Epoch: 4 [0/100 (0%)]\tLoss: 2.265594\n",
      "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.281590\n",
      "Train Epoch: 6 [0/100 (0%)]\tLoss: 2.277528\n",
      "Train Epoch: 7 [0/100 (0%)]\tLoss: 2.253400\n",
      "Train Epoch: 8 [0/100 (0%)]\tLoss: 2.240247\n",
      "Train Epoch: 9 [0/100 (0%)]\tLoss: 2.275128\n",
      "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.259719\n",
      "Train Epoch: 11 [0/100 (0%)]\tLoss: 2.235766\n",
      "Train Epoch: 12 [0/100 (0%)]\tLoss: 2.261380\n",
      "Train Epoch: 13 [0/100 (0%)]\tLoss: 2.222946\n",
      "Train Epoch: 14 [0/100 (0%)]\tLoss: 2.233634\n",
      "Train Epoch: 15 [0/100 (0%)]\tLoss: 2.215041\n",
      "Train Epoch: 16 [0/100 (0%)]\tLoss: 2.211816\n",
      "Train Epoch: 17 [0/100 (0%)]\tLoss: 2.153482\n",
      "Train Epoch: 18 [0/100 (0%)]\tLoss: 2.226041\n",
      "Train Epoch: 19 [0/100 (0%)]\tLoss: 2.205010\n",
      "Train Epoch: 20 [0/100 (0%)]\tLoss: 2.181653\n",
      "Train Epoch: 21 [0/100 (0%)]\tLoss: 2.178082\n",
      "Train Epoch: 22 [0/100 (0%)]\tLoss: 2.174243\n",
      "Train Epoch: 23 [0/100 (0%)]\tLoss: 2.135711\n",
      "epoch 25\n",
      "Train Epoch: 24 [0/100 (0%)]\tLoss: 2.140274\n",
      "Train Epoch: 25 [0/100 (0%)]\tLoss: 2.152752\n",
      "Train Epoch: 26 [0/100 (0%)]\tLoss: 2.053809\n",
      "Train Epoch: 27 [0/100 (0%)]\tLoss: 2.195863\n",
      "Train Epoch: 28 [0/100 (0%)]\tLoss: 2.127908\n",
      "Train Epoch: 29 [0/100 (0%)]\tLoss: 2.128439\n",
      "Train Epoch: 30 [0/100 (0%)]\tLoss: 2.076441\n",
      "Train Epoch: 31 [0/100 (0%)]\tLoss: 2.111759\n",
      "Train Epoch: 32 [0/100 (0%)]\tLoss: 2.048062\n",
      "Train Epoch: 33 [0/100 (0%)]\tLoss: 2.088525\n",
      "Train Epoch: 34 [0/100 (0%)]\tLoss: 2.042354\n",
      "Train Epoch: 35 [0/100 (0%)]\tLoss: 2.114212\n",
      "Train Epoch: 36 [0/100 (0%)]\tLoss: 2.055213\n",
      "Train Epoch: 37 [0/100 (0%)]\tLoss: 2.046972\n",
      "Train Epoch: 38 [0/100 (0%)]\tLoss: 2.023285\n",
      "Train Epoch: 39 [0/100 (0%)]\tLoss: 2.137415\n",
      "Train Epoch: 40 [0/100 (0%)]\tLoss: 1.971645\n",
      "Train Epoch: 41 [0/100 (0%)]\tLoss: 2.133899\n",
      "Train Epoch: 42 [0/100 (0%)]\tLoss: 2.037134\n",
      "Train Epoch: 43 [0/100 (0%)]\tLoss: 1.970930\n",
      "Train Epoch: 44 [0/100 (0%)]\tLoss: 1.932711\n",
      "Train Epoch: 45 [0/100 (0%)]\tLoss: 1.977324\n",
      "Train Epoch: 46 [0/100 (0%)]\tLoss: 1.931831\n",
      "Train Epoch: 47 [0/100 (0%)]\tLoss: 1.974907\n",
      "Train Epoch: 48 [0/100 (0%)]\tLoss: 1.956700\n",
      "epoch 50\n",
      "Train Epoch: 49 [0/100 (0%)]\tLoss: 1.964230\n",
      "Train Epoch: 50 [0/100 (0%)]\tLoss: 1.929910\n",
      "Train Epoch: 51 [0/100 (0%)]\tLoss: 1.928303\n",
      "Train Epoch: 52 [0/100 (0%)]\tLoss: 1.947537\n",
      "Train Epoch: 53 [0/100 (0%)]\tLoss: 1.815679\n",
      "Train Epoch: 54 [0/100 (0%)]\tLoss: 1.973608\n",
      "Train Epoch: 55 [0/100 (0%)]\tLoss: 1.897749\n",
      "Train Epoch: 56 [0/100 (0%)]\tLoss: 1.911866\n",
      "Train Epoch: 57 [0/100 (0%)]\tLoss: 1.852457\n",
      "Train Epoch: 58 [0/100 (0%)]\tLoss: 1.865901\n",
      "Train Epoch: 59 [0/100 (0%)]\tLoss: 1.950552\n",
      "Train Epoch: 60 [0/100 (0%)]\tLoss: 1.956398\n",
      "Train Epoch: 61 [0/100 (0%)]\tLoss: 1.808900\n",
      "Train Epoch: 62 [0/100 (0%)]\tLoss: 1.835626\n",
      "Train Epoch: 63 [0/100 (0%)]\tLoss: 1.941968\n",
      "Train Epoch: 64 [0/100 (0%)]\tLoss: 1.953205\n",
      "Train Epoch: 65 [0/100 (0%)]\tLoss: 1.793506\n",
      "Train Epoch: 66 [0/100 (0%)]\tLoss: 1.788983\n",
      "Train Epoch: 67 [0/100 (0%)]\tLoss: 1.713234\n",
      "Train Epoch: 68 [0/100 (0%)]\tLoss: 1.808517\n",
      "Train Epoch: 69 [0/100 (0%)]\tLoss: 1.817974\n",
      "Train Epoch: 70 [0/100 (0%)]\tLoss: 1.785767\n",
      "Train Epoch: 71 [0/100 (0%)]\tLoss: 1.826592\n",
      "Train Epoch: 72 [0/100 (0%)]\tLoss: 1.668909\n",
      "Train Epoch: 73 [0/100 (0%)]\tLoss: 1.855852\n",
      "epoch 75\n",
      "Train Epoch: 74 [0/100 (0%)]\tLoss: 1.793175\n",
      "Train Epoch: 75 [0/100 (0%)]\tLoss: 1.828224\n",
      "Train Epoch: 76 [0/100 (0%)]\tLoss: 1.761973\n",
      "Train Epoch: 77 [0/100 (0%)]\tLoss: 1.801351\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6376367d64ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochNum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m25\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalSets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e0d005939159>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, display)\u001b[0m\n\u001b[1;32m     11\u001b[0m         print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n\u001b[1;32m     12\u001b[0m           \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m           100. * batch_idx / len(train_loader), loss.item()))\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from numpy.random import RandomState\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "import PIL\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from wide_resnet import WideResNet\n",
    "from auto_augment import AutoAugment, Cutout\n",
    "from models.EfficientNets import EfficientNet\n",
    "\n",
    "results = {}\n",
    "\n",
    "\n",
    "\n",
    "VALIDATION_SET_NUM = 1\n",
    "\n",
    "\n",
    "\n",
    "for epochNum in [375]:\n",
    "    results[epochNum] = 0\n",
    "# Avoid cuda out of memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "    AUGMENT = True\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                      std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "    if AUGMENT:\n",
    "        dataAugmentation = [ \n",
    "#             torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "#             torchvision.transforms.RandomHorizontalFlip(),\n",
    "#             torchvision.transforms.RandomRotation(20, resample=PIL.Image.BILINEAR)\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            AutoAugment(),\n",
    "            Cutout()\n",
    "        ]\n",
    "        augment = \"Color Jitter+HorizFlip+rotation+AutoAugment\"\n",
    "    else: \n",
    "        dataAugmentation = []\n",
    "        augment = \"Nothing\"\n",
    "\n",
    "\n",
    "    transform_val = transforms.Compose([transforms.ToTensor(), normalize]) #careful to keep this one same\n",
    "    transform_train = transforms.Compose(dataAugmentation + [transforms.ToTensor(), normalize]) \n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    ##### Cifar Data\n",
    "    cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
    "    #We need two copies of this due to weird dataset api \n",
    "    cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
    "\n",
    "    # Extract a subset of 100 (class balanced) samples per class\n",
    "\n",
    "    accsGlobal = []\n",
    "\n",
    "    for seed in range(1):\n",
    "        accsLocal = []\n",
    "        prng = RandomState(seed)\n",
    "        random_permute = prng.permutation(np.arange(0, 5000))\n",
    "\n",
    "        indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:10]] for classe in range(0, 10)])\n",
    "        indx_val = np.concatenate([np.where(np.array(cifar_data_val.targets) == classe)[0][random_permute[0:10]] for classe in range(0, 10)])\n",
    "        \n",
    "        \n",
    "        valSets = getValSets(VALIDATION_SET_NUM,random_permute,cifar_data,cifar_data_val)\n",
    "\n",
    "        train_data = Subset(cifar_data, indx_train)\n",
    "        val_data = Subset(cifar_data_val, indx_val)\n",
    "\n",
    "\n",
    "        print('Num Samples For Training {} Num Samples For Val {}'.format(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                                 batch_size=128, \n",
    "                                                 shuffle=True)\n",
    "\n",
    "        val_loader = torch.utils.data.DataLoader(val_data,\n",
    "                                               batch_size=128, \n",
    "                                               shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "        model = WideResNet(28, 10, num_classes=10)\n",
    "#         model = EfficientNet.from_name('efficientnet-b5',num_classes=10)\n",
    "        model.to(device)\n",
    "#         optimizer = torch.optim.SGD(model.parameters(), \n",
    "#                                   lr=0.09, momentum=0.9,\n",
    "#                                   weight_decay=0.0005)\n",
    "        print(model.parameters())\n",
    "        optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                  lr=0.0001, weight_decay=0)\n",
    "        \n",
    "        \n",
    "\n",
    "        print(\"Begin Train for {} epochs\".format(epochNum))\n",
    "        for epoch in range(epochNum):\n",
    "            \n",
    "            train(model, device, train_loader, optimizer, epoch, display=True)\n",
    "            \n",
    "            if (epoch+1) % 25 == 0: \n",
    "                temp = []\n",
    "                print(\"epoch {}\".format(epoch+1))\n",
    "\n",
    "        for val_loader in valSets:\n",
    "            accsLocal.append(test(model, device, val_loader,verbose = False))\n",
    "\n",
    "        temp = getLatexRow(seed,\n",
    "                           net=\"default\",\n",
    "                           acc=round(float(np.mean(accsLocal)),3),\n",
    "                           epoch=epochNum,\n",
    "                           lr=0.2,\n",
    "                           dataAug=augment)\n",
    "        \n",
    "        accsGlobal = accsGlobal + accsLocal\n",
    "        accs = np.array(accsLocal)\n",
    "        print('[Trained for {} epochs and tested on {} sets of 2000 images] Avg Acc: {:.2f} +- {:.2f}'.format(\n",
    "            epochNum,VALIDATION_SET_NUM,accs.mean(),accs.std()))\n",
    "\n",
    "    accsGlobal = np.array(accsGlobal)\n",
    "  \n",
    "    \n",
    "    results[epochNum] = (accs.mean(),accs.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original3trainings = {10: (11.84, 0.3160696125855822),\n",
    " 50: (21.75, 0.7968688725254613),\n",
    " 100: (23.69, 0.7748548251124211),\n",
    " 150: (27.059999999999995, 0.7873372847769876),\n",
    " 200: (22.72, 0.9693296652842113)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondWith10TrainingsEach = {100: (24.979999999999997, 0.8562125904236637),\n",
    " 125: (23.845000000000002, 0.7198784619642397),\n",
    " 150: (23.574999999999996, 0.7413669806512837),\n",
    " 175: (28.440000000000005, 1.0403845442911963),\n",
    " 200: (22.69, 0.9350935782048763)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirdwith10 = {175: (24.009999999999998, 0.8882004278314662)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.from_numpy(np.arange(64)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
