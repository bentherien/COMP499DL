{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers+=[nn.Conv2d(3, 16,  kernel_size=3) , \n",
    "                      nn.ReLU(inplace=True)]\n",
    "        self.layers+=[nn.Conv2d(16, 16,  kernel_size=3, stride=2), \n",
    "                      nn.ReLU(inplace=True)]\n",
    "        self.layers+=[nn.Conv2d(16, 32,  kernel_size=3), \n",
    "                      nn.ReLU(inplace=True)]\n",
    "        self.layers+=[nn.Conv2d(32, 32,  kernel_size=3, stride=2), \n",
    "                      nn.ReLU(inplace=True)]\n",
    "        self.fc = nn.Linear(32*5*5, 10)\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)):\n",
    "          x = self.layers[i](x)\n",
    "        x = x.view(-1, 32*5*5)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Num Samples For Training 100 Num Samples For Val 2000\n",
      "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.303445\n",
      "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.301973\n",
      "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.298800\n",
      "Train Epoch: 15 [0/100 (0%)]\tLoss: 2.293989\n",
      "Train Epoch: 20 [0/100 (0%)]\tLoss: 2.285856\n",
      "Train Epoch: 25 [0/100 (0%)]\tLoss: 2.269397\n",
      "Train Epoch: 30 [0/100 (0%)]\tLoss: 2.234462\n",
      "Train Epoch: 35 [0/100 (0%)]\tLoss: 2.195818\n",
      "Train Epoch: 40 [0/100 (0%)]\tLoss: 2.127816\n",
      "Train Epoch: 45 [0/100 (0%)]\tLoss: 2.038943\n",
      "Train Epoch: 50 [0/100 (0%)]\tLoss: 1.900288\n",
      "Train Epoch: 55 [0/100 (0%)]\tLoss: 1.759894\n",
      "Train Epoch: 60 [0/100 (0%)]\tLoss: 1.567394\n",
      "Train Epoch: 65 [0/100 (0%)]\tLoss: 1.397699\n",
      "Train Epoch: 70 [0/100 (0%)]\tLoss: 1.216336\n",
      "Train Epoch: 75 [0/100 (0%)]\tLoss: 1.004221\n",
      "Train Epoch: 80 [0/100 (0%)]\tLoss: 1.125036\n",
      "Train Epoch: 85 [0/100 (0%)]\tLoss: 1.165849\n",
      "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.715333\n",
      "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.418263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/.local/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 10.6894, Accuracy: 375/2000 (18.75%)\n",
      "\n",
      "Num Samples For Training 100 Num Samples For Val 2000\n",
      "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.303427\n",
      "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.302537\n",
      "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.300617\n",
      "Train Epoch: 15 [0/100 (0%)]\tLoss: 2.298052\n",
      "Train Epoch: 20 [0/100 (0%)]\tLoss: 2.294733\n",
      "Train Epoch: 25 [0/100 (0%)]\tLoss: 2.289792\n",
      "Train Epoch: 30 [0/100 (0%)]\tLoss: 2.281243\n",
      "Train Epoch: 35 [0/100 (0%)]\tLoss: 2.264101\n",
      "Train Epoch: 40 [0/100 (0%)]\tLoss: 2.227575\n",
      "Train Epoch: 45 [0/100 (0%)]\tLoss: 2.174521\n",
      "Train Epoch: 50 [0/100 (0%)]\tLoss: 2.107082\n",
      "Train Epoch: 55 [0/100 (0%)]\tLoss: 2.027960\n",
      "Train Epoch: 60 [0/100 (0%)]\tLoss: 1.932490\n",
      "Train Epoch: 65 [0/100 (0%)]\tLoss: 1.834360\n",
      "Train Epoch: 70 [0/100 (0%)]\tLoss: 1.699047\n",
      "Train Epoch: 75 [0/100 (0%)]\tLoss: 1.513784\n",
      "Train Epoch: 80 [0/100 (0%)]\tLoss: 1.322633\n",
      "Train Epoch: 85 [0/100 (0%)]\tLoss: 1.699984\n",
      "Train Epoch: 90 [0/100 (0%)]\tLoss: 1.835116\n",
      "Train Epoch: 95 [0/100 (0%)]\tLoss: 1.647890\n",
      "\n",
      "Test set: Average loss: 2.6715, Accuracy: 377/2000 (18.85%)\n",
      "\n",
      "Acc over 2 instances: 18.80 +- 0.05\n"
     ]
    }
   ],
   "source": [
    "from lib.eugene import train, test\n",
    "from numpy.random import RandomState\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "  \n",
    "from torchvision import datasets, transforms\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transform_val = transforms.Compose([transforms.ToTensor(), normalize]) #careful to keep this one same\n",
    "transform_train = transforms.Compose([transforms.ToTensor(), normalize]) \n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "##### Cifar Data\n",
    "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
    "    \n",
    "#We need two copies of this due to weird dataset api \n",
    "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
    "    \n",
    "# Extract a subset of 100 (class balanced) samples per class\n",
    "\n",
    "accs = []\n",
    "\n",
    "for seed in [1,2]:\n",
    "    prng = RandomState(seed)\n",
    "    random_permute = prng.permutation(np.arange(0, 5000))\n",
    "    indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:10]] for classe in range(0, 10)])\n",
    "    indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[10:210]] for classe in range(0, 10)])\n",
    "\n",
    "\n",
    "    train_data = Subset(cifar_data, indx_train)\n",
    "    val_data = Subset(cifar_data_val, indx_val)\n",
    "\n",
    "    print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                             batch_size=128, \n",
    "                                             shuffle=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_data,\n",
    "                                           batch_size=128, \n",
    "                                           shuffle=False)\n",
    "\n",
    "\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), \n",
    "                              lr=0.01, momentum=0.9,\n",
    "                              weight_decay=0.0005)\n",
    "    for epoch in range(100):\n",
    "        train(model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
    "\n",
    "    accs.append(test(model, device, val_loader))\n",
    "\n",
    "accs = np.array(accs)\n",
    "print('Acc over 2 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
