{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:40% !important; }</style>\"))\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers+=[nn.Conv2d(3, 16,  kernel_size=3) , \n",
    "                      nn.ReLU(inplace=True)]\n",
    "        self.layers+=[nn.Conv2d(16, 16,  kernel_size=3, stride=2), \n",
    "                      nn.ReLU(inplace=True)]\n",
    "        self.layers+=[nn.Conv2d(16, 32,  kernel_size=3), \n",
    "                      nn.ReLU(inplace=True)]\n",
    "        self.layers+=[nn.Conv2d(32, 32,  kernel_size=3, stride=2), \n",
    "                      nn.ReLU(inplace=True)]\n",
    "        self.fc = nn.Linear(32*5*5, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "        x = x.view(-1, 32*5*5)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, display=True):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if display:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "          100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader,verbose=True):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    if verbose: print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "    return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValSets(num,random_permute,cifar_data,cifar_data_val):\n",
    "    sets = []\n",
    "    for x in range(num):\n",
    "        s = 200*x \n",
    "        indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == class_)[0][random_permute[s:s+200]] for class_ in range(0, 10)])\n",
    "        subset = Subset(cifar_data_val, indx_val)\n",
    "        sets.append(torch.utils.data.DataLoader(subset,\n",
    "                                           batch_size=128, \n",
    "                                           shuffle=False))\n",
    "    return sets\n",
    "\n",
    "def getLatexRow(seed,net,acc,epoch,lr,dataAug=\"Nothing\"):\n",
    "    categories = [\"seed\",\"network\",\"Accuracy\",\"Epochs\",\"learning rate\",\"data augmentation\"]\n",
    "    row = [str(seed),str(net),str(round(acc,3)),str(epoch),str(lr),str(dataAug)]\n",
    "    \n",
    "    c = \"&\".join(categories)\n",
    "    r = \"&\".join(row)\n",
    "    return \"{}\\\\\\\\\\n{}\\\\\\\\\\n\".format(c,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Num Samples For Training 100 Num Samples For Val 100\n",
      "<generator object Module.parameters at 0x7efafc091970>\n",
      "Begin Train for 375 epochs\n",
      "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.411645\n",
      "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.330801\n",
      "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.476824\n",
      "Train Epoch: 3 [0/100 (0%)]\tLoss: 2.338576\n",
      "Train Epoch: 4 [0/100 (0%)]\tLoss: 2.343259\n",
      "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.410312\n",
      "Train Epoch: 6 [0/100 (0%)]\tLoss: 2.474582\n",
      "Train Epoch: 7 [0/100 (0%)]\tLoss: 2.487737\n",
      "Train Epoch: 8 [0/100 (0%)]\tLoss: 2.364075\n",
      "Train Epoch: 9 [0/100 (0%)]\tLoss: 2.355901\n",
      "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.342491\n",
      "Train Epoch: 11 [0/100 (0%)]\tLoss: 2.412377\n",
      "Train Epoch: 12 [0/100 (0%)]\tLoss: 2.334548\n",
      "Train Epoch: 13 [0/100 (0%)]\tLoss: 2.480771\n",
      "Train Epoch: 14 [0/100 (0%)]\tLoss: 2.345807\n",
      "Train Epoch: 15 [0/100 (0%)]\tLoss: 2.392214\n",
      "Train Epoch: 16 [0/100 (0%)]\tLoss: 2.427873\n",
      "Train Epoch: 17 [0/100 (0%)]\tLoss: 2.293640\n",
      "Train Epoch: 18 [0/100 (0%)]\tLoss: 2.381480\n",
      "Train Epoch: 19 [0/100 (0%)]\tLoss: 2.274790\n",
      "Train Epoch: 20 [0/100 (0%)]\tLoss: 2.437071\n",
      "Train Epoch: 21 [0/100 (0%)]\tLoss: 2.386925\n",
      "Train Epoch: 22 [0/100 (0%)]\tLoss: 2.360893\n",
      "Train Epoch: 23 [0/100 (0%)]\tLoss: 2.376750\n",
      "epoch 25\n",
      "Train Epoch: 24 [0/100 (0%)]\tLoss: 2.395738\n",
      "Train Epoch: 25 [0/100 (0%)]\tLoss: 2.391014\n",
      "Train Epoch: 26 [0/100 (0%)]\tLoss: 2.418708\n",
      "Train Epoch: 27 [0/100 (0%)]\tLoss: 2.440538\n",
      "Train Epoch: 28 [0/100 (0%)]\tLoss: 2.431952\n",
      "Train Epoch: 29 [0/100 (0%)]\tLoss: 2.414447\n",
      "Train Epoch: 30 [0/100 (0%)]\tLoss: 2.378374\n",
      "Train Epoch: 31 [0/100 (0%)]\tLoss: 2.375687\n",
      "Train Epoch: 32 [0/100 (0%)]\tLoss: 2.359999\n",
      "Train Epoch: 33 [0/100 (0%)]\tLoss: 2.366376\n",
      "Train Epoch: 34 [0/100 (0%)]\tLoss: 2.382829\n",
      "Train Epoch: 35 [0/100 (0%)]\tLoss: 2.425331\n",
      "Train Epoch: 36 [0/100 (0%)]\tLoss: 2.408728\n",
      "Train Epoch: 37 [0/100 (0%)]\tLoss: 2.414490\n",
      "Train Epoch: 38 [0/100 (0%)]\tLoss: 2.374226\n",
      "Train Epoch: 39 [0/100 (0%)]\tLoss: 2.255540\n",
      "Train Epoch: 40 [0/100 (0%)]\tLoss: 2.383613\n",
      "Train Epoch: 41 [0/100 (0%)]\tLoss: 2.340144\n",
      "Train Epoch: 42 [0/100 (0%)]\tLoss: 2.323991\n",
      "Train Epoch: 43 [0/100 (0%)]\tLoss: 2.352198\n",
      "Train Epoch: 44 [0/100 (0%)]\tLoss: 2.431164\n",
      "Train Epoch: 45 [0/100 (0%)]\tLoss: 2.373825\n",
      "Train Epoch: 46 [0/100 (0%)]\tLoss: 2.359718\n",
      "Train Epoch: 47 [0/100 (0%)]\tLoss: 2.308041\n",
      "Train Epoch: 48 [0/100 (0%)]\tLoss: 2.409306\n",
      "epoch 50\n",
      "Train Epoch: 49 [0/100 (0%)]\tLoss: 2.452989\n",
      "Train Epoch: 50 [0/100 (0%)]\tLoss: 2.384557\n",
      "Train Epoch: 51 [0/100 (0%)]\tLoss: 2.451674\n",
      "Train Epoch: 52 [0/100 (0%)]\tLoss: 2.383488\n",
      "Train Epoch: 53 [0/100 (0%)]\tLoss: 2.413982\n",
      "Train Epoch: 54 [0/100 (0%)]\tLoss: 2.372256\n",
      "Train Epoch: 55 [0/100 (0%)]\tLoss: 2.340144\n",
      "Train Epoch: 56 [0/100 (0%)]\tLoss: 2.389353\n",
      "Train Epoch: 57 [0/100 (0%)]\tLoss: 2.399045\n",
      "Train Epoch: 58 [0/100 (0%)]\tLoss: 2.340573\n",
      "Train Epoch: 59 [0/100 (0%)]\tLoss: 2.364164\n",
      "Train Epoch: 60 [0/100 (0%)]\tLoss: 2.255437\n",
      "Train Epoch: 61 [0/100 (0%)]\tLoss: 2.420567\n",
      "Train Epoch: 62 [0/100 (0%)]\tLoss: 2.355459\n",
      "Train Epoch: 63 [0/100 (0%)]\tLoss: 2.379613\n",
      "Train Epoch: 64 [0/100 (0%)]\tLoss: 2.384629\n",
      "Train Epoch: 65 [0/100 (0%)]\tLoss: 2.342307\n",
      "Train Epoch: 66 [0/100 (0%)]\tLoss: 2.261464\n",
      "Train Epoch: 67 [0/100 (0%)]\tLoss: 2.285491\n",
      "Train Epoch: 68 [0/100 (0%)]\tLoss: 2.324077\n",
      "Train Epoch: 69 [0/100 (0%)]\tLoss: 2.289533\n",
      "Train Epoch: 70 [0/100 (0%)]\tLoss: 2.444762\n",
      "Train Epoch: 71 [0/100 (0%)]\tLoss: 2.320226\n",
      "Train Epoch: 72 [0/100 (0%)]\tLoss: 2.424392\n",
      "Train Epoch: 73 [0/100 (0%)]\tLoss: 2.370581\n",
      "epoch 75\n",
      "Train Epoch: 74 [0/100 (0%)]\tLoss: 2.305843\n",
      "Train Epoch: 75 [0/100 (0%)]\tLoss: 2.351386\n",
      "Train Epoch: 76 [0/100 (0%)]\tLoss: 2.403094\n",
      "Train Epoch: 77 [0/100 (0%)]\tLoss: 2.253018\n",
      "Train Epoch: 78 [0/100 (0%)]\tLoss: 2.376780\n",
      "Train Epoch: 79 [0/100 (0%)]\tLoss: 2.377887\n",
      "Train Epoch: 80 [0/100 (0%)]\tLoss: 2.370922\n",
      "Train Epoch: 81 [0/100 (0%)]\tLoss: 2.347371\n",
      "Train Epoch: 82 [0/100 (0%)]\tLoss: 2.303830\n",
      "Train Epoch: 83 [0/100 (0%)]\tLoss: 2.389539\n",
      "Train Epoch: 84 [0/100 (0%)]\tLoss: 2.286556\n",
      "Train Epoch: 85 [0/100 (0%)]\tLoss: 2.341946\n",
      "Train Epoch: 86 [0/100 (0%)]\tLoss: 2.372918\n",
      "Train Epoch: 87 [0/100 (0%)]\tLoss: 2.332503\n",
      "Train Epoch: 88 [0/100 (0%)]\tLoss: 2.346652\n",
      "Train Epoch: 89 [0/100 (0%)]\tLoss: 2.343240\n",
      "Train Epoch: 90 [0/100 (0%)]\tLoss: 2.361956\n",
      "Train Epoch: 91 [0/100 (0%)]\tLoss: 2.392501\n",
      "Train Epoch: 92 [0/100 (0%)]\tLoss: 2.377416\n",
      "Train Epoch: 93 [0/100 (0%)]\tLoss: 2.291168\n",
      "Train Epoch: 94 [0/100 (0%)]\tLoss: 2.306187\n",
      "Train Epoch: 95 [0/100 (0%)]\tLoss: 2.440010\n",
      "Train Epoch: 96 [0/100 (0%)]\tLoss: 2.320801\n",
      "Train Epoch: 97 [0/100 (0%)]\tLoss: 2.366765\n",
      "Train Epoch: 98 [0/100 (0%)]\tLoss: 2.392884\n",
      "epoch 100\n",
      "Train Epoch: 99 [0/100 (0%)]\tLoss: 2.364814\n",
      "Train Epoch: 100 [0/100 (0%)]\tLoss: 2.387607\n",
      "Train Epoch: 101 [0/100 (0%)]\tLoss: 2.373561\n",
      "Train Epoch: 102 [0/100 (0%)]\tLoss: 2.372956\n",
      "Train Epoch: 103 [0/100 (0%)]\tLoss: 2.292039\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-866d7d6ad5a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochNum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m25\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalSets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e0d005939159>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, display)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/torch11/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/torch11/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/torch11/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/torch11/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/torch11/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/torch11/lib/python3.8/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/torch11/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/torch11/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \"\"\"\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/torch11/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/torch11/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mtobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;31m# unpack data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_getencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m         \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/torch11/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m_getencoder\u001b[0;34m(mode, encoder_name, args, extra)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"encoder {encoder_name} not available\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from numpy.random import RandomState\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "import PIL\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from wide_resnet import WideResNet\n",
    "from auto_augment import AutoAugment, Cutout\n",
    "from models.EfficientNets import EfficientNet\n",
    "\n",
    "results = {}\n",
    "\n",
    "\n",
    "\n",
    "VALIDATION_SET_NUM = 1\n",
    "\n",
    "\n",
    "\n",
    "for epochNum in [375]:\n",
    "    results[epochNum] = 0\n",
    "# Avoid cuda out of memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "    AUGMENT = True\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                      std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "    if AUGMENT:\n",
    "        dataAugmentation = [ \n",
    "#             torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "#             torchvision.transforms.RandomHorizontalFlip(),\n",
    "#             torchvision.transforms.RandomRotation(20, resample=PIL.Image.BILINEAR)\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            AutoAugment(),\n",
    "            Cutout()\n",
    "        ]\n",
    "        augment = \"Color Jitter+HorizFlip+rotation+AutoAugment\"\n",
    "    else: \n",
    "        dataAugmentation = []\n",
    "        augment = \"Nothing\"\n",
    "\n",
    "\n",
    "    transform_val = transforms.Compose([transforms.ToTensor(), normalize]) #careful to keep this one same\n",
    "    transform_train = transforms.Compose(dataAugmentation + [transforms.ToTensor(), normalize]) \n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    ##### Cifar Data\n",
    "    cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
    "    #We need two copies of this due to weird dataset api \n",
    "    cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
    "\n",
    "    # Extract a subset of 100 (class balanced) samples per class\n",
    "\n",
    "    accsGlobal = []\n",
    "\n",
    "    for seed in range(1):\n",
    "        accsLocal = []\n",
    "        prng = RandomState(seed)\n",
    "        random_permute = prng.permutation(np.arange(0, 5000))\n",
    "\n",
    "        indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:10]] for classe in range(0, 10)])\n",
    "        indx_val = np.concatenate([np.where(np.array(cifar_data_val.targets) == classe)[0][random_permute[0:10]] for classe in range(0, 10)])\n",
    "        \n",
    "        \n",
    "        valSets = getValSets(VALIDATION_SET_NUM,random_permute,cifar_data,cifar_data_val)\n",
    "\n",
    "        train_data = Subset(cifar_data, indx_train)\n",
    "        val_data = Subset(cifar_data_val, indx_val)\n",
    "\n",
    "\n",
    "        print('Num Samples For Training {} Num Samples For Val {}'.format(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                                 batch_size=128, \n",
    "                                                 shuffle=True)\n",
    "\n",
    "        val_loader = torch.utils.data.DataLoader(val_data,\n",
    "                                               batch_size=128, \n",
    "                                               shuffle=False)\n",
    "\n",
    "\n",
    "        # model = Net()\n",
    "#         model = WideResNet(28, 10, num_classes=10)\n",
    "        model = EfficientNet.from_name('efficientnet-b5',num_classes=10)\n",
    "        model.to(device)\n",
    "#         optimizer = torch.optim.SGD(model.parameters(), \n",
    "#                                   lr=0.09, momentum=0.9,\n",
    "#                                   weight_decay=0.0005)\n",
    "        print(model.parameters())\n",
    "        optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                  lr=0.0001, weight_decay=0)\n",
    "        \n",
    "        \n",
    "\n",
    "        print(\"Begin Train for {} epochs\".format(epochNum))\n",
    "        for epoch in range(epochNum):\n",
    "            if (epoch+1) % 25 == 0: print(\"epoch {}\".format(epoch+1))\n",
    "            train(model, device, train_loader, optimizer, epoch, display=True)\n",
    "\n",
    "        for val_loader in valSets:\n",
    "            accsLocal.append(test(model, device, val_loader,verbose = False))\n",
    "\n",
    "        temp = getLatexRow(seed,\n",
    "                           net=\"default\",\n",
    "                           acc=round(float(np.mean(accsLocal)),3),\n",
    "                           epoch=epochNum,\n",
    "                           lr=0.2,\n",
    "                           dataAug=augment)\n",
    "        \n",
    "        accsGlobal = accsGlobal + accsLocal\n",
    "        accs = np.array(accsLocal)\n",
    "        print('[Trained for {} epochs and tested on {} sets of 2000 images] Avg Acc: {:.2f} +- {:.2f}'.format(\n",
    "            epochNum,VALIDATION_SET_NUM,accs.mean(),accs.std()))\n",
    "\n",
    "    accsGlobal = np.array(accsGlobal)\n",
    "  \n",
    "    \n",
    "    results[epochNum] = (accs.mean(),accs.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{140: (32.578, 1.2630186063554256),\n",
       " 150: (26.448, 1.0591015059945863),\n",
       " 175: (31.66, 0.9220629045786409)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original3trainings = {10: (11.84, 0.3160696125855822),\n",
    " 50: (21.75, 0.7968688725254613),\n",
    " 100: (23.69, 0.7748548251124211),\n",
    " 150: (27.059999999999995, 0.7873372847769876),\n",
    " 200: (22.72, 0.9693296652842113)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondWith10TrainingsEach = {100: (24.979999999999997, 0.8562125904236637),\n",
    " 125: (23.845000000000002, 0.7198784619642397),\n",
    " 150: (23.574999999999996, 0.7413669806512837),\n",
    " 175: (28.440000000000005, 1.0403845442911963),\n",
    " 200: (22.69, 0.9350935782048763)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirdwith10 = {175: (24.009999999999998, 0.8882004278314662)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
