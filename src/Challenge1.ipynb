{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:40% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:40% !important; }</style>\"))\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers+=[nn.Conv2d(3, 16,  kernel_size=3) , \n",
    "                      nn.ReLU(inplace=True)]\n",
    "        self.layers+=[nn.Conv2d(16, 16,  kernel_size=3, stride=2), \n",
    "                      nn.ReLU(inplace=True)]\n",
    "        self.layers+=[nn.Conv2d(16, 32,  kernel_size=3), \n",
    "                      nn.ReLU(inplace=True)]\n",
    "        self.layers+=[nn.Conv2d(32, 32,  kernel_size=3, stride=2), \n",
    "                      nn.ReLU(inplace=True)]\n",
    "        self.fc = nn.Linear(32*5*5, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "        x = x.view(-1, 32*5*5)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, display=True):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if display:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "          100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValSets(num,random_permute,cifar_data,cifar_data_val):\n",
    "    sets = []\n",
    "    for x in range(num):\n",
    "        s = 200*x + 10\n",
    "        indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[s:s+200]] for classe in range(0, 10)])\n",
    "        subset = Subset(cifar_data_val, indx_val)\n",
    "        sets.append(torch.utils.data.DataLoader(subset,\n",
    "                                           batch_size=128, \n",
    "                                           shuffle=False))\n",
    "    return sets\n",
    "\n",
    "def getLatexRow(seed,net,acc,epoch,lr,dataAug=\"Nothing\"):\n",
    "    categories = [\"seed\",\"network\",\"Accuracy\",\"Epochs\",\"learning rate\",\"data augmentation\"]\n",
    "    row = [str(seed),str(net),str(round(acc,3)),str(epoch),str(lr),str(dataAug)]\n",
    "    \n",
    "    c = \"&\".join(categories)\n",
    "    r = \"&\".join(row)\n",
    "    return \"{}\\\\\\\\\\n{}\\\\\\\\\\n\".format(c,r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Num Samples For Training 100 Num Samples For Val 2000\n",
      "\n",
      "Test set: Average loss: 9.9919, Accuracy: 351/2000 (17.55%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 9.9847, Accuracy: 339/2000 (16.95%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 9.7995, Accuracy: 378/2000 (18.90%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 10.1040, Accuracy: 333/2000 (16.65%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 10.4743, Accuracy: 318/2000 (15.90%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 10.3609, Accuracy: 336/2000 (16.80%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 10.0508, Accuracy: 337/2000 (16.85%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 10.0247, Accuracy: 296/2000 (14.80%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 9.8532, Accuracy: 373/2000 (18.65%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 10.1306, Accuracy: 333/2000 (16.65%)\n",
      "\n",
      "seed&network&Accuracy&Epochs&learning rate&data augmentation\\\\\n",
      "1&default&16.97&100&0.01&Nothing\\\\\n",
      "\n",
      "Num Samples For Training 100 Num Samples For Val 2000\n",
      "\n",
      "Test set: Average loss: 3.1144, Accuracy: 343/2000 (17.15%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 3.0945, Accuracy: 343/2000 (17.15%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 3.1090, Accuracy: 347/2000 (17.35%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 3.0986, Accuracy: 345/2000 (17.25%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 3.0936, Accuracy: 359/2000 (17.95%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 3.0450, Accuracy: 342/2000 (17.10%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 3.1235, Accuracy: 359/2000 (17.95%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 3.1086, Accuracy: 347/2000 (17.35%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 3.0377, Accuracy: 356/2000 (17.80%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 3.1040, Accuracy: 354/2000 (17.70%)\n",
      "\n",
      "seed&network&Accuracy&Epochs&learning rate&data augmentation\\\\\n",
      "2&default&17.223&100&0.01&Nothing\\\\\n",
      "\n",
      "Acc over 2 instances: 17.22 +- 0.87\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import RandomState\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "import PIL\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "AUGMENT = False\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if AUGMENT:\n",
    "    dataAugmentation = [ \n",
    "        torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.RandomRotation(20, resample=PIL.Image.BILINEAR)\n",
    "    ]\n",
    "    augment = \"Color Jitter+HorizFlip+rotation\"\n",
    "else: \n",
    "    dataAugmentation = []\n",
    "    augment = \"Nothing\"\n",
    "\n",
    "\n",
    "transform_val = transforms.Compose([transforms.ToTensor(), normalize]) #careful to keep this one same\n",
    "transform_train = transforms.Compose([transforms.ToTensor(), normalize] + dataAugmentation) \n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "##### Cifar Data\n",
    "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
    "    \n",
    "#We need two copies of this due to weird dataset api \n",
    "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
    "    \n",
    "# Extract a subset of 100 (class balanced) samples per class\n",
    "\n",
    "accs = []\n",
    "\n",
    "trainTRSF = torchvision.transforms.Compose([\n",
    "torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "torchvision.transforms.RandomHorizontalFlip(),\n",
    "torchvision.transforms.RandomRotation(20, resample=PIL.Image.BILINEAR)])\n",
    "\n",
    "for seed in [1,2]:\n",
    "    prng = RandomState(seed)\n",
    "    random_permute = prng.permutation(np.arange(0, 5000))\n",
    "    \n",
    "    indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:10]] for classe in range(0, 10)])\n",
    "    valSets = getValSets(10,random_permute,cifar_data,cifar_data_val)\n",
    "\n",
    "    train_data = Subset(cifar_data, indx_train)\n",
    "#     val_data = Subset(cifar_data_val, indx_val)\n",
    "    \n",
    "\n",
    "    print('Num Samples For Training {} Num Samples For Val {}'.format(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                             batch_size=128, \n",
    "                                             shuffle=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_data,\n",
    "                                           batch_size=128, \n",
    "                                           shuffle=False)\n",
    "    \n",
    "    \n",
    "\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), \n",
    "                              lr=0.01, momentum=0.9,\n",
    "                              weight_decay=0.0005)\n",
    "    for epoch in range(100):\n",
    "        train(model, device, train_loader, optimizer, epoch, display=False)\n",
    "        \n",
    "    for val_loader in valSets:\n",
    "        accs.append(test(model, device, val_loader))\n",
    "        \n",
    "    temp = getLatexRow(seed,\n",
    "                        net=\"default\",\n",
    "                       acc=round(float(np.mean(accs)),3),\n",
    "                       epoch=100,\n",
    "                       lr=0.01,\n",
    "                       dataAug=augment)\n",
    "    print(temp)\n",
    "\n",
    "accs = np.array(accs)\n",
    "print('Acc over 2 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
